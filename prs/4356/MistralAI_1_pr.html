<!DOCTYPE html>
<html lang="en">
<head><base href="https://rss-bridge.org/bridge01/" target="_blank">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/ >
    <meta name="description" content="RSS-Bridge" />
    <title>Mistral AI Bridge</title>
    <link href="static/style.css?2023-03-24" rel="stylesheet">
    <link rel="icon" type="image/png" href="static/favicon.png">

    
        <link
            href="?action=display&amp;bridge=MistralAIBridge&amp;limit=10&amp;format=Atom"
            title="Atom"
            rel="alternate"
            type="application/atom+xml"
        >
	
        <link
            href="?action=display&amp;bridge=MistralAIBridge&amp;limit=10&amp;format=Json"
            title="Json"
            rel="alternate"
            type="application/json"
        >
	
        <link
            href="?action=display&amp;bridge=MistralAIBridge&amp;limit=10&amp;format=Mrss"
            title="Mrss"
            rel="alternate"
            type="application/rss+xml"
        >
	
        <link
            href="?action=display&amp;bridge=MistralAIBridge&amp;limit=10&amp;format=Plaintext"
            title="Plaintext"
            rel="alternate"
            type="text/plain"
        >
	
        <link
            href="?action=display&amp;bridge=MistralAIBridge&amp;limit=10&amp;format=Sfeed"
            title="Sfeed"
            rel="alternate"
            type="text/plain"
        >
	
    <meta name="robots" content="noindex, follow">
</head>

<body>
    <div class="container">

        <h1 class="pagetitle">
            <a href="https://mistral.ai/" target="_blank">Mistral AI Bridge</a>
        </h1>

        <div class="buttons">
            <a href="./#bridge-MistralAIBridge">
                <button class="backbutton">← back to rss-bridge</button>
            </a>

                            <a href="?action=display&amp;bridge=MistralAIBridge&amp;limit=10&amp;format=Atom">
                    <button class="rss-feed">
                        Atom                    </button>
                </a>
                            <a href="?action=display&amp;bridge=MistralAIBridge&amp;limit=10&amp;format=Json">
                    <button class="rss-feed">
                        Json                    </button>
                </a>
                            <a href="?action=display&amp;bridge=MistralAIBridge&amp;limit=10&amp;format=Mrss">
                    <button class="rss-feed">
                        Mrss                    </button>
                </a>
                            <a href="?action=display&amp;bridge=MistralAIBridge&amp;limit=10&amp;format=Plaintext">
                    <button class="rss-feed">
                        Plaintext                    </button>
                </a>
                            <a href="?action=display&amp;bridge=MistralAIBridge&amp;limit=10&amp;format=Sfeed">
                    <button class="rss-feed">
                        Sfeed                    </button>
                </a>
            
                    </div>

                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://mistral.ai//news/mistral-chat/"
                    >Mistral has entered the chat</a>
                </h2>

                                    <time datetime="2024-11-18 10:17:26">
                        2024-11-18 10:17:26                    </time>
                    <p></p>
                
                
                <!-- Intentionally not escaping for html context -->
                <p class=hero-description>Search, vision, ideation, coding… all yours for free.</p><div class="content mt-5 justify md-content"><p>AI is as much a pursuit of seamless interaction between human and machine intelligence, as it is of frontier science that makes machines reason better. At Mistral AI, we began our journey by developing state of the art models; we have now begun to invest in AI interfaces, starting with some big updates to <a href=https://chat.mistral.ai/>le Chat</a>.</p><h4 id=whats-new>What&rsquo;s New</h4><p>If you&rsquo;re new to le Chat, welcome! Le Chat [/lə ʃa/, French: ‘the cat&rsquo;] is the free generative AI work assistant from Mistral AI. You can use le Chat as your creative partner, teach it to perform specific tasks, use it as a coding copilot… basically, use it to get stuff done.</p><p>Today, we&rsquo;re giving le Chat new powers (in beta). Here&rsquo;s a quick summary:</p><ol><li>Web search with citations</li><li>Canvas for ideation, in-line editing, and export</li><li>State of the art document and image understanding, powered by the <a href=https://mistral.ai/news/pixtral-large/>new multimodal Pixtral Large</a></li><li>Image generation, powered by Black Forest Labs Flux Pro</li><li>Fully integrated offering, from models to outputs</li><li>Faster responses powered by speculative editing</li></ol><p>Here&rsquo;s how le Chat compares to leading chat and search assistants in the market today:</p><table><thead><tr><th>Feature</th><th>ChatGPT</th><th>Perplexity</th><th>Claude</th><th>Le Chat</th></tr></thead><tbody><tr><td>Access to latest frontier model</td><td>Paid</td><td>Yes</td><td>Yes</td><td>Yes</td></tr><tr><td>Web search with citations</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td></tr><tr><td>Canvas for ideation, in-line editing, and export</td><td>Paid</td><td>No</td><td>Limited</td><td>Yes</td></tr><tr><td>Image generation</td><td>Yes</td><td>No</td><td>No</td><td>Yes</td></tr><tr><td>Image / document analysis</td><td>Yes</td><td>Paid</td><td>Yes</td><td>Yes</td></tr><tr><td>Task agents</td><td>Paid</td><td>No</td><td>No</td><td>Yes</td></tr><tr><td>Single platform from models to outputs</td><td>Yes</td><td>No</td><td>Yes</td><td>Yes</td></tr></tbody></table><p>Let&rsquo;s dive in.</p><h5 id=web-search><strong>Web search</strong></h5><p>A significant share of le Chat usage comes from students and professionals who use it for learning, research, and work. To help better facilitate these use cases, we&rsquo;ve augmented le Chat&rsquo;s training knowledge with internet search.</p><p>Here&rsquo;s one of our marketing teammates using it to assess the AI market for healthcare.</p><div><a href="https://www.youtube.com/embed/IFLWwYPvgek">https://www.youtube.com/embed/IFLWwYPvgek</a></div><p>With state of the art language models that have access to the latest knowledge and happenings, you have a powerful tool to level up your work.</p><h5 id=canvas><strong>Canvas</strong></h5><p>Canvas is a new interface that pops up in your le Chat window when you need to go beyond conversations and into ideation. It enables you to collaborate with Mistral LLMs on shared outputs, enabling 0-1 creation and ideation for knowledge work.</p><p>No longer restricted to bidirectional conversations, Canvas enables you to directly modify, edit or transform content aided by the powerful reasoning capabilities of LLMs.</p><p>Here&rsquo;s a video of our product team creating a react mockup of a product metrics dashboard in preparation for the launch of Canvas.</p><div><a href="https://www.youtube.com/embed/K9TYg8-suvo">https://www.youtube.com/embed/K9TYg8-suvo</a></div><p>You can use Canvas to create documents, presentations, code, mockups&mldr; the list goes on. As you can see, you&rsquo;re able to modify its contents in place without regenerating responses, version your drafts, and preview your designs.</p><p>Canvas allows for infinite creative ideation for any team.</p><h5 id=fast-and-accurate-document-understanding><strong>Fast and accurate document understanding</strong></h5><p>Le Chat is now able to process large, complex PDF documents and images, for analysis and summarization. Simply attach documents to your chat bar and get insights on anything&mdash;graphs, tables, diagrams, text, formulae, equations&mldr; basically, the entirety of the document.</p><p>These advanced capabilities are made possible by the new Pixtral Large, a state of the art multimodal model that is significantly more powerful than our previous vision model, <a href=https://mistral.ai/news/pixtral-12b/>Pixtral 12B</a>, in conjunction with an experimental model (more on that in a few weeks).</p><p>In the example below, we show the extraction of information, summarization, and semantic understanding of the famous quantum entanglement paper from 1935 authored by Einstein, Podolsy, and Rosen.</p><div><a href="https://www.youtube.com/embed/b3nfBtYRhf4">https://www.youtube.com/embed/b3nfBtYRhf4</a></div><h5 id=image-generation><strong>Image generation</strong></h5><p>We have partnered with our friends at <a href=https://blackforestlabs.ai/>Black Forest Labs</a> to bring their leading image generation models to le Chat. You can now generate high quality images for your work right from within le Chat.</p><h5 id=le-chat-agents-to-automate-repetitive-workflows><strong>Le Chat Agents to automate repetitive workflows</strong></h5><p>If you find yourself reusing prompts, or if you&rsquo;ve identified certain patterns of creative work that you prefer to be automated, you can codify these workflows as agents. Popular use cases include receipt scanning for expense reporting, creating TL;DR&rsquo;s of long meeting minutes, and invoice processing.</p><p>Agents can be published and shared with your teammates. You can ideate recursively, and build increasingly powerful agents that help you get your work done faster. For more on agents, check out <a href=https://mistral.ai/news/build-tweak-repeat/>this blog</a> or our <a href=https://docs.mistral.ai/capabilities/agents/>documentation</a>.</p><h5 id=how-much-does-all-of-this-cost><strong>How much does all of this cost?</strong></h5><p>Currently, all of these features are introduced as free beta offerings. At Mistral AI, our approach to AI is different &ndash; we&rsquo;re not chasing AGI at all costs; our mission is to instead place <a href=https://mistral.ai/>frontier AI in your hands</a>, so you get to decide what to do with advanced AI capabilities. This approach has allowed us to be quite frugal with our capital, while consistently delivering frontier capabilities at affordable price points. On le Chat, we&rsquo;re offering a generous free tier with these beta features and working towards premium tiers with higher service guarantees.</p><h4 id=try-it><strong>Try it</strong></h4><p>Head over to <a href=http://chat.mistral.ai>chat.mistral.ai</a> to try le Chat. If your current AI-productivity toolchain is siloed or expensive, le Chat will give you an alternative to do your best work. All new features will be rolled out gradually over the next few weeks&mdash;be sure to let us know what you think!</p></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://mistral.ai//news/pixtral-large/"
                    >Pixtral Large</a>
                </h2>

                                    <time datetime="2024-11-18 10:17:26">
                        2024-11-18 10:17:26                    </time>
                    <p></p>
                
                
                <!-- Intentionally not escaping for html context -->
                <p class=hero-description>Pixtral grows up.</p><div class="content mt-5 justify md-content"><h3 id=pixtral-large-in-short>Pixtral Large in short:</h3><ul><li>Frontier-class multimodal performance</li><li>State-of-the-art on MathVista, DocVQA, VQAv2</li><li>Extends Mistral Large 2 without compromising text performance</li><li>123B multimodal decoder, 1B parameter vision encoder</li><li>128K context window: fits minimum of 30 high-resolution images</li><li>Use:<ul><li>Try it on <a href=https://chat.mistral.ai/>le Chat</a></li><li>In our <a href=https://console.mistral.ai/>API</a> as <code>pixtral-large-latest</code></li><li>Download it <a href=https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411>here</a></li></ul></li></ul><p>Today we announce Pixtral Large, a 124B open-weights multimodal model built on top of Mistral Large 2. Pixtral Large is the second model in our multimodal family and demonstrates frontier-level image understanding. Particularly, the model is able to understand documents, charts and natural images, while maintaining the leading text-only understanding of Mistral Large 2.</p><p>The model is available under the Mistral Research License (MRL) for research and educational use; and the <a href=https://mistral.ai/contact/>Mistral Commercial License</a> for experimentation, testing, and production for commercial purposes.</p><div style=text-align:center><img src=https://mistral.ai/images/news/pixtral-large/pixtral-large-header-fig.png alt="Benchmark summary" width=80% class=center></div><h3 id=performance>Performance</h3><p>We evaluate Pixtral Large against frontier models on a set of standard multimodal benchmarks, through a common testing harness. On MathVista, which evaluates complex mathematical reasoning over visual data, our model achieves 69.4%, outperforming all other models. To assess reasoning capabilities over complex charts and documents, we evaluate performance using ChartQA and DocVQA, where Pixtral Large surpasses GPT-4o and Gemini-1.5 Pro.</p><p>Finally, Pixtral Large demonstrates competitive capabilities on MM-MT-Bench, outperforming all of Claude-3.5 Sonnet (new), Gemini-1.5 Pro and GPT-4o (latest). MM-MT-Bench is an <a href=https://github.com/mistralai/mistral-evals>open-source</a>, judge-based evaluation intended to reflect real-world use cases of multimodal LLMs (see the <a href=https://arxiv.org/abs/2410.07073>Pixtral 12B technical report</a> for details).</p><div style=text-align:center><img src=https://mistral.ai/images/news/pixtral-large/pixtral-large-main-table.png alt="Detailed benchmarks" width=80% class=center></div><h3 id=qualitative-samples>Qualitative Samples</h3><h4 id=multilingual-ocr-and-reasoning>Multilingual OCR and reasoning&mldr;</h4><div style=text-align:center><img src=https://mistral.ai/images/news/pixtral-large/german-receipt.png alt="German receipt" width=50% class=center></div><p><strong>Prompt:</strong></p><p>I bought the coffee and the sausage. How much do I owe? Add a 18% tip.</p><p><strong>Response:</strong></p><p>To calculate the total amount you owe, including the 18% tip, follow these steps:</p><ol><li><p>Calculate the cost of the coffee and the sausage:</p><ul><li>2x Latte Macchiato: 2 * 4.50 CHF = 9.00 CHF</li><li>1x Schweinschnitzel: 22.00 CHF</li><li>Total cost for coffee and sausage: 9.00 CHF + 22.00 CHF = 31.00 CHF</li></ul></li><li><p>Calculate the 18% tip:</p><ul><li>Tip: 31.00 CHF * 0.18 = 5.58 CHF</li></ul></li><li><p>Add the tip to the total cost:</p><ul><li>Total amount owed: 31.00 CHF + 5.58 CHF = 36.58 CHF</li></ul></li></ol><p>So, you owe a total of 36.58 CHF.</p><h4 id=chart-understanding>Chart understanding&mldr;</h4><div style=text-align:center><img src=https://mistral.ai/images/news/pixtral-large/loss-curve.png alt="Loss curve" width=80% class=center></div><p><strong>Prompt:</strong></p><p>When did things start to go wrong for dark dragon?</p><p><strong>Response:</strong></p><p>The training loss for the &ldquo;dark-dragon-50&rdquo; model starts to exhibit significant instability around the 10,000 steps mark. There is a noticeable spike in the loss value at this point, and the instability continues with another major spike around the 20,000 steps mark. These spikes indicate that the training process for the &ldquo;dark-dragon-50&rdquo; model began to encounter issues starting around the 10,000 steps mark.</p><h4 id=and-more>And more&mldr;</h4><div style=text-align:center><img src=https://mistral.ai/images/news/pixtral-large/mistral-adopters.png alt="Website screenshot" width=80% class=center></div><p><strong>Prompt:</strong></p><p>Which companies use models made by Mistral AI?</p><p><strong>Response:</strong></p><p>The companies that use models made by Mistral AI, as shown in the image, are:</p><ol><li>BNP Paribas</li><li>Brave</li><li>Cloudflare</li><li>CMA CGM</li><li>Front</li></ol><h3 id=one-more-thing>One more thing&mldr;</h3><p>Along with Pixtral Large, <strong>Mistral Large</strong>, our state-of-the-art text model, also gets an update. The model is available as <code>pixtral-large-latest</code> on our <a href=https://docs.mistral.ai/api/>API</a>, as well as for self-deployment as Mistral Large 24.11 on HuggingFace under the Mistral Research License (MRL) for research, or with a <a href=https://mistral.ai/contact/>commercial license</a> from Mistral AI for commercial use.</p><p>This newest model provides a significant upgrade on the previous Mistral Large 24.07, with notable improvements in long context understanding, a new system prompt, and more accurate function calling. The model is highly capable for RAG and agentic workflows, making it a suitable choice for enterprise use cases such as knowledge exploration and sharing, semantic understanding of documents, task automation, and improved customer experiences.</p><p>Mistral Large 24.11 will be available from our cloud provider partners soon, starting with Google Cloud and Microsoft Azure within a week.</p></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://mistral.ai//news/batch-api/"
                    >Mistral Batch API</a>
                </h2>

                                    <time datetime="2024-11-07 10:17:26">
                        2024-11-07 10:17:26                    </time>
                    <p></p>
                
                
                <!-- Intentionally not escaping for html context -->
                <p class=hero-description>Lower cost API for AI builders.</p><div class="content mt-5 justify md-content"><p>This could&rsquo;ve been a tweet, but&mldr; While the AI developer community was faced with several API price hikes in the past few weeks, we’re trying to make sure to continue bringing frontier AI to you at affordable price points. To that end, we’re introducing the batch API, available today on <a href=https://console.mistral.ai/>La Plateforme</a>.</p><p>The batch API introduces a more efficient way to process high-volume requests to Mistral models at 50% lower cost than that of a synchronous API call. If you’re building AI applications where you prioritize volume of data over synchronous responses, the batch API can be an ideal solution. You simply upload your batch file, and once the requests have been processed, download and use the output file. For detailed instructions, check out our <a href=https://docs.mistral.ai/capabilities/batch/>batch API documentation</a>.</p><p>Popular applications for the batch API include customer feedback and sentiment analysis, document summarization and translation in bulk, vector embedding to prepare search indexes, and data labeling.</p><p>The batch API is available for all models served on la Plateforme, and coming soon to our cloud provider partners. Rate of usage is limited to 1 million ongoing requests per workspace.</p><p>Please be sure to let us know what you think, and <a href=https://mistral.ai/contact/>contact us</a> for custom or private deployments.</p></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://mistral.ai//news/mistral-moderation/"
                    >Mistral Moderation API</a>
                </h2>

                                    <time datetime="2024-11-07 10:17:26">
                        2024-11-07 10:17:26                    </time>
                    <p></p>
                
                
                <!-- Intentionally not escaping for html context -->
                <p class=hero-description>We are introducing our new moderation service enabling our users to detect undesirable text content along several policy dimensions.</p><div class="content mt-5 justify md-content"><p>Safety plays a key role in making AI useful. At Mistral AI, we believe that system level guardrails are critical to protecting downstream deployments.That&rsquo;s why we are releasing a new content moderation API. It is the same API that powers the moderation service in Le Chat. We are launching it to empower our users to utilize and tailor this tool to their specific applications and safety standards.</p><p>Over the past few months, we&rsquo;ve seen growing enthusiasm across the industry and research community for new LLM based moderation systems, which can help make moderation more scalable and robust across applications. Our model is an LLM classifier trained to classify text inputs into 9 categories defined below. We are releasing two end-points: one for raw text and one for conversational content. Undesirable content is very specific to a given context, therefore we&rsquo;ve trained our model to classify the last message of conversation within a conversational context. Check out our technical documentation for more information. The model is natively multilingual and in particular trained on Arabic, Chinese, English, French, German, Italian, Japanese, Korean, Portuguese, Russian, Spanish.</p><div style=text-align:center><img src=https://mistral.ai/images/news/moderation/moderation-overview.png alt="Detailed benchmarks" width=100% class=center></div><p>The Content Moderation classifier leverages the most relevant policy categories for effective guardrails and introduces a pragmatic approach to LLM safety by addressing model-generated harms such as unqualified advice and PII. The full set of policy definitions and details on how to get started are available in our <a href=https://docs.mistral.ai/capabilities/guardrailing/>technical documentation</a>.</p><h4 id=performance>Performance</h4><p>We are sharing AUC PR across policies on our internal testset below.</p><div style=text-align:center><img src=https://mistral.ai/images/news/moderation/moderation-performance.png alt="Detailed benchmarks" width=70% class=center></div><p>We&rsquo;re working with our customers to build and share scalable, lightweight and customizable moderation tooling, and will continue to engage with the research community to contribute safety advancements to the broader field.</p></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://mistral.ai//news/ministraux/"
                    >Un Ministral, des Ministraux</a>
                </h2>

                                    <time datetime="2024-10-16 10:17:26">
                        2024-10-16 10:17:26                    </time>
                    <p></p>
                
                
                <!-- Intentionally not escaping for html context -->
                <p class=hero-description>Introducing the world’s best edge models.</p><div class="content mt-5 justify md-content"><h2 id=introducing-the-worlds-best-edge-models>Introducing the world’s best edge models</h2><p>On the first anniversary of the release of Mistral 7B, the model that revolutionized independent frontier AI innovation for millions, we are proud to introduce two new state-of-the-art models for on-device computing and at-the-edge use cases. We call them les Ministraux: Ministral 3B and Ministral 8B.</p><p>These models set a new frontier in knowledge, commonsense, reasoning, function-calling, and efficiency in the sub-10B category, and can be used or tuned to a variety of uses, from orchestrating agentic workflows to creating specialist task workers. Both models support up to 128k context length (currently 32k on vLLM) and Ministral 8B has a special interleaved sliding-window attention pattern for faster and memory-efficient inference.</p><h3 id=use-cases>Use cases</h3><p>Our most innovative customers and partners have increasingly been asking for local, privacy-first inference for critical applications such as on-device translation, internet-less smart assistants, local analytics, and autonomous robotics. Les Ministraux were built to provide a compute-efficient and low-latency solution for these scenarios. From independent hobbyists to global manufacturing teams, les Ministraux deliver for a wide variety of use cases.</p><p>Used in conjunction with larger language models such as Mistral Large, les Ministraux are also efficient intermediaries for function-calling in multi-step agentic workflows. They can be tuned to handle input parsing, task routing, and calling APIs based on user intent across multiple contexts at extremely low latency and cost.</p><h3 id=benchmarks>Benchmarks</h3><p>We demonstrate the performance of les Ministraux across multiple tasks where they consistently outperform their peers. We re-evaluated all models with our internal framework for fair comparison.</p><h4 id=pretrained-models>Pretrained Models</h4><div style=text-align:center><img src=https://mistral.ai/images/news/ministraux/pretrain_table.png alt="Pretrained model comparison table" width=80% class=center></div><p style=text-align:center;font-style:italic><b>Table 1:</b> Ministral 3B and 8B models compared to Gemma 2 2B, Llama 3.2 3B, Llama 3.1 8B and Mistral 7B on multiple categories</p><div style=text-align:center><img src=https://mistral.ai/images/news/ministraux/pretrain_with_gemma.png alt="Pretrained model comparison graph" width=80% class=center></div><p style=text-align:center;font-style:italic><b>Figure 1:</b> Ministral 3B and 8B base models compared to Gemma 2 2B, Llama 3.2 3B, Llama 3.1 8B and Mistral 7B</p><h4 id=instruct-models>Instruct Models</h4><div style=text-align:center><img src=https://mistral.ai/images/news/ministraux/instruct_table_with_gemma.png alt="Instruct model comparison table" width=80% class=center></div><p style=text-align:center;font-style:italic></p><p style=text-align:center;font-style:italic><b>Table 2:</b> Ministral 3B and 8B Instruct models compared to Gemma 2 2B, Llama 3.2 3B, Llama 3.1 8B, Gemma 2 9B and Mistral 7B on different evaluation categories.</p><div style=text-align:center><img src=https://mistral.ai/images/news/ministraux/instruct_plot_3b_no_qwen_with_mistral_logo.png alt="3B Instruct model comparison graph" width=80% class=center></div><p style=text-align:center;font-style:italic><b>Figure 2:</b> A comparison of the 3B family of Instruct models - Gemma 2 2B, Llama 3.2 3B and Ministral 3B. The figure showcases the improvements of Ministral 3B over the much larger Mistral 7B.</p><div style=text-align:center><img src=https://mistral.ai/images/news/ministraux/instruct_plot_8b_with_mistral_logo.png alt="8B Instruct model comparison graph" width=80% class=center></div><p style=text-align:center;font-style:italic><b>Figure 3:</b> A comparison of the 8B family of Instruct models - Gemma 2 9B, Llama 3.1 8B, Mistral 7B and Ministral 8B.</p><h3 id=availability-and-pricing>Availability and pricing</h3><p>Both models are available starting today.</p><table><thead><tr><th>Model</th><th>API</th><th>Pricing on la Plateforme</th><th>License</th></tr></thead><tbody><tr><td>Ministral 8B</td><td>ministral-8b-latest</td><td>$0.1 / M tokens (input and output)</td><td>Mistral Commercial License<br>Mistral Research License</td></tr><tr><td>Ministral 3B</td><td>ministral-3b-latest</td><td>$0.04 / M tokens (input and output)</td><td>Mistral Commercial License</td></tr></tbody></table><p>For self-deployed use, <a href=https://mistral.ai/contact/>please reach out to us</a> for commercial licenses. We will also assist you in lossless quantization of the models for your specific use-cases to derive maximum performance.</p><p>The model weights for <a href=https://huggingface.co/mistralai/Ministral-8B-Instruct-2410>Ministral 8B Instruct</a> are available for research use. Both models will be available from our <a href=https://docs.mistral.ai/deployment/cloud/overview/>cloud partners</a> shortly.</p><h3 id=more-to-come>More to come</h3><p>At Mistral AI, we continue pushing the state-of-the-art for frontier models. It’s been only a year since the release of Mistral 7B, and yet our smallest model today (Ministral 3B) already outperforms it on most benchmarks. We can’t wait for you to try out les Ministraux and give us feedback.</p><div style=text-align:center><img src=https://mistral.ai/images/news/ministraux/meme_cropped.png alt="More to come" width=50% class=center></div><p style=text-align:center;font-style:italic></p></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://mistral.ai//news/september-24-release/"
                    >AI in abundance</a>
                </h2>

                                    <time datetime="2024-09-17 10:17:27">
                        2024-09-17 10:17:27                    </time>
                    <p></p>
                
                
                <!-- Intentionally not escaping for html context -->
                <p class=hero-description>Introducing a free API, improved pricing across the board, a new enterprise-grade Mistral Small, and free vision capabilities on le Chat.</p><div class="content mt-5 justify md-content"><p>We&rsquo;re taking new steps in our mission to bring frontier AI in the hands of everyone. Today, we are releasing:</p><ul><li>A free tier on la Plateforme</li><li>A pricing update over our entire family of models</li><li>A new, better Mistral Small</li><li>Free vision capabilities on le Chat with Pixtral 12B</li></ul><h3 id=free-tier-on-la-plateforme>Free tier on la Plateforme</h3><p><a href=https://console.mistral.ai/>La Plateforme</a>, the serverless platform to tune and build with Mistral models as API endpoints, now offers a free tier enabling developers to get started with experimentation, evaluation, and prototyping at no cost. Users can seamlessly evolve their endpoints into a commercial tier, and benefit from full data isolation (with a free zero-retention option) and higher rate limits. Users can also choose to deploy our models to different infrastructure: whether using our cloud partners (Azure / AWS / GCP), or choosing to deploy our solutions on their own <a href=https://mistral.ai/contact/>tenant</a>.</p><h3 id=reduced-prices-across-the-board>Reduced prices across the board</h3><p>We’ve worked hard on making our endpoints faster and more efficient. This enables us to reduce prices across the board, with the following prices</p><table><thead><tr><th>Model</th><th>New price</th><th>Old price</th><th>Price drop</th></tr></thead><tbody><tr><td>Mistral Nemo</td><td>$0.15 / M input tokens</td><td>$0.3 / M tokens</td><td>50%</td></tr><tr><td></td><td>$0.15 / M output tokens</td><td>$0.3 / M tokens</td><td></td></tr><tr><td>Pixtral 12B</td><td>$0.15 / M input tokens</td><td></td><td></td></tr><tr><td></td><td>$0.15 / M output tokens</td><td></td><td></td></tr><tr><td>Mistral Small</td><td>$0.2 / M input tokens</td><td>$1 / M input tokens</td><td>80%</td></tr><tr><td></td><td>$0.6 / M output tokens</td><td>$3 / M output tokens</td><td></td></tr><tr><td>Codestral</td><td>$0.2 / M input tokens</td><td>$1 / M input tokens</td><td>80%</td></tr><tr><td></td><td>$0.6 / M output tokens</td><td>$3 / M output tokens</td><td></td></tr><tr><td>Mistral Large</td><td>$2 / M input tokens</td><td>$3 / M input tokens</td><td>33%</td></tr><tr><td></td><td>$6 / M output tokens</td><td>$9 / M output tokens</td><td></td></tr></tbody></table><p>This price update makes Mistral Large 2 the most cost-efficient frontier model, make our smaller models extremely cost efficient, and allows customers to realize significantly faster returns on their AI investments. Updated pricing will also reflect on our cloud platform partner offerings (Azure AI Studio, Amazon Bedrock, Google Vertex AI).</p><h3 id=small-gets-a-big-update>Small gets a big update</h3><p>We are proud to unveil Mistral Small v24.09, our latest enterprise-grade small model, an upgrade of Mistral Small v24.02. Available under the Mistral Research License, this model offers customers the flexibility to choose a cost-efficient, fast, yet reliable option for use cases such as translation, summarization, sentiment analysis, and other tasks that do not require full-blown general purpose models.</p><p>With 22 billion parameters, Mistral Small v24.09 offers customers a convenient mid-point between Mistral NeMo 12B and Mistral Large 2, providing a cost-effective solution that can be deployed across various platforms and environments. As shown below, the new small model delivers significant improvements in human alignment, reasoning capabilities, and code over the previous model.</p><div style=text-align:center><img src=https://mistral.ai/images/news/Small-Release.png alt="Detailed benchmarks" width=40%> <img src=https://mistral.ai/images/news/Small-Release2.png alt="Detailed benchmarks" width=40%></div><p>We’re releasing Mistral Small v24.09 under the MRL license. You may self-deploy it for non-commercial purposes, using e.g. <a href=https://docs.mistral.ai/deployment/self-deployment/vllm/>vLLM</a></p><h3 id=eye-of-the-tiger---pixtral-on-le-chat>Eye of the Tiger - Pixtral on le Chat</h3><p>Following our latest Apache model release, <a href=https://mistral.ai/news/pixtral-12b/>Pixtral 12B</a>, a vision-capable model with image understanding capabilities, is now freely available on <a href=https://chat.mistral.ai/>le Chat</a>. Pixtral 12B is the first open source model to support images of any size without degradation in text-based performance, and you can now use it on le Chat to scan, analyze, search, caption, and better understand your personal or enterprise knowledge files.</p><p>Importantly, the model is available under the Apache 2.0 license, so you can bring visual understanding capabilities to your own environment without having to upload your files to a third-party provider. This is a critical capability for customers that operate with sensitive or proprietary information.</p><h3 id=do-more-with-less>Do more with less</h3><p>All the above announcements are now available. Head over to <a href=https://chat.mistral.ai/>le Chat</a> to try the new image understanding capabilities. To try the free tier of la Plateforme, sign in at <a href=https://console.mistral.ai>console.mistral.ai</a>. To learn more about Mistral Small v24.09, Pixtral 12B, and other Mistral models and pricing, click <a href=https://mistral.ai/technology/>here</a>.</p></div>
                
                            </section>
        
    </div>
 </body>
</html>
