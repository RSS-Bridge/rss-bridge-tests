<!DOCTYPE html>
<html lang="en">
<head><base href="https://rss-bridge.org/bridge01/" target="_blank">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/ >
    <meta name="description" content="RSS-Bridge" />
    <title>ARM Community - AI</title>
    <link href="static/style.css?2023-03-24" rel="stylesheet">
    <link rel="icon" type="image/png" href="static/favicon.png">

    
        <link
            href="?action=display&amp;bridge=ARMCommunityBridge&amp;context=Blog&amp;community=ai-blog&amp;format=Atom"
            title="Atom"
            rel="alternate"
            type="application/atom+xml"
        >
	
        <link
            href="?action=display&amp;bridge=ARMCommunityBridge&amp;context=Blog&amp;community=ai-blog&amp;format=Json"
            title="Json"
            rel="alternate"
            type="application/json"
        >
	
        <link
            href="?action=display&amp;bridge=ARMCommunityBridge&amp;context=Blog&amp;community=ai-blog&amp;format=Mrss"
            title="Mrss"
            rel="alternate"
            type="application/rss+xml"
        >
	
        <link
            href="?action=display&amp;bridge=ARMCommunityBridge&amp;context=Blog&amp;community=ai-blog&amp;format=Plaintext"
            title="Plaintext"
            rel="alternate"
            type="text/plain"
        >
	
        <link
            href="?action=display&amp;bridge=ARMCommunityBridge&amp;context=Blog&amp;community=ai-blog&amp;format=Sfeed"
            title="Sfeed"
            rel="alternate"
            type="text/plain"
        >
	
    <meta name="robots" content="noindex, follow">
</head>

<body>
    <div class="container">

        <h1 class="pagetitle">
            <a href="https://developer.arm.com" target="_blank">ARM Community - AI</a>
        </h1>

        <div class="buttons">
            <a href="./#bridge-ARMCommunityBridge">
                <button class="backbutton">← back to rss-bridge</button>
            </a>

                            <a href="?action=display&amp;bridge=ARMCommunityBridge&amp;context=Blog&amp;community=ai-blog&amp;format=Atom">
                    <button class="rss-feed">
                        Atom                    </button>
                </a>
                            <a href="?action=display&amp;bridge=ARMCommunityBridge&amp;context=Blog&amp;community=ai-blog&amp;format=Json">
                    <button class="rss-feed">
                        Json                    </button>
                </a>
                            <a href="?action=display&amp;bridge=ARMCommunityBridge&amp;context=Blog&amp;community=ai-blog&amp;format=Mrss">
                    <button class="rss-feed">
                        Mrss                    </button>
                </a>
                            <a href="?action=display&amp;bridge=ARMCommunityBridge&amp;context=Blog&amp;community=ai-blog&amp;format=Plaintext">
                    <button class="rss-feed">
                        Plaintext                    </button>
                </a>
                            <a href="?action=display&amp;bridge=ARMCommunityBridge&amp;context=Blog&amp;community=ai-blog&amp;format=Sfeed">
                    <button class="rss-feed">
                        Sfeed                    </button>
                </a>
            
                    </div>

                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://developer.arm.com/community/arm-community-blogs/b/ai-blog/posts/rethinking-the-role-of-cpus-in-ai-a-practical-rag-implementation-on-dgx-spark"
                    >Rethinking the role of CPUs in AI: A practical RAG implementation on DGX Spark</a>
                </h2>

                                    <time datetime="2025-11-28 00:00:00">
                        2025-11-28 00:00:00                    </time>
                    <p></p>
                
                                    <p class="author">by: odinlmshen</p>
                
                <!-- Intentionally not escaping for html context -->
                <h2 id="mcetoc_1jb2sqrn30">The technical problem: Latency, data fragmentation, and privacy constraints</h2> <p>In many enterprise environments, engineers and technical staff need to find information quickly. They search internal documents such as hardware specifications, project manuals, and technical notes. These materials are often scattered, making traditional search inefficient</p> <p>These documents are often confidential or proprietary. This constraint prevents these documents from being processed by external cloud services or public large language models (LLMs). The challenge is to implement an AI-powered retrieval system that delivers secure, fast, and contextually accurate answers directly on-device.</p> <h2 id="mcetoc_1jb2sqrn31">The architectural solution: Heterogeneous RAG on DGX Spark</h2> <p>GPUs are often considered the default compute workhorse in modern AI pipelines. The AI inference flow involves multiple distinct stages, each with different compute needs. GPUs are optimized for large-scale matrix operations. Other phases, such as query parsing, data retrieval, and vector encoding, require low-latency and high-flexibility processing.</p> <p>This is where the <a target="_blank" rel="noopener noreferrer" href="https://www.nvidia.com/en-gb/products/workstations/dgx-spark/" title="NVIDIA DGX Spark on NVIDIA website.">DGX Spark desktop</a> platform, built on the <a target="_blank" rel="noopener noreferrer" href="https://www.nvidia.com/en-gb/data-center/grace-cpu/" title="Grace CPU overview on NVIDIA website.">Grace</a>-<a target="_blank" rel="noopener noreferrer" href="https://www.nvidia.com/en-gb/data-center/technologies/blackwell-architecture/" title="NVIDIA Blackwell Architecture overview on NVIDIA website">Blackwell</a> GB10 Superchip architecture, provides a foundational advantage:</p> <ul> <li>Heterogeneous Compute: It combines NVIDIA Blackwell-series GPUs with a high-performance Arm CPU complex (with <a target="_blank" rel="noopener noreferrer" href="https://www.arm.com/products/cortex-x" title="Cortex-X overview on Arm website.">Cortex-X</a> and <a target="_blank" rel="noopener noreferrer" href="https://www.arm.com/products/silicon-ip-cpu/cortex-a/cortex-a725" title="Cortex-A725 overview on Arm website">Cortex-A</a> cores). The Arm CPU handles the latency-sensitive text embedding stage in the Retrieval-Augmented Generation (RAG) pipeline.</li> <li>Unified Memory: The system uses a Unified Memory architecture so the CPU and GPU can share a single memory space.</li> </ul> <p></p> <p>With this hardware foundation and software stack, including FAISS and llama.cpp, we show how the CPU transitions from a passive pre-processor to an active, latency-optimized engine that drives responsive local AI.</p> <h2 id="mcetoc_1jb2sqrn32">Architecture and design: Running RAG locally on desktop hardware</h2> <p>Retrieval-Augmented Generation (RAG) is an AI architecture well-suited for querying private, on-premises data. This includes internal contracts, unpublished technical documents, and other sensitive materials either absent from public large language models or unsuitable for processing via external cloud services. The strength of RAG lies in its ability to convert such closed-domain knowledge into a searchable vector database. This enables language models to generate responses locally, so you receive AI-assisted answers while preserving data privacy and ownership.</p> <p>To show how this works in practice, we used a practical and familiar dataset: the full range of Raspberry Pi hardware specifications, programming guides, and application notes. These documents are often long and inconsistently formatted. It is common to spend significant time flipping through PDFs just to find details like GPIO pin assignments, voltage thresholds, or default states.</p> <p>With a localized RAG system, users can enter natural language questions. The system retrieves relevant passages from a local document database and then generates a contextually accurate answer using a language model. The overall query flow can be summarized in three steps:</p> <ol> <li>The user inputs a natural language query.</li> <li>The system searches the local document database for relevant content.</li> <li>The language model generates a natural language response based on the retrieved context.</li> </ol> <p></p> <p>For our platform, we chose the DGX Spark desktop system from <a target="_blank" rel="noopener noreferrer" href="https://www.msi.com/index.php" title="MSI website">MSI</a> <a target="_blank" rel="noopener noreferrer" href="https://ipc.msi.com/product_detail/Industrial-Computer-Box-PC/AI-Supercomputer/EdgeXpert-MS-C931">EdgeXpert</a>, built on the Grace-Blackwell GB10 Superchip architecture. It offers thermal and acoustic advantages suitable for edge AI development.</p> <p>It combines NVIDIA Blackwell-series GPUs with a high-performance Arm CPU complex that includes Cortex-X and Cortex-A cores. It also supports a Unified Memory architecture, so the CPU and GPU share a single memory space. This setup reduces data transfer latency and cuts coordination overhead between compute units.</p> <p>On this hardware foundation, we implemented the RAG system using the following software stack:</p> <ul> <li>FAISS for efficient vector search.</li> <li>llama.cpp for executing quantized language model inference.</li> <li>Python for data processing and orchestrating the overall pipeline logic.</li> </ul> <p></p> <p>With a quantized language model running on llama.cpp and a prebuilt FAISS vector index, the system can run without any model retraining. Once the documents are loaded and embedded, you can deploy a fully local RAG-based query assistant. This setup offers clear practical value in terms of performance, development flexibility, and data privacy. It also shows that desktop-class AI platforms are feasible for real-world applications.</p> <p>With the RAG architecture in place, we can now focus on a key decision in system design: where and how to run the embedding stage for best performance and responsiveness.</p> <h2 id="mcetoc_1jb2sqrn33">Why CPUs are a smarter choice in RAG pipelines</h2> <p>In a RAG architecture, the first stage converts a user’s input query into a vector. This is known as text embedding. This step is critical for overall system accuracy, but its computational profile is very different from the large-scale matrix operations GPUs are built for.</p> <p>In practice, user queries are usually short phrases or single sentences. This makes embedding a low-throughput, latency-sensitive task. Offloading such small batches to the GPU adds unnecessary overhead such as scheduling delays, PCIe transfer latency, and inefficient resource utilization.</p> <p>This is precisely why we chose to run embedding on the CPU and why the Arm-based SoC in the DGX Spark platform proves to be an ideal match.</p> <p>DGX Spark includes high-performance Arm Cortex-X and Cortex-A cores in a heterogeneous CPU architecture. The Cortex-X series supports high-frequency, low-latency operation and delivers excellent multithreaded performance with strong energy efficiency. This makes it well-suited for small-batch, memory-intensive inference tasks like embeddings.</p> <p>Paired with quantized int8 embedding models, these CPUs deliver stable performance under low-power conditions. This ensures fast responses and a smooth interactive experience. For desktop- or edge-class query systems, the Arm Cortex-X architecture is optimized for latency-critical workloads such as real-time search and inference. It balances high single-thread performance with strong power efficiency.</p> <p>To show why this decision matters, let us look at a real-world example where embedding quality directly affects the reliability of the system’s output.</p> <h2 id="mcetoc_1jb2sqrn34">Grounded answers: Eliminating hallucination with RAG</h2> <p>As mentioned in the section above, the first stage involves text embedding. The quality of the RAG system's output depends on how precise that initial embedding is.</p> <p>More importantly, RAG is designed to solve the problem of AI hallucination. When language models lack specific context or access to up-to-date documentation, they often generate plausible-sounding responses that are not grounded in facts. This creates serious risks in technical and enterprise domains.</p> <p>We ran a controlled experiment using a common developer question against internal documentation.</p> <h3 id="mcetoc_1jb2sqrn35">Scenario 1: Language Model (LLM) without RAG</h3> <p>Query: “Which GPIOs on the Raspberry Pi 4 are configured with a default pull-down resistor?”</p> <p>If the raw query is passed directly to the Meta-Llama-3.1-8B model, without RAG or vector retrieval, the results show how unstable an ungrounded LLM can be:</p> <p>Trial 1: The model listed GPIOs 1, 3, 5, 7, 9, 11... as having pull-down resistors.</p> <p>Trial 2 (Same query): The model returned a completely different list (GPIOs 3, 5, 7, 8...) and reclassified pins (for example, GPIO 4, previously listed as pull-up, was classified as pull-down).</p> <p>Observation: The model remained confident in both trials, despite generating contradictory and factually incorrect results.</p> <p><img src="https://community.arm.com/resized-image/__size/2530x0/__key/communityserver-blogs-components-weblogfiles/00-00-00-38-23/llama.cpp_5F00_rpipulllow_5F00_twice.png" alt="Results of controlled experiment using a common developer question against internal documentation" style="height:auto;max-width:100%;"></p> <p>This kind of behavior is not unusual. When language models lack specific context or up-to-date documentation, they often produce plausible-sounding responses that are ultimately ungrounded. This issue is known as hallucination and creates serious risks in technical domains.</p> <h3>Scenario 2: The same query with RAG architecture</h3> <p>Query: “Which GPIOs on the Raspberry Pi 4 are configured with a default pull-down resistor?”</p> <p>When the exact same query was run through our local RAG system, the outcome was completely different:</p> <p>Factually correct: The response matched the official documentation and verified as correct.</p> <p>Traceability: The system explicitly referenced the relevant section and table number (e.g., "Table 5: Power consumption...") from the datasheet.</p> <p><img src="https://community.arm.com/resized-image/__size/2530x0/__key/communityserver-blogs-components-weblogfiles/00-00-00-38-23/llama.cpp_5F00_rag.png" alt="Outcomes of scenario 2 running the same query with a RAG architecture." style="height:auto;max-width:100%;"></p> <p>This highlights one of the core strengths of the RAG architecture. Instead of relying on the language model to “guess” based on its training distribution, RAG grounds each response in retrieved, document-based evidence. This approach helps prevent hallucinations and ensures the model answers with real, verifiable information.</p> <p>From a performance standpoint, the embedding step, handled by the Arm CPU, typically took between 70 to 90 milliseconds. This meets the low-latency requirement for an interactive system.</p> <h2 id="mcetoc_1jb2t8d7j6">Architectural Advantage: Unified Memory</h2> <p>In traditional systems, data generated by the CPU must move across an interconnect, such as PCIe, to the GPU's dedicated memory space. This process adds latency and consumes valuable bandwidth. In contrast, the Unified Memory architecture enables the CPU and GPU to share one memory space. This enables the GPU to access output vectors generated by the CPU without doing explicit data copies or transfers. This creates a faster, more efficient pipeline. This is especially beneficial in real-time AI inference scenarios where every millisecond counts.</p> <p>The following memory profile across RAG stages provides quantitative evidence of this efficiency on the DGX Spark platform.</p> <h2 id="mcetoc_1jb2t8iis7">DRAM usage across RAG execution stages</h2> <table> <thead> <tr> <td> <p>Stage</p> </td> <td> <p>Description</p> </td> <td> <p>Observed DRAM Usage (Approx.)</p> </td> <td> <p>Key Technical Insight</p> </td> </tr> </thead> <tbody> <tr> <td> <p>Idle (Pre-launch)</p> </td> <td> <p>System idle before RAG tasks are initiated</p> </td> <td> <p>~3.5 GiB</p> </td> <td> <p>Baseline usage from OS and background services</p> </td> </tr> <tr> <td> <p>Model Load</p> </td> <td> <p>Launching <span>llama-server</span>, loading LLaMA 3.1 8B Q8_0 model</p> </td> <td> <p>~12 GiB</p> </td> <td> <p>Model weights mapped directly to Unified Memory for shared access.</p> </td> </tr> <tr> <td> <p>Embedding Phase</p> </td> <td> <p>Text chunks embedded on CPU using E5-base-v2 model</p> </td> <td> <p>~13 GiB</p> </td> <td> <p>CPU stores vector data directly into the shared DRAM.</p> </td> </tr> <tr> <td> <p>Query Execution</p> </td> <td> <p>FAISS retrieval followed by GPU-based generation via llama.cpp</p> </td> <td> <p>~14 GiB</p> </td> <td> <p><span>Shared tensors accessed directly by the GPU, eliminating PCIe copy overhead</span>.</p> </td> </tr> <tr> <td> <p>Post-completion</p> </td> <td> <p>Query completes, partial memory cache released</p> </td> <td> <p>~12 GiB</p> </td> <td> <p>Model remains in DRAM, ready for next query</p> </td> </tr> </tbody> </table> <p>The memory profile across these stages provides valuable insight into how well the system manages resources in a unified memory environment.</p> <p>Please note:</p> <p>All performance and memory usage observations reflect our test environment and workload configuration on DGX Spark. Actual results may vary depending on system settings and model size. This profile can serve as a practical reference for planning local AI workloads with predictable memory scaling.<br> <br>These observations lead to several key insights:</p> <ul> <li>Stable memory usage across pipeline stages: DRAM use increased from 3.5 GiB at idle to about 14 GiB during peak RAG activity, an overall rise of only ~10 GiB. This shows highly efficient memory control and resource use.</li> <li>Models and vector data persist in memory: After the model loads and memory rises to 12 GiB, memory usage remains stable even after multiple queries. This confirms the model and embeddings are not evicted from DRAM, allowing reuse without reloads.</li> <li>Minimal memory shifts during CPU-to-GPU transition: Memory usage increased slightly, from 13 GiB during embedding to 14 GiB during GPU generation. This shows that vectors and prompt tensors are directly used by the GPU in-place. No significant reallocations or PCIe transfers occurred.</li> </ul> <p></p> <p>These metrics reinforce the value of Unified Memory in AI system design. Unified Memory simplifies pipeline integration and enables predictable, low-latency inference behavior on desktop and edge platforms.<br>These findings validate the architectural importance of Unified Memory in AI system design. Beyond simplifying development, it enables stable and efficient memory behavior throughout execution. It is an important foundation for AI inference on desktop and edge-class platforms.</p> <p>Developers can find the complete example and step-by-step instructions from this blog post in the <a target="_blank" rel="noopener noreferrer" href="https://learn.arm.com/learning-paths/laptops-and-desktops/dgx_spark_rag" title="Build a RAG pipeline on Arm-based NVIDIA DGX Spark Learning Path">Arm Learning Path</a>. Whether you are building a proof-of-concept or planning a production deployment, these modular tutorials offer a fast and reliable way to work with real RAG workflows. They use CPU-efficient embedding and Unified Memory on Arm-based platforms.</p> <h2 id="mcetoc_1jb2tieij8">Conclusion: The CPU is a critical collaborator in AI system design</h2> <p>Building and running a local RAG system on DGX Spark has reshaped our understanding of the CPU’s role modern AI architectures.</p> <p>In many real-world AI applications, especially those centered around retrieval, search, and natural language interaction, much of the computational workload occurs outside the core language model inference. Tasks like query handling, text embedding, document retrieval, and prompt assembly are the domains where CPUs excel.</p> <p>The implementation on DGX Spark validates two important architectural components:</p> <ol> <li>Latency-optimized processing: The high-performance Arm CPU complex (Cortex-X/A cores) is proven to be the superior choice for handling the low-throughput, latency-sensitive embedding stage. This approach removes unnecessary overhead associated with scheduling small batches on the GPU and ensures fast response times.</li> <li>Seamless compute collaboration: The Unified Memory architecture enables the data generated by the CPU to be accessed by the GPU without manual data transfers or synchronization. This collaboration removes a performance bottleneck and enables each processing unit to focus on its optimized task.</li> </ol> <p></p> <p>The result is that the CPU is not a passive pre-processor but an active, latency-optimized engine that is important for inference responsiveness on desktop platforms.</p> <p>As AI continues to move toward on-device and edge deployments, the role of the CPU, particularly the high-efficiency Arm architecture, will become more central. This project shows that DGX Spark is more than a hardware showcase. It is a fully capable development platform for real-world prototyping.</p> <p>Now is the time for developers to reconsider the CPU’s position as a core enabler in the future of low-latency, privacy-preserving AI systems.</p><div style="clear:both;"></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://developer.arm.com/community/arm-community-blogs/b/ai-blog/posts/build-smarter-and-faster-hands-on-kubernetes-and-ai-workshop-on-google-arm-based-axion-cpus"
                    >Build smarter and faster: Hands-on Kubernetes and AI workshop on Google Arm-based Axion CPUs</a>
                </h2>

                                    <time datetime="2025-10-31 00:00:00">
                        2025-10-31 00:00:00                    </time>
                    <p></p>
                
                                    <p class="author">by: Michael Hall</p>
                
                <!-- Intentionally not escaping for html context -->
                <p>Heading to KubeCon + CloudNativeCon North America? Join <a target="_blank" rel="noopener noreferrer" href="https://developer.arm.com/developer-partners/google-cloud" title="Arm + Google Cloud partnership information">Arm and Google Cloud</a> engineers for a hands-on workshop. You will explore how to build, deploy, and optimize AI workloads across multi-architecture Kubernetes clusters powered by Google’s new Arm-based Axion CPUs.</p> <h2>Where and when</h2> <p>Monday, November 10th, 12–5 p.m. EST</p> <p>Georgia World Congress Center, Atlanta (Room A407)</p> <p style="text-align:left;"><a target="_blank" rel="noopener noreferrer" href="https://events.arm.com/kubecon25northamerica?utm_source=devto&amp;utm_medium=blog&amp;utm_content=event&amp;utm_campaign=mk24_developer_kubecon" title="Register here" class="button cta green">Register here</a></p> <p>Seats are limited. Register early to secure yours.</p> <h2>Why you should join</h2> <p>As multi-architecture computing becomes the norm, developers and platform engineers need to deploy portable, efficient workloads that perform consistently across CPU architectures. In this workshop, we will go beyond diagrams and theory to build and run workloads optimized for Google Arm-based Axion CPUs, built on Arm Neoverse technology.</p> <ul> <li>Run multi-architecture Kubernetes deployments: Deploy microservices seamlessly across x86 and Arm-based nodes using Google Kubernetes Engine (GKE).</li> <li>Build and automate multi-arch container workflows: Explore container image best practices and CI/CD automation for heterogeneous clusters.</li> <li>Accelerate AI inference on CPU: See how Axion’s Arm Neoverse cores enable efficient, low-latency inference directly on CPU—no GPU required.</li> <li>Optimize LLMs and ML models for Arm: Get practical tips on optimizing models for performance and cost efficiency on Arm infrastructure.</li> </ul> <h2>What you will work with</h2> <p>You will walk through a real-world software stack, DevOps orchestration, and explore open-source tooling for optimization, automation, and model deployment. All in a collaborative, hands-on environment. By the end, you will have working examples to adapt to your own infrastructure.</p> <h2>Why it matters</h2> <p>The future of cloud-native AI is multi-architecture, and it is already here. Developers and platform engineers need to build portable, scalable, and energy-efficient applications that can run anywhere. Google Axion CPUs, built on Arm Neoverse technology, combine high performance with superior efficiency to deliver:</p> <ul> <li>Improved cost-to-performance ratios for compute-heavy workloads.</li> <li>Sustainable scaling of AI and microservices.</li> <li>Faster token generation for real-time inference.</li> </ul> <h2>Who should attend</h2> <p>This workshop is ideal for:</p> <ul> <li>Kubernetes platform engineers and SREs managing mixed-architecture clusters.</li> <li>AI/ML engineers deploying models at scale (edge, cloud, or hybrid).</li> <li>Cloud native developers and architects building containerized microservices.</li> <li>DevOps engineers running CI/CD pipelines across heterogeneous environments.</li> </ul> <p></p> <p>If you have ever faced performance inconsistencies between Arm and x86, or wondered how to optimize LLM inference on CPU, this workshop is for you.</p> <h2>Learn. Build. Connect.</h2> <p>Throughout the afternoon, you will have the opportunity to:</p> <ul> <li>Collaborate with other engineers and cloud native developers.</li> <li>Exchange ideas with experts from Arm and Google Cloud.</li> <li>Enjoy lunch, coffee, and informal networking in a relaxed, technical setting.</li> </ul> <p></p> <p>Join us to see how the future of cloud native AI is being built on Arm-powered Google Axion and take your Kubernetes and AI skills to the next level.</p> <p>Reminder: Seats are limited. Register early to secure yours:</p> <p><a target="_blank" rel="noopener noreferrer" href="https://events.arm.com/kubecon25northamerica?utm_source=devto&amp;utm_medium=blog&amp;utm_content=event&amp;utm_campaign=mk24_developer_kubecon" title="Register here" class="button cta green">Register here</a></p><div style="clear:both;"></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://developer.arm.com/community/arm-community-blogs/b/ai-blog/posts/unlocking-the-power-of-neural-camera-denoising-with-arm-sme2"
                    >Smarter smartphone photography: Unlocking the power of neural camera denoising with Arm SME2</a>
                </h2>

                                    <time datetime="2025-10-30 00:00:00">
                        2025-10-30 00:00:00                    </time>
                    <p></p>
                
                                    <p class="author">by: David Packwood</p>
                
                <!-- Intentionally not escaping for html context -->
                <p>Every smartphone photographer has seen it. Images that look sharp in daylight but fall apart in dim lighting. This happens because signal-to-noise ratio (SNR) drops dramatically when sensors capture fewer photons. At 1000 lux, the signal dominates and images look clean. At 1 lux, readout noise appears as grain, color speckles, and loss of fine detail.</p> <p>That is why neural camera denoising is one of the most critical and computationally demanding steps in the camera pipeline. When done well, it transforms noisy frames into sharp, vibrant captures. When done poorly, it leaves smudges and artifacts that ruin the shot.</p> <p><a target="_blank" rel="noopener noreferrer" href="https://www.arm.com/technologies/sme2" title="Overview on Arm dot com of Accelerate On Device AI with Arm SME2">Arm Scalable Matrix Extension 2 (SME2)</a>&nbsp;advances denoising on mobile. It is a powerful new technology for CPU-based AI inference that is enabled across our new C1 CPUs. It is featured in several new flagship smartphones, see the <a href="https://learn.arm.com/learning-paths/cross-platform/multiplying-matrices-with-sme2/1-get-started#devices">device list</a>.</p> <p>SME2 is designed to accelerate a range of AI operations from generative AI to computer vision. It enhances the latest computational photography experiences. This brings automated image improvements for sharper, cleaner images with unprecedented speed and efficiency.</p> <p>In this blog post, we explain how this happens.</p> <h2 id="mcetoc_1j7jqeac10">Scalable Matrix Extensions for imaging innovation</h2> <p>Dedicated image signal processor (ISP) hardware remains highly effective for standard tasks such as denoising, demosaic, and tone mapping. However, imaging algorithms are evolving rapidly, and fixed-function blocks cannot easily adapt.</p> <p>Arm Scalable Matrix Extension 2 (SME2) adds a new layer of flexibility. SME2 combines wide SIMD and matrix-multiply compute capability, enabled by Arm’s SVE2 (Scalable Vector Extension 2) and SME ISA features.<br>This combination enables high-throughput AI and computer vision (CV) acceleration directly into the CPU pipeline. Making it easier to integrate new algorithms without waiting for hardware refreshes.</p> <p><a target="_blank" rel="noopener noreferrer" href="https://newsroom.arm.com/blog/arm-c1-cpu-cluster-on-device-ai-performance" title="Blog post on Arm Newsroom website titled Unleashing Leading On-Device AI Performance and Efficiency with New Arm C1 CPU Cluster">SME2-enabled C1 CPUs</a> enable OEMs and developers to:</p> <ul> <li>Match or exceed DSP-level performance in imaging workloads.</li> <li>Run some applications without using separate AI accelerators, thanks to SME2’s scalable throughput.</li> <li>Benefit from a CPU-like programming model, making it easier for developers to optimize and evolve code.</li> </ul> <p></p> <p>Neural camera denoising on SME2-enabled C1 CPUs</p> <p>Arm has developed a neural camera denoising pipeline purpose-built for SME2. It operates directly in the RAW domain for superior noise modeling and detail retention.</p> <p>It is built from two complementary algorithms:</p> <p>UltraLite</p> <ul> <li>Temporal</li> <li>Downscale, per-channel processing, motion mask estimation, temporal accumulation.</li> <li>Efficient; stabilizes video in low light.</li> </ul> <p></p> <p>CollapseNet</p> <ul> <li>Spatial</li> <li>Cascaded, pyramid-based denoiser (UGGV color space)</li> <li>Superior detail retention in sub-lux conditions</li> </ul> <p></p> <p>When combined, UltraLite and CollapseNet form a spatio-temporal denoising pipeline. UltraLite delivers both temporal stability and CollapseNet restores spatial detail.</p> <p>This combination ensures versatility. UltraLite excels at video, while CollapseNet ensures high-quality stills. Together, they provide robust denoising across the full range of scenarios.</p> <h2 id="mcetoc_1j7jqeac11">Real-time performance on a single core</h2> <p>Neural camera denoising achieves real-time throughput, even when running on a single CPU core with SME2 enabled. The table below shows how SME2-enabled CPUs balance efficiency and flexibility to deliver DSP-class performance without requiring separate accelerators.</p> <table> <tbody> <tr> <td width="156">UltraLite (temporal only)</td> <td width="156">1080p</td> <td width="156">&gt;180fps</td> <td width="156">Lightweight, efficient temporal denoising</td> </tr> <tr> <td width="156">CollapseNet (spatial)</td> <td width="156">4K</td> <td width="156">~30fps</td> <td width="156">High-quality RAW-domain denoising</td> </tr> <tr> <td width="156">Combined (spatio-temporal variant)</td> <td width="156">4k</td> <td width="156">~30fps &nbsp;</td> <td width="156">UltraLite + CollapseNet pipeline for video and stills</td> </tr> </tbody> </table> <h2 id="mcetoc_1j7jqeac12">Programmability and developer tools</h2> <p>Neural camera denoising is implemented as optimized C++ code and includes standalone benchmarking binaries for aarch64 targets. Developers can provide custom inputs, measure performance, and debug with ease. <br>Crucially, SME2 supports Arm C Language Extensions (ACLE) intrinsics. This allows</p> <ul> <li>Low-level tuning of critical kernels, such as convolutions and blending.</li> <li>Familiar workflows, using the same toolchains developers already rely on for Arm CPUs.</li> </ul> <p></p> <p>For experimentation, PyTorch and Keras models are also available. This enables rapid prototyping before deploying optimized implementations.</p> <p>Explore the open-source code in the <a target="_blank" rel="noopener noreferrer" href="https://gitlab.arm.com/kleidi/kleidi-examples/ai-camera-pipelines" title="Link to the KleidiAI Camera Pipelines repository on GitLab">KleidiAI Camera Pipelines repository on GitLab</a>.</p> <h2 id="mcetoc_1j7jqeac13">Results: Extending image quality</h2> <p>Lab evaluations show that SME2-based neural camera denoising improves image quality in the conditions that matter most; 1 lux and below. In these low-light conditions, SME2-based denoising produces sharper, cleaner, and more natural results than ISP-only pipelines or even premium handsets</p> <p>This highlights SME2’s complementary role. It works alongside the ISP, and takes over when fixed-function hardware reaches its limits.</p> <h2 id="mcetoc_1j7jqeac14">Looking ahead</h2> <p>Neural camera denoising is only the beginning. SME2 also accelerates cinematic mode (depth-of-field effects), low-light enhancement, and other advanced camera features. The combination of performance, programmability, and scalability positions SME2 as a general-purpose imaging accelerator. It complements ISPs and enables continuous software innovation.</p> <h2 id="mcetoc_1j7jqeac15">Conclusion</h2> <p>Noise is one of the hardest problems in photography. Low-light conditions push sensors to their limits. SME2-enabled C1 CPU neural camera denoising gives device makers a flexible, high-performance tool to deliver superior low-light imaging. It acts not as a replacement for ISP hardware, but as a complementary capability that extends what cameras can do.</p> <p>SME2 combines ACLE programmability, real-time 4K performance on a single core, and open-source examples available today. Together, these make it a powerful technology for the next generation of computational photography.</p> <p>Importantly, SME2 demonstrates the power of hardware and software algorithm co-design, where silicon capabilities and software techniques evolve together to unlock entirely new imaging possibilities.</p> <p>Try it today with the AI Camera Pipelines on GitLab:</p> <p style="text-align:center;"><a target="_blank" rel="noopener noreferrer" href="https://gitlab.arm.com/kleidi/kleidi-examples/ai-camera-pipelines" title="AI Camera Pipelines repository on GitLab" class="button cta green">AI Camera Pipelines repository on GitLab</a></p><div style="clear:both;"></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://developer.arm.com/community/arm-community-blogs/b/ai-blog/posts/introducing-the-arm-mcp-server-simplifying-cloud-migration-with-ai"
                    >Introducing the Arm MCP Server: Simplifying cloud migration with AI</a>
                </h2>

                                    <time datetime="2025-10-28 00:00:00">
                        2025-10-28 00:00:00                    </time>
                    <p></p>
                
                                    <p class="author">by: Zach Lasiuk</p>
                
                <!-- Intentionally not escaping for html context -->
                <h3 id="mcetoc_1j8j7o22h0">Key takeaways:</h3> <ul> <li>The Arm MCP Server integrates Arm cloud migration tools, documentation, and expert knowledge directly into your AI assistant.</li> <li>Local containerized installation ensures your data remains private—ideal for enterprises.</li> <li>Now Generally Available - <a href="https://developer.arm.com/servers-and-cloud-computing/arm-mcp-server">product homepage here</a>.</li> </ul> <hr> <h2 id="mcetoc_1j8iqetke0">Introduction</h2> <p>Migrating workloads to Arm delivers clear wins such as better performance per watt and lower costs. For many developers, it is not the why that is hard, it is the how. What should you rebuild, what can you reuse, and how do you even start?b</p> <p>Developers often face migration questions like:</p> <ul> <li>Do I need to rebuild my app for Arm compatibility?</li> <li>Which dependencies should I reuse and which should I replace?</li> <li>How do I build multi-architecture containers?</li> <li>What in my CI/CD pipeline needs to change to support Arm?</li> </ul> <p>Arm and our software ecosystem have built a rich collection of <a target="_blank" rel="noopener noreferrer" href="https://developer.arm.com/servers-and-cloud-computing/arm-cloud-migration#tools" title="Overview of how to Migrate Your Workloads to Arm-Based Cloud on the Arm Developer website">tools</a> and <a target="_blank" rel="noopener noreferrer" href="https://developer.arm.com/servers-and-cloud-computing/arm-cloud-migration#tutorials" title="Guides and tutorials: Migrating your workloads to Arm? Dive into workload-specific developer launchpads (guides) and learning paths (tutorials) to guide your workload migration.">tutorials</a> to answer these questions and make migrations easier. However, we know finding what you need, when you need it can be challenging. Context switching from terminal tools to documentation to your AI assistant can slow you down.</p> <p>That is why we built the Arm MCP Server. It brings migration tools, docs, and expert tips directly into your favorite AI Assistant.</p> <h2 id="mcetoc_1j8iqetke0">What is an MCP Server?</h2> <p>The Model Context Protocol (MCP) is an open standard that lets any AI assistant securely connect to external tools and knowledge sources. Announced in 2025, it has quickly become the industry standard for extending AI assistants with real capabilities. Once initialized, you can prompt your AI Assistant using natural language. It will automatically leverage the resources provided through the MCP server.</p> <p>This represents a major shift in AI capabilities. When you integrate the right MCP Servers, your AI Assistants take agentic action with quality information using the tools you provide. This significantly reduces the time and effort it takes to accomplish your goal.</p> <p><img src="https://community.arm.com/resized-image/__size/2530x0/__key/communityserver-blogs-components-weblogfiles/00-00-00-38-23/MCPserverprocess.png" alt="Flow diagram showing the stages of how the MCP Server works" style="height:auto;max-width:100%;"></p> <h2 id="mcetoc_1j8iqjckl0">Why use the Arm MCP Server?</h2> <p>If your goal is migrating codebases or containers from x86 to Arm-based servers, such as AWS Graviton, Google Axion, or Microsoft Azure Cobalt 100-based virtual machines, the Arm MCP Server is the right choice.&nbsp;Developers already use it to make migration faster and easier.</p> <p>See how one developer used the Arm MCP Server to migrate his team’s containerized workloads to Arm while learning new tools and services:</p> <blockquote> <p>"Before, migrating x86 containers to Arm meant constant context switching. Moving across docs, container registries, GitHub Copilot, and command-line tools. Now I’ve got everything right inside my AI assistant thanks to the Arm MCP Server, helping me get work done faster." –Jamshed Damkewala, Principal TPM Manager @ Microsoft</p> </blockquote> <p>In the Arm MCP Server, you will get:</p> <ul> <li><a target="_blank" rel="noopener noreferrer" href="https://github.com/ArmDeveloperEcosystem/sysreport" title="System Capabilities Reporter ReadMe on GitHub">Sysreport</a>: Analyzes your system to ensure it is configured for performance analysis.</li> <li><a target="_blank" rel="noopener noreferrer" href="https://github.com/containers/skopeo" title="Skopeo GitHub repo">Skopeo</a>: Inspects and manipulates container images, both local and remote.</li> <li><a target="_blank" rel="noopener noreferrer" href="https://github.com/migrate-ease/migrate-ease" title="Migrate-ease GitHub repo">Migrate-ease</a>: Analyzes your codebase and suggests changes needed to port your code from x86 to Arm. Currently supports C, C++, Go, Python, Rust, Java, and Dockerfiles.</li> <li><a target="_blank" rel="noopener noreferrer" href="https://learn.arm.com/learning-paths/cross-platform/docker/check-images/" title="Check image chapter of Learn how to use Docker Arm Learning Path course">Check-image</a>: Checks remote container images for Arm architecture support.</li> <li><a target="_blank" rel="noopener noreferrer" href="https://github.com/llvm/llvm-project" title="Machine Code Analyzer (MCA) GitHub repo">Machine Code Analyzer (MCA)</a>: Analyzes performance of compiled code. Part of LLVM.</li> <li>Arm Knowledgebase Search: Provides expert knowledge sourced from the Learning Path tutorial site (<a target="_blank" rel="noopener noreferrer" href="https://learn.arm.com/" title="Link to Arm Learning Paths site where you can access tutorials to help you develop software on Arm.">learn.arm.com</a>), the Arm Ecosystem Dashboard site (<a target="_blank" rel="noopener noreferrer" href="https://developer.arm.com/ecosystem-dashboard/" title="Software Ecosystem Dashboard for Arm For AI, Cloud, Data Center, 5G, Networking and Edge Software Packages">developer.arm.com/ecosystem-dashboard</a>), and beyond.</li> </ul> <p>Today, these existing resources focus on assisting Linux-based migrations. Results may vary with Windows-based applications. We will soon extend support for Windows on Arm apps, IoT systems, Linux server optimization, and beyond.</p> <p>The Arm MCP Server works seamlessly with any AI Assistant that supports MCP Servers. We have successfully used the Arm MCP Server inside of:</p> <ul> <li>GitHub Copilot (VSCode and CLI)</li> <li>AmazonQ (UI and CLI)</li> <li>Claude, Cursor</li> <li>WARP.dev</li> </ul> <p>Feel free to integrate with your favorite AI Assistant.</p> <p>Our lead partners have already started leveraging the Arm MCP server with Github Copilot:</p> <blockquote> <p>“Running the Arm MCP Server entirely through GitHub Copilot showed how seamlessly AI-assisted development can accelerate migration and optimization on arm64. From image validation to code migration and optimization search, Copilot made it effortless to invoke MCP tools, interpret results, and turn them into actionable improvements. The combination delivered quick, reliable answers and proved how well Arm’s ecosystem integrates with modern AI-assisted development.” — Juma Aman, Senior DevOps Consultant, InfoMagnus</p> </blockquote> <p>If you use GitHub Copilot, Arm has partnered with GitHub to deliver the Arm Migration Assistant Custom Agent for Copilot. Read our <a target="_blank" rel="noopener noreferrer" href="https://newsroom.arm.com/blog/arm-cloud-migration-assistant-custom-agent-for-github-copilot" title="Arm Newsroom blog post announcing that Arm has partnered with GitHub to deliver the Arm Migration Assistant Custom Agent for Copilot">parallel announcement blog post</a> for more details.</p> <h2 id="mcetoc_1j8j7o22h1">Start using the Arm MCP Server today</h2> <p>You can start using the Arm MCP Server today with the help of several install guides for AI Assistants and Learning Path tutorials. And this is only the beginning. We are continually improving the Arm MCP Server based on your feedback and will continue to support developers building amazing apps on Arm.</p> <p style="text-align:center;"><a href="https://developer.arm.com/servers-and-cloud-computing/arm-mcp-server" title="Access the Arm MCP Server" class="button cta green">Access the Arm MCP Server</a></p><div style="clear:both;"></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://developer.arm.com/community/arm-community-blogs/b/ai-blog/posts/ethos-u-and-beyond-how-executorch-1-0-powers-ai-at-the-edge"
                    >Ethos-U and Beyond: How ExecuTorch 1.0 powers AI at the edge</a>
                </h2>

                                    <time datetime="2025-10-22 00:00:00">
                        2025-10-22 00:00:00                    </time>
                    <p></p>
                
                                    <p class="author">by: Per Åstrand</p>
                
                <!-- Intentionally not escaping for html context -->
                <p style="text-align:center;">This blog post is published on behalf of&nbsp;<span class="NormalTextRun SCXW46389342 BCX0">Per </span><span class="NormalTextRun SpellingErrorV2Themed SCXW46389342 BCX0">Åstrand&nbsp;and&nbsp;</span><span class="NormalTextRun SCXW46389342 BCX0">Fredrik Knutsson</span></p> <hr> <h2 id="mcetoc_1j7peovqe0">Introduction: A new era for embedded AI</h2> <p>AI is getting leaner. No longer confined to the cloud or powerful smartphones, the next generation of intelligence is moving into the tiniest devices: smart sensors, wearables, and industrial systems that run on milliwatts and kilobytes. These environments are unforgiving because every cycle and every byte counts. Bringing modern AI into this space has long required trade-offs between accuracy, efficiency, and developer productivity.</p> <p>With the <a target="_blank" rel="noopener noreferrer" href="https://newsroom.arm.com/news/executorch-1-0-ga-release-edge-ai" title="Arm Newsroom blog post announcing General Availability (GA) release of ExecuTorch 1.0">General Availability (GA) release of ExecuTorch 1.0</a>, those trade-offs are beginning to disappear. As part of the PyTorch ecosystem, ExecuTorch bridges the gap between innovation and embedded deployment, empowering developers to run state-of-the-art models on Arm-based edge devices, from power-efficient microcontrollers paired with Arm Ethos-U NPUs to high-performance industrial solutions built on Arm CPUs.</p> <h2 id="mcetoc_1j7peovqe1">ExecuTorch: Portable, performant, productive</h2> <p>ExecuTorch brings the PyTorch strengths directly to the edge, built on three guiding principles:</p> <ul> <li>Portable: A unified execution stack that targets Arm CPUs, GPUs, and NPUs without rewriting models.</li> <li>Performant: Optimized runtimes and backends ensure models fit and run within the strict limits of embedded devices.</li> <li>Productive: Developers stay within the familiar PyTorch flow, with native lowering and quantization tooling that makes deployment straightforward.</li> </ul> <p>With ExecuTorch 1.0, deploying AI at the edge is not just possible, it is PyTorch end-to-end. No new frameworks, no conversion, just a direct, optimized path from research to production.</p> <h2 id="mcetoc_1j7peovqe2">The Arm advantage: Seamless edge IP support</h2> <h3 id="mcetoc_1j7peovqe3">From PyTorch to the edge: The whole flow</h3> <p>ExecuTorch streamlines the journey from PyTorch models to efficient embedded execution. As shown in the diagram below, models can be exported and lowered through different backend delegates that map onto the full spectrum of Arm edge IPs, from NPUs to Cortex-A and Cortex-M CPUs. This unified stack means developers can begin in PyTorch and seamlessly deploy across diverse Arm-based edge devices with consistent tooling, predictable performance, and a clear path from research to production.</p> <p><img src="https://community.arm.com/resized-image/__size/2530x0/__key/communityserver-blogs-components-weblogfiles/00-00-00-38-23/Arm_5F00_Spotify_5F00_SocialAds_5F00_Reddit_5F00_VersionB_5F00_Artboard-1.png" alt="Diagram showing how Arm's edge IPs are supported via ExecuTorch backend delegation." style="height:auto;max-width:100%;"></p> <p style="text-align:center;">Figure 1. How Arm's edge IPs are supported via ExecuTorch backend delegation.</p> <h3 id="mcetoc_1j7pf04sn5">TOSA: A unified foundation for Arm NPUs</h3> <p>The <a target="_blank" rel="noopener noreferrer" href="https://www.mlplatform.org/tosa/tosa_spec_1_0_0.html" title="TOSA (Tensor Operator Set Architecture) 1.0 specification">TOSA (Tensor Operator Set Architecture) 1.0 specification</a> and tooling, released earlier this year, establishes a consistent foundation for deploying AI workloads across Arm NPUs and <a target="_blank" rel="noopener noreferrer" href="https://newsroom.arm.com/news/arm-announces-arm-neural-technology" title="Arm Newsroom blog post titled 'Arm Neural Technology Delivers Smarter, Sharper, More Efficient Mobile Graphics for Developers'">neural technology</a>. Within ExecuTorch, the TOSA backend ensures predictable behavior by lowering the majority of edge operators (int8 and float32) into a common, portable form. Any operators not yet covered fall back to CPU execution through reference kernels. This unified path makes TOSA the backbone for scalable embedded AI, enabling seamless acceleration across Arm’s NPU family.</p> <h3 id="mcetoc_1j7pf0h526">Ethos-U: Production-ready AI acceleration</h3> <p>ExecuTorch 1.0 delivers production-quality support for the Ethos-U family of NPUs, designed specifically for ultra-low-power AI acceleration. Key highlights include:</p> <ul> <li>~80% operator coverage for edge AI workloads.</li> <li>Support for int8, with int16 experimental.</li> <li>100+ models from popular sources such as TorchVision, TorchAudio, and HuggingFace running end-to-end on Ethos-U.</li> <li>Performance on par with other AI frameworks across representative edge workloads, validating the efficiency of the TOSA legalization and execution flow.</li> </ul> <p>This makes Ethos-U the most complete path today for running advanced PyTorch models at the microcontroller level. For example, transformer-based architectures like the Conformer model can now run on Ethos-U85-based edge devices, something unthinkable just a few years ago.</p> <h3 id="mcetoc_1j7pevnb64">Neural technology with the VGF backend</h3> <p>ExecuTorch 1.0 also introduces support for Arm’s upcoming neural technology that will feature in 2026 Arm GPUs through the new VGF backend. This enables ahead-of-time export and execution of neural networks that will power use cases like Neural Super Sampling (NSS), denoising, and ML-driven rendering on future Arm GPUs. While this is covered in detail in <a href="/community/arm-community-blogs/b/ai-blog/posts/arm-neural-technology-in-executorch-1-0" title="Arm Community bog post titled 'Arm neural technology in ExecuTorch 1.0'">this blog&nbsp;post</a>, the important takeaway is that the same ExecuTorch and TOSA infrastructure supporting Ethos-U today also extends to next-generation neural graphics acceleration. This ensures that developers can start experimenting now and be ready for what is coming next, all in the same Python and PyTorch-based development flow.</p> <h3 id="mcetoc_1j7pf643j7">Cortex-M + CMSIS-NN: Efficient everywhere</h3> <p>Arm’s Cortex-M CPUs are the backbone of the embedded world, shipping in billions of devices annually. With ExecuTorch integrating <a target="_blank" rel="noopener noreferrer" href="https://www.arm.com/technologies/cmsis" title="Common Microcontroller Software Interface Standard (CMSIS) overview">CMSIS-NN</a>, even CPU-only inference benefits from optimized kernels. Accelerated support for Cortex-M is already available, with further improvements underway to expand efficiency across the family.</p> <h3 id="mcetoc_1j7pfc7n78">Cortex-A + XNNPACK: Scaling performance with Arm KleidiAI</h3> <p>For higher-performance Linux-based platforms, ExecuTorch 1.0 brings the same seamless flow to Cortex-A CPUs. Using XNNPACK, tuned by <a target="_blank" rel="noopener noreferrer" href="https://www.arm.com/markets/artificial-intelligence/software/kleidi" title="Overview of Arm KleidiAI on Arm dot com">Arm KleidiAI</a>, developers can achieve peak performance for edge workloads. This allows models to scale naturally from microcontrollers up to Cortex-A–based edge compute without changing the workflow.</p> <h3 id="mcetoc_1j7pff3pg9">Evaluate without hardware</h3> <p>Not every developer has hardware on hand, and getting started with edge AI should not depend on waiting for silicon. Across different Arm backends, developers can evaluate and validate their models even before hardware is available:</p> <ul> <li>Ethos-U: Through <a target="_blank" rel="noopener noreferrer" href="https://www.arm.com/products/silicon-ip-subsystems" title="Arm Corstone subsystems overview on Arm dot com">Arm Corstone subsystems</a>, developers can use the Fixed Virtual Platform (FVP) to emulate Ethos-U targets. From there, the path to deployment is straightforward, moving from FVP to FPGA prototyping, and eventually onto silicon implementations built on Arm Corstone.</li> <li>Neural technology–enabled GPUs: For future GPU acceleration, ExecuTorch provides an <a target="_blank" rel="noopener noreferrer" href="https://github.com/arm/ai-ml-emulation-layer-for-vulkan" title="ai-ml-emulation-layer-for-vulkan GitHub repo">emulation path</a> that mimics the behavior of the hardware, enabling early development and testing long before first silicon arrives.</li> </ul> <p>This layered approach makes it possible to prototype, validate, and refine AI workloads today, without needing physical hardware in hand.&nbsp;This is well aligned with the productivity goals of ExecuTorch.</p> <h3 id="mcetoc_1j7pfjijua">Python-native backends: Hackable and extensible</h3> <p>One of the strengths of the Arm integration in ExecuTorch 1.0 is that the entire flow – from model lowering down to the backend compiler – is implemented in Python. This makes the backends transparent and hackable:</p> <ul> <li>Developers can easily inspect and modify the lowering path.</li> <li>Missing functionality can be added without diving into opaque C++ or proprietary toolchains.</li> <li>Contributions back to the project are straightforward, empowering the community to expand operator support and optimize performance.</li> </ul> <p>This design choice keeps the developer experience close to PyTorch, while still unlocking the efficiency of Arm hardware. It means that if you need to adapt ExecuTorch for your own model, workflow, or hardware configuration, the tools are right there at your fingertips.</p> <p>For deployment, ExecuTorch provides a lightweight C++ runtime that is easy to integrate into any application. The runtime is designed for portability and efficiency, delivering predictable performance on resource-constrained devices while keeping the deployment footprint small. Together, the Python development flow and the C++ runtime create a seamless bridge from experimentation to production.</p> <p><img src="https://community.arm.com/resized-image/__size/2530x0/__key/communityserver-blogs-components-weblogfiles/00-00-00-38-23/Arm_5F00_Spotify_5F00_SocialAds_5F00_Reddit_5F00_VersionB_5F00_Artboard-2.png" alt="Diagram showing that ExecuTorch has an efficient C++ based runtime and a hackable AoT comilation flow." style="height:auto;max-width:100%;"></p> <p style="text-align:center;">Figure 2. ExecuTorch has an efficient C++-based runtime and a hackable AoT compilation flow.</p> <h2 id="mcetoc_1j7pfnv7jb">A developer journey: From PyTorch to Ethos-U</h2> <p>Here is what an Ethos-U–focused workflow looks like with ExecuTorch:</p> <ol> <li>Start in PyTorch with a model from HuggingFace, TorchAudio, or TorchVision.</li> <li>Quantize and lower directly in Python using ExecuTorch’s native tooling.</li> <li>Deploy on Ethos-U to achieve milliwatt-scale inference performance.</li> <li>Validate on Corstone FVP without hardware or run fallback paths on Cortex-M with CMSIS-NN.</li> </ol> <p>Turning a PyTorch model into a .pte artifact ready for deployment is straightforward. The snippet below captures the representative steps involved.</p> <p></p><pre data-mode="python" class="ui-code"><code class="language-python"># Conformer model with the same hyper parameters as how we have trained it. model = Conformer(num_classes=vocab_size)  dataset = torchaudio.datasets.LIBRISPEECH() # Pick 100 random indexes for calibration calibration_set = torch.utils.data.Subset(dataset, random.sample(range(len(dataset)), 100)) calibration_loader = torch.utils.data.DataLoader(     calibration_set, batch_size=1, shuffle=False, collate_fn=collate_fn )  # Load the checkpoint data for the model weights checkpoint = torch.load(path_to_checkpoint, weights_only=True) model.load_state_dict(checkpoint["model"]) model.eval()  exported_program = torch.export.export(model, example_inputs, strict=True) graph_module = exported_program.module()  compile_spec = EthosUCompileSpec("ethos-u85-256") # Create the quantizer and use the ExecuTorch PT2E flow to quantize the model compile_spec = EthosUCompileSpec("ethos-u85-256") quantizer = EthosUQuantizer(compile_spec) quantizer.set_global(get_symmetric_quantization_config(is_per_channel=True)) quantized_graph_module = prepare_pt2e(graph_module, quantizer)  # Do the post-training quantization calibration using the dataset for feats, feat_lens, *_ in calibration_loader:     feats, feat_lens, *_ = next(         iter(calibration_loader)     )     quantized_graph_module(feats, feat_lens)  # quantization parameters are captured and the model is re-exported quantized_exported_program = torch.export.export(     convert_pt2e(quantized_graph_module), example_inputs, strict=True )  # Create partitioner that delegates the parts it can accelerate to the backend edge_program_manager = executorch.exir.to_edge_transform_and_lower(     quantized_exported_program,     partitioner=[EthosUPartitioner(compile_spec)], ) # Create the artifact representation of the quantized model executorch_program_manager = edge_program_manager.to_executorch(     config=executorch.exir.ExecutorchBackendConfig(extract_delegate_segments=False) ) # And save to disk for deployment on the Ethos-U85 target executorch.exir.save_pte_program(     executorch_program_manager, "conformer_quantized_ethos-u85-256.pte" )</code></pre><p></p> <p><code></code></p> <p>This end-to-end flow demonstrates how ExecuTorch 1.0 makes Ethos-U deployment production-ready, while keeping developers in a familiar and flexible PyTorch environment. For complete examples, please find the <a target="_blank" rel="noopener noreferrer" href="https://docs.pytorch.org/executorch/main/tutorial-arm-ethos-u.html" title="Arm Ethos-U NPU Backend Tutorial on PyTorch website.">ExecuTorch documentation</a> and the <a target="_blank" rel="noopener noreferrer" href="https://github.com/Arm-Examples/ML-examples/tree/main/pytorch-conformer-train-quantize/post_training_quantization" title="PTQ part of the ASR example on GitHub">PTQ part of the ASR example</a>.</p> <h2 id="mcetoc_1j7pfsrfjc">Looking ahead: The next frontier</h2> <p>ExecuTorch 1.0 is just the beginning. The Arm roadmap includes:</p> <ul> <li>Smarter quantization: Dynamic Range Quantization (DRQ) and selective quantization for better accuracy control.</li> <li>Broader CPU acceleration: Expanded CMSIS-NN support to further optimize Cortex-M.</li> <li>Richer examples: New use cases and hands-on tutorials to showcase ExecuTorch in practical deployments.</li> <li>Expanded datatypes: Beyond int8, float32, and experimental int16 support, to the full range of TOSA extensions.</li> </ul> <p>Together, these advances will make AI on Arm not only more efficient but also more accessible to developers everywhere. ExecuTorch 1.0 already marks a milestone, delivering efficient CPU paths, a seamless PyTorch-to-Arm workflow, and simple prototyping on emulated hardware. The future of AI at the edge is here. Head over to the <a target="_blank" rel="noopener noreferrer" href="https://docs.pytorch.org/executorch/1.0/" title="ExecuTorch Documentation">ExecuTorch</a> site to start deploying your PyTorch models with ExecuTorch today.</p> <h2 id="mcetoc_1j7pfv0vnd">Further reading</h2> <ul> <li><a target="_blank" rel="noopener noreferrer" href="https://newsroom.arm.com/news/executorch-1-0-ga-release-edge-ai" title="Arm Newsroom blog">Arm Newsroom blog</a>.</li> <li><a target="_blank" rel="noopener noreferrer" href="https://docs.pytorch.org/executorch/main/" title="ExecuTorch documentation">ExecuTorch documentation</a>.</li> <li><a target="_blank" rel="noopener noreferrer" href="https://github.com/Arm-Examples/ML-examples/tree/main/pytorch-conformer-train-quantize" title="ASR training example">ASR training example</a> on Ethos-U using ExecuTorch.</li> <li>Learning paths: <ul> <li><a target="_blank" rel="noopener noreferrer" href="https://learn.arm.com/learning-paths/embedded-and-microcontrollers/visualizing-ethos-u-performance" title="Visualize Ethos-U NPU performance with ExecuTorch on Arm FVPs learning path">Visualize Ethos-U NPU performance with ExecuTorch on Arm FVPs</a>.</li> <li><a target="_blank" rel="noopener noreferrer" href="https://learn.arm.com/learning-paths/mobile-graphics-and-gaming/build-llama3-chat-android-app-using-executorch-and-xnnpack/" title="Build an Android chat app with Llama, KleidiAI, ExecuTorch, and XNNPACK learning path">Build an Android chat app with Llama, KleidiAI, ExecuTorch, and XNNPACK</a>.</li> <li><a target="_blank" rel="noopener noreferrer" href="https://learn.arm.com/learning-paths/embedded-and-microcontrollers/rpi-llama3/" title="Run Llama 3 on a Raspberry Pi 5 using ExecuTorch learning path">Run Llama 3 on a Raspberry Pi 5 using ExecuTorch</a>.</li> <li><a target="_blank" rel="noopener noreferrer" href="https://learn.arm.com/learning-paths/embedded-and-microcontrollers/introduction-to-tinyml-on-arm/" title="Introduction to TinyML on Arm using PyTorch and ExecuTorch learning path">Introduction to TinyML on Arm using PyTorch and ExecuTorch</a>.</li> </ul> </li> <li><a href="/community/arm-community-blogs/b/ai-blog/posts/arm-neural-technology-in-executorch-1-0" title="Arm Community blog post titled 'Arm neural technology in ExecuTorch 1.0'">Arm Community blog post on ExecuTorch support for Arm neural technology</a></li> <li>ExecuTorch <a target="_blank" rel="noopener noreferrer" href="https://github.com/pytorch/executorch/blob/main/examples/arm/ethos_u_minimal_example.ipynb" title="Ethos-U minimal example notebook">Ethos-U minimal example notebook</a>.</li> </ul><div style="clear:both;"></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://developer.arm.com/community/arm-community-blogs/b/ai-blog/posts/arm-neural-technology-in-executorch-1-0"
                    >Arm neural technology in ExecuTorch 1.0</a>
                </h2>

                                    <time datetime="2025-10-22 00:00:00">
                        2025-10-22 00:00:00                    </time>
                    <p></p>
                
                                    <p class="author">by: Robert Elliott</p>
                
                <!-- Intentionally not escaping for html context -->
                <p><span>With the announcement of&nbsp;<a target="_blank" rel="noopener noreferrer" href="https://newsroom.arm.com/news/arm-announces-arm-neural-technology?utm_source=chatgpt.com" title="Blog post on Arm Newsroom announcing Arm neural technology">Arm neural technology</a>,&nbsp;</span>Arm is enabling neural networks and a new class of neural graphics capabilities to run efficiently on mobile GPUs. Neural Super Sampling (NSS), denoising, and machine learning (ML)-powered rendering enhancements are just the start.</p> <p>Today, we are excited to share that<span> <a target="_blank" rel="noopener noreferrer" href="https://github.com/pytorch/executorch/" title="ExecuTorch GitHub repo">ExecuTorch</a>, </span><span>through the new <a target="_blank" rel="noopener noreferrer" href="https://newsroom.arm.com/news/executorch-1-0-ga-release-edge-ai" title="ExecuTorch General Availability (GA) blog announcement on Arm Newsroom">ExecuTorch General Availability (GA)</a> release, now includes support for Arm Neural Technology through the backend. This backend provides a&nbsp;<a target="_blank" rel="noopener noreferrer" href="https://github.com/pytorch/executorch/blob/main/examples/arm/vgf_minimal_example.ipynb" title="VGF Backend flow example on GitHub">complete ahead-of-time (AOT) export and runtime execution path</a>&nbsp;for a large set of neural networks that will target Arm’s next-generation neural GPU acceleration. It also enables&nbsp;<a target="_blank" rel="noopener noreferrer" href="https://github.com/arm/neural-graphics-for-unreal" title="Neural Super Sampling Unreal® Engine Plugin GitHub repo">export for direct use in game engines</a>.</span></p> <p><span>This builds on the foundations Arm has prepared over the past few years. A key enabler is&nbsp;<a target="_blank" rel="noopener noreferrer" href="https://www.mlplatform.org/tosa/" title="TOSA (Tensor Operator Set Architecture) overview">TOSA (Tensor Operator Set Architecture)</a>, which standardizes ML operators for acceleration on Arm platforms. Thanks to TOSA, ExecuTorch could already target Arm Ethos-U-based devices — and that same infrastructure now extends forward to future neural technology capable hardware, providing:</span></p> <ul> <li>Over 80% edge operator coverage, capturing most major networks seen today.</li> <li>Support for many network architectures, covering neural graphics, convolutional, generative, language models, and more.</li> </ul> <p></p> <p>The TOSA standard provides consistent behavior and high performance across our accelerator technology, be that Ethos-U or the neural technology in our future Arm GPUs. In addition, the suite of open-source software for using TOSA (including compilers, torch.fx passes, and an MLIR dialect) makes it easy to use in an interoperable way with both PyTorch and ExecuTorch.</p> <p>This continues the story we told in earlier blogs: <a href="/community/arm-community-blogs/b/ai-blog/posts/executorch-and-tosa-enabling-pytorch-on-arm-platforms" title="Blog post titled ExecuTorch and TOSA enabling PyTorch on Arm platforms">ExecuTorch and TOSA</a> and <a href="/community/arm-community-blogs/b/ai-blog/posts/executorch-support-ethos-u85" title="Blog post titled 'Getting started with PyTorch, ExecuTorch, and Ethos-U85 in three easy steps'">ExecuTorch support for Ethos-U85</a>.</p> <p><img src="https://community.arm.com/resized-image/__size/2530x0/__key/communityserver-blogs-components-weblogfiles/00-00-00-38-23/Executorch_5F00_1_2D00_0_5F00_GA_5F00_Release_5F00_3.png" alt="Diagram showing Neural Technology Flow" style="height:auto;max-width:100%;"></p> <p>What makes this new support possible is the VGF backend. It introduces an ahead-of-time compilation flow and runtime integration that bridge the gap between PyTorch models and efficient deployment on neural technology capable hardware. The backend provides tooling to export models as portable files and to run them through the ExecuTorch runtime and execute them on a VGF emulator. This makes it possible to develop networks on a standard ML development platform and target any hardware with future Arm GPUs.</p> <h2 id="mcetoc_1j7p601dj0">Getting started</h2> <p>To run this example, you need the following packages:</p> <p></p><pre data-mode="text" class="ui-code"><code class="language-text">pip install executorch ./examples/arm/setup.sh --i-agree-to-the-contained-eula --disable-ethos-u-deps --enable-mlsdk-deps</code></pre><p></p> <p><span>Then the following Python example will produce an exported model:</span></p> <p><span></span></p> <p><span>You can also explore the example models in the Executorch examples/models tree:</span></p> <p><span></span></p><pre data-mode="python" class="ui-code"><code class="language-python">python3 -m examples.arm.aot_arm_compiler -t vgf --delegate --model_name="add" -i ./out_add -o out_add.pte</code></pre><p></p> <p><span>This PTE file can then be executed by building and using the test executor runner:</span></p> <p><span></span></p><pre data-mode="python" class="ui-code"><code class="language-python"># Set up target build environment – host linux with mlsdk emulator ./setup.sh --disable-ethos-u-deps --enable-mlsdk-deps source examples/arm/ethos-u-scratch/setup_path.sh  # Build the ExecuTorch Runtime cmake --preset linux -DEXECUTORCH_BUILD_VULKAN=ON -DEXECUTORCH_BUILD_VGF=ON -DCMAKE_INSTALL_PREFIX=cmake-vgf -Bcmake-vgf cmake --build cmake-vgf -j$(nproc) --target executor_runner  # Run the produced PTE file with the runtime example application ./cmake-vgf/executor_runner -model_path add_module_vgf.pte</code></pre><p></p> <p><span>For further details and many additional options to tailor the flow to your requirements, take a look at our <a target="_blank" rel="noopener noreferrer" href="https://github.com/pytorch/executorch/blob/main/examples/arm/vgf_minimal_example.ipynb" title="ExecuTorch example notebooks on GitHub">ExecuTorch example notebooks</a>.</span></p> <p><span>Better still, these networks can be used <a target="_blank" rel="noopener noreferrer" href="https://github.com/arm/neural-graphics-for-unreal" title="Neural Super Sampling Unreal® Engine Plugin overview on GitHub.">directly in game engines</a> for use cases such as NSS.</span></p> <h2 id="mcetoc_1j7p601dj1"><span>Next steps</span></h2> <p><span>We would like to invite developers to try out the VGF backend today. By doing so, you will be ready to target Arm neural technology as it arrives in upcoming Arm GPU generations.</span></p> <p><span>To help you get started, here are some resources:</span></p> <ul> <li><span>Explore training neural graphics uses cases using the <a target="_blank" rel="noopener noreferrer" href="http://github.com/arm/neural-graphics-model-gym" title="Neural Graphics Model Gym on GitHub">Neural Graphics Model Gym</a>.</span></li> <li><span>Prepare for the next generation of ML hardware with the <a target="_blank" rel="noopener noreferrer" href="https://arm.github.io/ai-ml-sdk-for-vulkan/index.html" title="‘ML SDK for Vulkan®’ documentation on GitHub">AI/ML SDK for Vulkan</a>.</span></li> <li><span>Explore the ExecuTorch Export and Device Runtime flows with <a target="_blank" rel="noopener noreferrer" href="https://github.com/pytorch/executorch/tree/main/examples/arm" title="ExecuTorch Arm backend examples on GitHub">ExecuTorch Arm backend examples</a>.</span></li> <li><span>To keep up with the latest development in ExecuTorch:</span> <ul> <li><span>Visit the <a target="_blank" rel="noopener noreferrer" href="https://github.com/pytorch/executorch/" title="ExecuTorch project on GitHub">ExecuTorch project on GitHub</a>.</span></li> <li><span>See the <a target="_blank" rel="noopener noreferrer" href="https://github.com/pytorch/executorch/tree/release/1.0/backends/arm" title="Arm backend code on GitHub">Arm backend code</a>.</span></li> <li><span>Learn more about the <a target="_blank" rel="noopener noreferrer" href="https://www.mlplatform.org/tosa/" title="TOSA specification">TOSA specification</a>. </span></li> </ul> </li> <li><span>Explore example models:</span> <ul> <li><span><a target="_blank" rel="noopener noreferrer" href="https://github.com/pytorch/executorch/tree/main/examples/models" title="ExecuTorch example models">ExecuTorch example models</a>.</span></li> <li><span><a target="_blank" rel="noopener noreferrer" href="https://github.com/pytorch/executorch/tree/release/1.0/backends/arm/test/models" title="Arm model tests on GitHub">Arm model tests</a>.</span></li> </ul> </li> <li><span>Visualize these models using the TOSA and VGF extensions to model explorer:</span> <ul> <li><span><a target="_blank" rel="noopener noreferrer" href="https://github.com/arm/tosa-adapter-model-explorer" title="TOSA adapter on GitHub">TOSA adapter</a>.</span></li> <li><span><a target="_blank" rel="noopener noreferrer" href="https://github.com/arm/vgf-adapter-model-explorer" title="VGF Adapter for Model Explorer on GitHub">VGF adapter</a>.<br></span></li> </ul> </li> </ul><div style="clear:both;"></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://developer.arm.com/community/arm-community-blogs/b/ai-blog/posts/executorch-1-0-is-here-and-with-sme2-optimizations-through-kleidiai"
                    >ExecuTorch 1.0 is here and with SME2 optimizations through KleidiAI</a>
                </h2>

                                    <time datetime="2025-10-22 00:00:00">
                        2025-10-22 00:00:00                    </time>
                    <p></p>
                
                                    <p class="author">by: Gian Marco Iodice</p>
                
                <!-- Intentionally not escaping for html context -->
                <p>Today marks an exciting milestone with the official <a target="_blank" rel="noopener noreferrer" href="https://newsroom.arm.com/news/executorch-1-0-ga-release-edge-ai" title="Blog post on Arm Newsroom announcing the official general availability (GA) release of ExecuTorch 1.0,">general availability (GA) release of ExecuTorch 1.0</a>, a lightweight, production-ready runtime from the PyTorch ecosystem. This GA release empowers developers to deploy AI models efficiently across a wide variety of devices, unlocking new possibilities in performance, portability, and efficiency. Especially for those building on Arm CPUs now and in the future.</p> <h2>What is ExecuTorch?</h2> <p>ExecuTorch is a framework designed to export and execute PyTorch models on resource-constrained devices. With PyTorch’s standard tooling, developers can export their models and run them natively on edge AI and IoT devices, smartphones, PCs and laptops, or even cloud CPUs using the same model.</p> <h2>Optimized AI on Arm: SME2 and KleidiAI</h2> <p>One of the most exciting additions for developers targeting Arm CPUs is the inclusion of optimized AI routines for <a target="_blank" rel="noopener noreferrer" href="https://www.arm.com/technologies/sme2" title="Overview of Scalable Matrix Extension 2 (SME2) on Arm dot com">Scalable Matrix Extension 2 (SME2)</a> through XNNPACK via the Arm KleidiAI integration.</p> <p>SME2 is a technology introduced in the Armv9-A CPU architecture. It builds on <a target="_blank" rel="noopener noreferrer" href="https://developer.arm.com/documentation/102340/0100/Introducing-SVE2" title="Introducing SVE2 documentation on Arm Developer site">SVE2 (Scalable Vector Extension 2)</a>, extending its versatility with advanced features that enhance performance in areas such as generative AI, computer vision (CV), and linear algebra. All of this while preserving the programmability and flexibility of existing Arm technologies like Neon and SVE/SVE2.</p> <p>At the heart of SME2 lies the MOPA (Matrix Outer Product Accumulate) instruction, which accelerates matrix operations by efficiently performing outer products.</p> <p>For more insights into SME2 and KleidiAI’s integration with XNNPACK, see this detailed <a href="/community/arm-community-blogs/b/ai-blog/posts/arm-kleidiai-in-xnnpack" title="Arm Community blog post titled 'One year of Arm KleidiAI in XNNPack: Seamless and transparent AI performance'">blog post on Arm Community</a>.</p> <p>Thanks to this integration, ExecuTorch automatically leverages SME2-optimized kernels in XNNPACK whenever SME2 is available at runtime. This optimization enhances key operators on Arm CPUs, including:</p> <ul> <li>MatMul f32</li> <li>MatMul f16</li> <li>MatMul int8</li> <li>MatMul int8 (dynamic quantization)</li> </ul> <h2>Demonstrating the power: Stable Audio Open Small on Arm</h2> <p>The full potential of ExecuTorch 1.0 is being showcased at the 2025 PyTorch Conference (22-23 October), where the&nbsp;<a href="/community/arm-community-blogs/b/ai-blog/posts/audio-generation-arm-cpus-stable-audio-open-small-kleidiai" title="Arm Community blog post titled 'Unlocking audio generation on Arm CPUs to all: Running Stable Audio Open Small with KleidiAI'">Stable Audio Open Small</a> text-to-audio model is running entirely on SME2-enabled Arm CPUs via KleidiAI optimizations and delivering remarkable performance. These include:</p> <ul> <li>11 seconds of audio generated in just 7 to 8 seconds on a broad range of Arm-based CPUs.</li> <li>Generation time dropped to under 4 seconds on SME2-enabled devices, like the Mac Mini and MacBook Pro.</li> </ul> <p>This performance highlights how ExecuTorch, combined with Arm’s scalable hardware architecture, achieves exceptional results. Without requiring code changes or hardware-specific tuning.</p> <h2>Optimize once, deploy everywhere</h2> <p>The story is not just about SME2. It is about a fundamental advantage of the Arm ecosystem: “Optimize once, deploy everywhere.”</p> <p>The same model and code run efficiently across the cloud, mobile, and edge. From Arm Neoverse CPUs in data centers to smartphones and embedded systems powered by the range of Arm CPUs.</p> <p>For developers, no modifications are needed, with performance scaling seamlessly with the underlying Arm architecture.</p> <p>The tables below show the Neon-only performance results for Stable Audio Open Small with ExecuTorch and KleidiAI: on mobile devices using 1, 2, and 4 cores, and on the Arm CPU of a Graviton 4 system using 1, 2, 4, 8, and 16 cores.</p> <p>Mobile: 1x @3.25 GHz Arm Cortex-X4 &amp; 3x @2.85 GHz Arm Cortex-X4 CPU.</p> <table> <tbody> <tr> <td width="200"> <p>1 core - s</p> </td> <td width="200"> <p>2 cores - s</p> </td> <td width="200"> <p>4 cores - s</p> </td> </tr> <tr> <td width="200"> <p>16.6</p> </td> <td width="200"> <p>11.6</p> </td> <td width="200"> <p>8.4</p> </td> </tr> </tbody> </table> <p>Cloud (Graviton 4): Arm Neoverse V2 CPU @2.8GHz</p> <table> <tbody> <tr> <td width="120"> <p>1 core - s</p> </td> <td width="120"> <p>2 cores - s</p> </td> <td width="120"> <p>4 cores - s</p> </td> <td width="120"> <p>8 cores - s</p> </td> <td width="120"> <p>16 cores - s</p> </td> </tr> <tr> <td width="120"> <p>17.4</p> </td> <td width="120"> <p>9.2</p> </td> <td width="120"> <p>5.1</p> </td> <td width="120"> <p>3.2</p> </td> <td width="120"> <p>2.2</p> </td> </tr> </tbody> </table> <p>This makes AI development and deployment simpler and faster across all Arm-based devices.</p> <h2>Final thoughts</h2> <p>The release of ExecuTorch 1.0 represents a significant leap forward in enabling efficient, scalable on-device AI for everyone. With SME2 support via KleidiAI integrations, optimized operators, and proven real-world results, it gives developers the power to deploy state-of-the-art AI models across the entire Arm ecosystem.</p> <h3>Resources</h3> <ul> <li><a target="_blank" rel="noopener noreferrer" href="https://pytorch.org/executorch" title="ExecuTorch Landing Page on PyTorch site">ExecuTorch Landing Page on PyTorch site</a>.</li> <li><a target="_blank" rel="noopener noreferrer" href="https://docs.pytorch.org/executorch/1.0/" title="ExecuTorch 1.0 Getting started documentation">ExecuTorch 1.0 Getting started docs</a>.</li> <li><a target="_blank" rel="noopener noreferrer" href="https://pypi.org/project/executorch/" title="ExecuTorch 1.0 download page">ExecuTorch 1.0 download page</a>.</li> <li><a target="_blank" rel="noopener noreferrer" href="https://developer.arm.com/search#q=executorch&amp;f-navigationhierarchiescontenttype=Learning%20Path&amp;numberOfResults=48" title="ExecuTorch on Arm learning paths">ExecuTorch on Arm learning paths</a>.</li> <li><a target="_blank" rel="noopener noreferrer" href="https://www.arm.com/resources/webinar/code-along-android-chat-app-llama-executorch?utm_source=community&amp;utm_medium=blog&amp;utm_content=landingpage&amp;utm_campaign=mk24_developer_code-along" title="Code Along and Expert Q&amp;A: Build an Android Chat App with Llama, Arm KleidiAI, ExecuTorch, and XNNPACK">Arm Code Along and Expert Q&amp;A:&nbsp;<span>Build an Android Chat App with Llama, Arm KleidiAI, ExecuTorch, and XNNPACK</span></a>.</li> </ul><div style="clear:both;"></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://developer.arm.com/community/arm-community-blogs/b/ai-blog/posts/advancing-pytorch-performance-on-arm-key-enhancements-in-the-2-9-release"
                    >Advancing PyTorch Performance on Arm: Key Enhancements in the 2.9 Release</a>
                </h2>

                                    <time datetime="2025-10-15 00:00:00">
                        2025-10-15 00:00:00                    </time>
                    <p></p>
                
                                    <p class="author">by: Ashok Bhat</p>
                
                <!-- Intentionally not escaping for html context -->
                <p style="text-align:center;">This blog post is published on behalf of&nbsp;Aditya Tewari, Nikhil Gupta and Arm’s PyTorch Engineering Team</p> <hr> <p>As part of the new <a target="_blank" rel="noopener noreferrer" href="https://pytorch.org/blog/pytorch-2-9/" title="PyTorch 2.9 release blog post">PyTorch 2.9 release</a>, Arm contributed key enhancements to&nbsp;improve performance and stability on Arm platforms. This includes updated library support via oneDNN and OpenBLAS optimizations and AArch64 reliability fixes. These advances align with the release’s broader focus on expanding hardware compatibility and strengthening deployment reliability across the ecosystem.</p> <h2 id="mcetoc_1j7jjle1h0">Accelerating PyTorch on Arm</h2> <p>The PyTorch 2.9 release brings performance and reliability improvements across the Arm CPU backend</p> <p>We focused on four areas of improvement:</p> <ol> <li>Extended operator coverage and enhanced performance for convolution, activation, and quantized operators on AArch64.</li> <li>Performance uplift using Scalable Vector Extensions (SVE/SVE2).</li> <li>Strengthened TorchInductor and vectorization correctness for Arm backends.</li> <li>Expanded Arm continuous integration (CI) coverage and improved testing infrastructure.</li> </ol> <p>Together, these updates ensure that models running on Arm CPUs benefit from optimized math libraries, improved kernel performance, and consistent compiler behavior.</p> <h2 id="mcetoc_1j7jkd5792">Extending operator coverage and performance</h2> <p>A key focus for this release was expanding and optimizing the set of PyTorch operators available on AArch64. These included:</p> <ul> <li>Convolution and Activation Improvements: Core kernels were tuned for better cache utilization and vectorized math execution, improving convolutional neural network (CNN), and image-based workloads.</li> <li>Quantized Operator Expansion: Enhanced quantized operator implementations improve inference speed.</li> <li>Runtime Consistency and Correctness: Updates across the operator stack ensure consistent numerical results between eager and compiled execution paths.</li> </ul> <h2 id="mcetoc_1j7jkepuu3">Boosting math performance with SVE and SVE2</h2> <p>Arm’s SVE and SVE2 enable flexible vector lengths, allowing code to scale efficiently across different CPUs. PyTorch 2.9 introduces optimizations that better leverage this hardware capability:</p> <ul> <li>Enhanced vectorized math routines in oneDNN and OpenBLAS.</li> <li>Optimized GEMM and reduction kernels.</li> <li>Better vector alignment and correctness handling in TorchInductor.</li> </ul> <p>This improves throughput and efficiency across transformer, CNN, and mixed-precision workloads.</p> <h2 id="mcetoc_1j7mft4dc1">Compiler and runtime enhancements for Arm</h2> <p><span data-contrast="none">PyTorch 2.9 includes multiple compiler and runtime updates</span><span>:</span><span>&nbsp;</span></p> <ul> <li data-aria-level="1" data-aria-posinset="1" data-listid="10" data-font="Symbol"><span data-contrast="none">AOTInductor Vectorization: Improved automatic vectorization and graph lowering for Arm architectures.</span></li> <li data-aria-level="1" data-aria-posinset="1" data-listid="10" data-font="Symbol"><span data-contrast="none">Optimized Runtime Configurations: Runtime backend cpp_wrapper was tuned for better scheduling and reduced CPU overhead.</span><span>&nbsp;</span></li> <li data-aria-level="1" data-aria-posinset="1" data-listid="10" data-font="Symbol"><span data-contrast="none">Mixed-Precision Handling: Improved fusion and precision handling in compiler passes ensure consistent results and faster execution.</span><span>&nbsp;</span></li> </ul> <p><span data-contrast="none">These updates make PyTorch’s compiler stack more robust for Arm targets, providing more general optimizations. Transformer-based models such as </span><span>BERT</span> <span data-contrast="none">and Llama, which spend over 40% of their runtime in GEMM operations (e.g. torch.Linear) see </span><span>up to </span><span>2.5×</span> <span data-contrast="none">speedup</span><span>.</span></p> <h2 id="mcetoc_1j7jkj7v94">Ecosystem integration and continuous testing</h2> <p>Long-term reliability and maintainability are ensured through stronger integration and validation in PyTorch’s continuous integration (CI) and testing infrastructure.</p> <ul> <li>Expanded CI Coverage: CI now includes Neoverse V2 based AWS Graviton 4 instances.</li> <li>Smarter Tensor Handling: Better test coverage for edge cases ensures correctness across various tensor shapes and datatypes.</li> <li>Upgraded Math Libraries: OpenBLAS 0.3.30 introduces architecture-aware optimizations for matrix operations, boosting linear algebra performance. As a result, TorchBench, HuggingFace, and TIMM test suites in torch.compile mode are faster than Eager mode.</li> </ul> <p><img src="https://community.arm.com/resized-image/__size/2530x0/__key/communityserver-blogs-components-weblogfiles/00-00-00-38-23/4721.PyTorchUpliFTcOMP.png" alt="Graph showing relative performance uplift on Arm with PyTorch 2.9" style="height:auto;max-width:100%;"></p> <p><span class="TrackedChange SCXW204879933 BCX0"><span data-contrast="none" lang="EN-US" class="TextRun SCXW204879933 BCX0"><span class="NormalTextRun SCXW204879933 BCX0">Detailed </span></span></span><span class="TrackedChange SCXW204879933 BCX0"><span data-contrast="none" lang="EN-US" class="TextRun SCXW204879933 BCX0"><span class="NormalTextRun SCXW204879933 BCX0">results </span></span></span><span class="TrackedChange SCXW204879933 BCX0"><span data-contrast="none" lang="EN-US" class="TextRun SCXW204879933 BCX0"><span class="NormalTextRun SCXW204879933 BCX0">are available </span></span></span><span class="TrackedChange SCXW204879933 BCX0"><span data-contrast="none" lang="EN-US" class="TextRun SCXW204879933 BCX0"><span class="NormalTextRun SCXW204879933 BCX0">on the </span></span></span><span data-contrast="auto" lang="EN-US" class="TextRun EmptyTextRun SCXW204879933 BCX0"></span><a target="_blank" rel="noopener noreferrer" href="https://hud.pytorch.org/benchmark/compilers?dashboard=torchinductor&amp;startTime=Sat%2C%2014%20Jun%202025%2014%3A09%3A51%20GMT&amp;stopTime=Fri%2C%2012%20Sep%202025%2014%3A09%3A51%20GMT&amp;granularity=week&amp;mode=inference&amp;dtype=bfloat16&amp;deviceName=cpu%20(aarch64)&amp;lBranch=main&amp;lCommit=6b59a19242e0862563bfe6b595f7db3ef44ade7f&amp;rBranch=main&amp;rCommit=655b3b14ffba4ae73e26a63b4289329e8d160a6f" title="PyTorch HUD Dashboard" class="Hyperlink TrackedChange SCXW204879933 BCX0"><span data-contrast="none" lang="EN-US" class="TextRun Underlined EmptyTextRun SCXW204879933 BCX0"></span><span class="SCXW204879933 BCX0"><span class="TrackedChange SCXW204879933 BCX0"><span data-contrast="none" lang="EN-US" class="TextRun Underlined SCXW204879933 BCX0"><span data-ccp-charstyle="Hyperlink" class="NormalTextRun SCXW204879933 BCX0">PyTorch HUD Dashboard</span></span></span></span></a>.</p> <h2 id="mcetoc_1j7jkkrjo6">Looking ahead</h2> <p><span data-contrast="none">With PyTorch 2.9, Arm and the PyTorch community continue to demonstrate how to deliver high-performance AI on Arm CPUs.</span><span>&nbsp;</span></p> <p><span data-contrast="none">Future work will focus on:</span><span>&nbsp;</span></p> <ul> <li data-aria-level="1" data-aria-posinset="1" data-listid="12" data-font="Symbol"><span data-contrast="none">Expanding operator-level optimizations with deeper SVE/SVE2 integration.</span></li> <li data-aria-level="1" data-aria-posinset="1" data-listid="12" data-font="Symbol"><span data-contrast="none">Enhancing TorchInductor’s scheduling and code generation on the Arm architecture.</span></li> <li data-aria-level="1" data-aria-posinset="1" data-listid="12" data-font="Symbol"><span data-contrast="none">Further strengthening Arm-native CI coverage to maintain upstream reliability.</span><span>&nbsp;</span></li> </ul> <p><span data-contrast="none">As the PyTorch ecosystem continues to evolve, Arm remains committed to enabling open, efficient, and scalable AI performance across the global developer community.</span><span>&nbsp;</span></p><div style="clear:both;"></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="https://developer.arm.com/community/arm-community-blogs/b/ai-blog/posts/are-you-attending-pytorch-conference-2025"
                    >Are you attending PyTorch Conference 2025?</a>
                </h2>

                                    <time datetime="2025-10-15 00:00:00">
                        2025-10-15 00:00:00                    </time>
                    <p></p>
                
                                    <p class="author">by: Michelle Yung</p>
                
                <!-- Intentionally not escaping for html context -->
                <p>Join us on site October 22-23 to learn how Arm empowers developers to build and deploy AI applications easily using PyTorch and ExecuTorch. Discover the latest Arm and ecosystem AI technologies, and connect with other AI engineers to grow your network.</p> <h2 id="mcetoc_1j7ja44m90">Connect, chat, chill</h2> <p>Start the conference with an evening of food, drinks, and good conversation. Whether you want to relax or network, you will be in good company with fellow AI engineers.</p> <p>This pre-conference gathering offers a relaxed setting to meet Arm experts and AI professionals, share experiences, and make valuable connections before the formal sessions begin. The event will include a variety of delicious refreshments to enjoy, creating a welcoming atmosphere for both casual mingling and engaging discussions. This evening promises a memorable and enjoyable start to the conference experience.</p> <p style="text-align:center;"><a target="_blank" rel="noopener noreferrer" href="https://events.arm.com/pytorch2025?utm_source=community&amp;utm_medium=blog&amp;utm_content=event&amp;utm_campaign=mk24_developer_pytorch" title="Join our meetup" class="button cta green">Join our meetup</a></p> <h2 id="mcetoc_1j7ja44m91">Strengthen your AI product</h2> <p>We are offering one-on-one workshop sessions with design experts to help improve the usability of your product. Focus on responsible AI development with best practices like Yellow Teaming to help avoid unintended issues before they arise.</p> <p>Each workshop is tailored to address the unique challenges and goals of your AI projects. Work directly with Arm experts and design professionals to receive actionable guidance on refining user interfaces, streamlining user experiences, and implementing intuitive workflows.</p> <p>In addition to usability enhancements, the sessions emphasize responsible AI practices. Our experts will introduce Yellow Teaming, a proactive approach to identify, assess, and mitigate potential risks and consequences in AI systems before deployment. Through hands-on activities and scenario-based discussions, you will learn how to build trustworthiness and safety into your solutions from the ground up. Helping you anticipate issues related to fairness, transparency, privacy, and security.</p> <p>Whether you are an AI engineer, product manager, or developer, these workshops help you strengthen your skills and expand your product’s impact in a rapidly changing landscape. Join us to gain fresh insights, practical tools, and the confidence to deliver exceptional AI-powered experiences responsibly. <a target="_blank" rel="noopener noreferrer" href="https://events.arm.com/pytorchworkshop2025?utm_source=community&amp;utm_medium=blog&amp;utm_content=event&amp;utm_campaign=mk24_developer_pytorch" title="AI Product Workshop">Sign up here</a>.</p> <p></p> <h2 id="mcetoc_1j7ja44m92">Shape the future of AI on Arm</h2> <p>Join our Voice of the Developer sessions. They are focused 30-minute one-on-one conversations designed to understand how developers build, deploy, and scale AI across cloud, edge, and mobile platforms.</p> <p>These sessions offer engineers the opportunity to share firsthand experiences. Such as migrating from NVIDIA/x86 to Arm, profiling or debugging models at scale, and running LLMs reliably at the edge. The insights gathered will directly inform how Arm evolves its next generation of AI tools, SDKs, and documentation to better align with real-world development workflows.</p> <p>By contributing your perspective, you can help ensure that Arm’s AI platforms continue to meet the practical needs of those who build on them, from research to production.</p> <p>Sessions are available throughout the conference on a first-come, first-served basis at the Arm booth. <a target="_blank" rel="noopener noreferrer" href="https://events.arm.com/pytorchvoiceofthedeveloper2025?utm_source=community&amp;utm_medium=blog&amp;utm_content=event&amp;utm_campaign=mk24_developer_pytorch" title="Voice of the Developer Workshop">Sign up here</a>.</p> <p></p> <h2 id="mcetoc_1j7ja44m93">Visit us at the Arm Booth and join our talks!</h2> <p>Visit Booth P1 to explore interactive demos, including training neural graphics, running audio generation and speech recognition workloads, and exploring agentic AI workflows. With the latest technologies like vLLM and Mixture of Experts in the cloud and ExecuTorch for mobile, gaming, and Edge AI, see the latest Arm-optimized use cases in action on the latest developer platforms.</p> <p>Also be sure to check out our insightful talks and “Birds of a Feather” sessions:</p> <p></p> <table> <tbody> <tr> <td></td> <td></td> </tr> <tr> <td> <h3 data-component="Primary" style="text-align:center;" class="c-image-above-text-card__primary u-text-lg-3 u-text-md-3 u-text-sm-3 u-text-bold-500" id="mcetoc_1j7jc2lbe4">Architecting AI: Efficient Models, Everywhere</h3> </td> <td style="text-align:center;"> <h3 data-component="Primary" class="c-image-above-text-card__primary u-text-lg-3 u-text-md-3 u-text-sm-3 u-text-bold-500" id="mcetoc_1j7jcfnoj0">Strong Model, Weak Product? Let’s Fix That</h3> </td> </tr> <tr> <td> <h4 data-component="Secondary" style="text-align:center;" class="c-image-above-text-card__secondary u-text-lg-4 u-text-md-4 u-text-sm-4">Sharbani Roy, VP of AI Services, Arm</h4> </td> <td style="text-align:center;"> <h4 data-component="Secondary" class="c-image-above-text-card__secondary u-text-lg-4 u-text-md-4 u-text-sm-4">Zach Lasiuk, Principle Solutions Designer, Arm</h4> </td> </tr> <tr> <td> <p style="text-align:center;">Wednesday, October 22<br>12:10 pm – 12.35 pm PT</p> <p style="text-align:center;">Discover how Arm is enabling developers to harness small language models (SLMs) and vision models, optimize inference at scale, and build sustainable AI systems.</p> </td> <td style="text-align:center;"><br> <p>Wednesday, October 22<br>10:30 am – 11:00 am PT</p> <p>Swap stories on the human side of GenAI, where tools have failed or flourished, and explore how we can collectively build experiences that keep users coming back.</p> </td> </tr> <tr> <td style="text-align:center;"><span style="font-size:150%;"><a target="_blank" rel="noopener noreferrer" href="https://sched.co/28Yu4" title="Add to schedule button">Add to schedule</a></span></td> <td style="text-align:center;"><span style="font-size:150%;"><a target="_blank" rel="noopener noreferrer" href="https://sched.co/27QND" title="Add to schedule button">Add to schedule</a></span></td> </tr> <tr> <td style="text-align:center;"></td> <td style="text-align:center;"></td> </tr> <tr> <td style="text-align:center;"> <h3 data-component="Primary" class="c-image-above-text-card__primary u-text-lg-3 u-text-md-3 u-text-sm-3 u-text-bold-500" id="mcetoc_1j7jct3av0">Deploying GenAI for Audio Gen on Mobile CPUs with ExecuTorch</h3> </td> <td style="text-align:center;"> <h3 data-component="Primary" class="c-image-above-text-card__primary u-text-lg-3 u-text-md-3 u-text-sm-3 u-text-bold-500" id="mcetoc_1j7jct3av1">Deploying LLMs at the Edge: Lessons from the Field</h3> </td> </tr> <tr> <td style="text-align:center;"> <h4 data-component="Secondary" class="c-image-above-text-card__secondary u-text-lg-4 u-text-md-4 u-text-sm-4">Gian Marco Iodice, GenAI Engineering Lead, Arm</h4> </td> <td style="text-align:center;"> <h4 data-component="Secondary" class="c-image-above-text-card__secondary u-text-lg-4 u-text-md-4 u-text-sm-4">Parichay Das, Principal Architect at LTIMindtree and Distinguished Arm Ambassador</h4> </td> </tr> <tr> <td style="text-align:center;"> <p>Wednesday, October 22<br>11:35 am – 12:00 pm PT</p> <p>Explore practical use cases of GenAI running on device and gain insight into integration workflows, optimizations, and efficient deployment.</p> </td> <td style="text-align:center;"> <p>Wednesday, October 22<br>3:20 pm – 3:50 pm PT</p> <p>Learn about tools, workflows, and optimizations for deploying LLMs on mobile, and how to overcome challenges like limited memory and compute constraints.</p> </td> </tr> <tr> <td style="text-align:center;"><span style="font-size:150%;"><a target="_blank" rel="noopener noreferrer" href="https://sched.co/27QCb" title="Add to schedule button">Add to schedule</a></span></td> <td style="text-align:center;"><span style="font-size:150%;"><a target="_blank" rel="noopener noreferrer" href="https://sched.co/27QNh" title="Add to schedule button">Add to schedule</a></span></td> </tr> </tbody> </table> <p style="text-align:center;"></p> <p style="text-align:center;"></p> <p style="text-align:center;"></p> <p style="text-align:center;"><a target="_blank" rel="noopener noreferrer" href="https://developer.arm.com/developer-partners/pytorch?utm_source=community&amp;utm_medium=blog&amp;utm_content=event&amp;utm_campaign=mk24_developer_pytorch" title="Explore Arm’s complete schedule at the PyTorch Conference here" class="button cta green">Explore Arm's activities at PyTorch Conference 2025</a></p><div style="clear:both;"></div>
                
                            </section>
        
    </div>
 </body>
</html>
