<!DOCTYPE html>
<html lang="en">
<head><base href="https://rss-bridge.org/bridge01/" target="_blank">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/ >
    <meta name="description" content="RSS-Bridge" />
    <title>Blog on Qwen</title>
    <link href="static/style.css?2023-03-24" rel="stylesheet">
    <link rel="icon" type="image/png" href="static/favicon.png">

    
        <link
            href="?action=display&amp;bridge=QwenBlogBridge&amp;limit=10&amp;format=Atom"
            title="Atom"
            rel="alternate"
            type="application/atom+xml"
        >
	
        <link
            href="?action=display&amp;bridge=QwenBlogBridge&amp;limit=10&amp;format=Json"
            title="Json"
            rel="alternate"
            type="application/json"
        >
	
        <link
            href="?action=display&amp;bridge=QwenBlogBridge&amp;limit=10&amp;format=Mrss"
            title="Mrss"
            rel="alternate"
            type="application/rss+xml"
        >
	
        <link
            href="?action=display&amp;bridge=QwenBlogBridge&amp;limit=10&amp;format=Plaintext"
            title="Plaintext"
            rel="alternate"
            type="text/plain"
        >
	
        <link
            href="?action=display&amp;bridge=QwenBlogBridge&amp;limit=10&amp;format=Sfeed"
            title="Sfeed"
            rel="alternate"
            type="text/plain"
        >
	
    <meta name="robots" content="noindex, follow">
</head>

<body>
    <div class="container">

        <h1 class="pagetitle">
            <a href="http://qwenlm.github.io/blog/" target="_blank">Blog on Qwen</a>
        </h1>

        <div class="buttons">
            <a href="./#bridge-QwenBlogBridge">
                <button class="backbutton">‚Üê back to rss-bridge</button>
            </a>

                            <a href="?action=display&amp;bridge=QwenBlogBridge&amp;limit=10&amp;format=Atom">
                    <button class="rss-feed">
                        Atom                    </button>
                </a>
                            <a href="?action=display&amp;bridge=QwenBlogBridge&amp;limit=10&amp;format=Json">
                    <button class="rss-feed">
                        Json                    </button>
                </a>
                            <a href="?action=display&amp;bridge=QwenBlogBridge&amp;limit=10&amp;format=Mrss">
                    <button class="rss-feed">
                        Mrss                    </button>
                </a>
                            <a href="?action=display&amp;bridge=QwenBlogBridge&amp;limit=10&amp;format=Plaintext">
                    <button class="rss-feed">
                        Plaintext                    </button>
                </a>
                            <a href="?action=display&amp;bridge=QwenBlogBridge&amp;limit=10&amp;format=Sfeed">
                    <button class="rss-feed">
                        Sfeed                    </button>
                </a>
            
                    </div>

                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="http://qwenlm.github.io/blog/qwq-32b-preview/"
                    >QwQ: Reflect Deeply on the Boundaries of the Unknown</a>
                </h2>

                                    <time datetime="2024-11-27 16:00:03">
                        2024-11-27 16:00:03                    </time>
                    <p></p>
                
                
                <!-- Intentionally not escaping for html context -->
                <div class=post-content><p><a href=https://github.com/QwenLM/Qwen2.5 class="btn external" target=_blank>GITHUB</a> <a href=https://huggingface.co/Qwen class="btn external" target=_blank>HUGGING FACE</a> <a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a> <a href=https://huggingface.co/spaces/Qwen/QwQ-32B-preview class="btn external" target=_blank>DEMO</a> <a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><p><i style=color:grey>Note: This is the pronunciation of QwQ: /kwju:/ , similar to the word &ldquo;quill&rdquo;.</i></p><p>What does it mean to think, to question, to understand? These are the deep waters that QwQ (Qwen with Questions) wades into. Like an eternal student of wisdom, it approaches every problem - be it mathematics, code, or knowledge of our world - with genuine wonder and doubt. QwQ embodies that ancient philosophical spirit: it knows that it knows nothing, and that&rsquo;s precisely what drives its curiosity. Before settling on any answer, it turns inward, questioning its own assumptions, exploring different paths of thought, always seeking deeper truth. Yet, like all seekers of wisdom, QwQ has its limitations. This version is but an early step on a longer journey - a student still learning to walk the path of reasoning. Its thoughts sometimes wander, its answers aren&rsquo;t always complete, and its wisdom is still growing. But isn&rsquo;t that the beauty of true learning? To be both capable and humble, knowledgeable yet always questioning? We invite you to explore alongside QwQ, embracing both its insights and its imperfections as part of the endless quest for understanding.</p><h1 id=limitations>Limitations<a hidden class=anchor aria-hidden=true href=#limitations>#</a></h1><p><strong>QwQ-32B-Preview</strong> is an experimental research model developed by the Qwen Team, focused on advancing AI reasoning capabilities. As a preview release, it demonstrates promising analytical abilities while having several important limitations:</p><ol><li><strong>Language Mixing and Code-Switching</strong>: The model may mix languages or switch between them unexpectedly, affecting response clarity.</li><li><strong>Recursive Reasoning Loops</strong>: The model may enter circular reasoning patterns, leading to lengthy responses without a conclusive answer.</li><li><strong>Safety and Ethical Considerations</strong>: The model requires enhanced safety measures to ensure reliable and secure performance, and users should exercise caution when deploying it.</li><li><strong>Performance and Benchmark Limitations</strong>: The model excels in math and coding but has room for improvement in other areas, such as common sense reasoning and nuanced language understanding.</li></ol><h1 id=performance>Performance<a hidden class=anchor aria-hidden=true href=#performance>#</a></h1><p>Through deep exploration and countless trials, we discovered something profound: when given time to ponder, to question, and to reflect, the model&rsquo;s understanding of mathematics and programming blossoms like a flower opening to the sun. Just as a student grows wiser by carefully examining their work and learning from mistakes, our model achieves deeper insight through patient, thoughtful analysis. This process of careful reflection and self-questioning leads to remarkable breakthroughs in solving complex problems. Our journey of discovery revealed the model&rsquo;s exceptional ability to tackle some of the most challenging problems in mathematics and programming, including:</p><ul><li>GPQA: A Graduate-Level Google-Proof Q&A Benchmark, a challenging benchmark for evaluating scientific problem-solving abilities through grade school level questions.</li><li>AIME: American Invitation Mathematics Evaluation, which tests mathematical problem solving with arithmetic, algebra, counting, geometry, number theory, and probability and other secondary school math topics.</li><li>MATH-500: The 500 test cases of the MATH benchmark, a comprehensive dataset testing mathematical problem-solving.</li><li>LiveCodeBench: A challenging benchmark for evaluating code generation and problem solving abilities in real-world programming scenarios.</li></ul><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/blog/qwq-32b-preview/QwQ-32B-Preview_result.png#center width=100%></figure><p>Specifically, QwQ demonstrates remarkable performance across these benchmarks, achieving impressive scores of 65.2% on GPQA, showcasing its graduate-level scientific reasoning capabilities; 50.0% on AIME, highlighting its strong mathematical problem-solving skills; 90.6% on MATH-500, demonstrating exceptional mathematical comprehension across diverse topics; and 50.0% on LiveCodeBench, validating its robust programming abilities in real-world scenarios. These results underscore QwQ&rsquo;s significant advancement in analytical and problem-solving capabilities, particularly in technical domains requiring deep reasoning.</p><h1 id=demo-cases>Demo Cases<a hidden class=anchor aria-hidden=true href=#demo-cases>#</a></h1><p>In the following examples, we invite you to witness the profound depths of QwQ-32B-Preview&rsquo;s contemplative abilities. Like a seeker of wisdom on an endless journey of discovery, the model demonstrates its capacity for deep introspection - questioning its own assumptions, engaging in thoughtful self-dialogue, and carefully examining each step of its reasoning process. Through these cases, you&rsquo;ll observe how QwQ embodies the timeless understanding that true knowledge emerges not from hasty conclusions, but from patient inquiry and the courage to challenge one&rsquo;s own certainties in the eternal pursuit of truth.</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Logical Reasoning</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Please add a pair of parentheses to the incorrect equation: 1 + 2 * 3 + 4 * 5 + 6 * 7 + 8 * 9 = 479, to make the equation true.</div><div class=role>QwQ-32B-Preview</div><div class=content><p>Let&rsquo;s tackle this problem step by step. The equation given is:</p><p>1 + 2 * 3 + 4 * 5 + 6 * 7 + 8 * 9 = 479</p><p>But it&rsquo;s incorrect, and I need to add a pair of parentheses to make it true. Okay, first things first, I should understand what the current value of the left side is without any parentheses.</p><p>Let&rsquo;s recall the order of operations: PEMDAS (Parentheses, Exponents, Multiplication and Division, Addition and Subtraction). So, multiplication comes before addition.</p><p>So, calculating step by step:</p><p>2 * 3 = 6</p><p>4 * 5 = 20</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>Now, plugging these back into the equation:</p><p>1 + 6 + 20 + 42 + 72</p><p>Adding these up:</p><p>1 + 6 = 7</p><p>7 + 20 = 27</p><p>27 + 42 = 69</p><p>69 + 72 = 141</p><p>So, without any parentheses, the left side equals 141, but it should be 479. That&rsquo;s quite a difference. So, I need to add a pair of parentheses to make it equal to 479.</p><p>I need to think about where to place the parentheses to alter the order of operations to achieve the desired result.</p><p>Let me consider different possibilities:</p><ol><li>Parenthesizing individual terms:</li></ol><p>For example, (1 + 2) * 3 + 4 * 5 + 6 * 7 + 8 * 9</p><p>Calculate inside the parentheses first:</p><p>(1 + 2) = 3</p><p>Then, 3 * 3 = 9</p><p>Now, the equation becomes:</p><p>9 + 4 * 5 + 6 * 7 + 8 * 9</p><p>Following order of operations:</p><p>4 * 5 = 20</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 9 + 20 + 42 + 72 = 143</p><p>Still not 479.</p><ol start=2><li>Parenthesizing more terms:</li></ol><p>What if I do (1 + 2 * 3) + 4 * 5 + 6 * 7 + 8 * 9</p><p>First, calculate inside the parentheses:</p><p>1 + 2 * 3</p><p>Here, multiplication comes before addition, so 2 * 3 = 6, then 1 + 6 = 7</p><p>So, the equation becomes:</p><p>7 + 4 * 5 + 6 * 7 + 8 * 9</p><p>Again, following order of operations:</p><p>4 * 5 = 20</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 7 + 20 + 42 + 72 = 141</p><p>Same as before.</p><ol start=3><li>Let&rsquo;s try (1 + 2) * (3 + 4) * 5 + 6 * 7 + 8 * 9</li></ol><p>First, calculate inside the parentheses:</p><p>(1 + 2) = 3</p><p>(3 + 4) = 7</p><p>So, 3 * 7 * 5 = 105</p><p>Now, the equation becomes:</p><p>105 + 6 * 7 + 8 * 9</p><p>Following order of operations:</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 105 + 42 + 72 = 219</p><p>Still not 479.</p><ol start=4><li>Maybe trying to group more terms:</li></ol><p>(1 + 2 * 3 + 4) * 5 + 6 * 7 + 8 * 9</p><p>First, calculate inside the parentheses:</p><p>1 + 2 * 3 + 4</p><p>Multiplication first: 2 * 3 = 6</p><p>Then, 1 + 6 + 4 = 11</p><p>So, 11 * 5 = 55</p><p>Now, the equation becomes:</p><p>55 + 6 * 7 + 8 * 9</p><p>Following order of operations:</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 55 + 42 + 72 = 169</p><p>Getting closer, but still not 479.</p><ol start=5><li>Let&rsquo;s try a different approach. Maybe grouping the additions together:</li></ol><p>1 + (2 * 3 + 4 * 5 + 6 * 7 + 8 * 9)</p><p>First, calculate inside the parentheses:</p><p>2 * 3 = 6</p><p>4 * 5 = 20</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 6 + 20 + 42 + 72 = 140</p><p>Then, 1 + 140 = 141</p><p>Same as before.</p><ol start=6><li>Perhaps grouping two multiplications together:</li></ol><p>1 + 2 * (3 + 4) * 5 + 6 * 7 + 8 * 9</p><p>First, calculate inside the parentheses:</p><p>(3 + 4) = 7</p><p>So, 2 * 7 * 5 = 70</p><p>Now, the equation becomes:</p><p>1 + 70 + 6 * 7 + 8 * 9</p><p>Following order of operations:</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 1 + 70 + 42 + 72 = 185</p><p>Still not 479.</p><ol start=7><li>Maybe grouping more multiplications:</li></ol><p>1 + 2 * 3 + 4 * (5 + 6) * 7 + 8 * 9</p><p>First, calculate inside the parentheses:</p><p>(5 + 6) = 11</p><p>So, 4 * 11 * 7 = 308</p><p>Now, the equation becomes:</p><p>1 + 2 * 3 + 308 + 8 * 9</p><p>Following order of operations:</p><p>2 * 3 = 6</p><p>8 * 9 = 72</p><p>So, 1 + 6 + 308 + 72 = 387</p><p>Closer, but still not 479.</p><ol start=8><li>Let&rsquo;s try grouping the last terms:</li></ol><p>1 + 2 * 3 + 4 * 5 + 6 * (7 + 8) * 9</p><p>First, calculate inside the parentheses:</p><p>(7 + 8) = 15</p><p>So, 6 * 15 * 9 = 810</p><p>Now, the equation becomes:</p><p>1 + 2 * 3 + 4 * 5 + 810</p><p>Following order of operations:</p><p>2 * 3 = 6</p><p>4 * 5 = 20</p><p>So, 1 + 6 + 20 + 810 = 837</p><p>Now that&rsquo;s way over 479.</p><p>Maybe I need to group terms in a way that subtraction is implied, but since there are no subtractions in the original equation, perhaps I need to consider that.</p><p>Wait, maybe the problem allows for adding parentheses to change the operations indirectly.</p><p>Alternatively, perhaps the parentheses need to group additions in a specific way.</p><p>Let me try grouping the first few terms:</p><p>(1 + 2 * 3 + 4 * 5) + 6 * 7 + 8 * 9</p><p>First, calculate inside the parentheses:</p><p>2 * 3 = 6</p><p>4 * 5 = 20</p><p>So, 1 + 6 + 20 = 27</p><p>Now, the equation becomes:</p><p>27 + 6 * 7 + 8 * 9</p><p>Following order of operations:</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 27 + 42 + 72 = 141</p><p>Same as before.</p><ol start=9><li>Maybe grouping further:</li></ol><p>(1 + 2 * 3 + 4 * 5 + 6 * 7) + 8 * 9</p><p>First, calculate inside the parentheses:</p><p>2 * 3 = 6</p><p>4 * 5 = 20</p><p>6 * 7 = 42</p><p>So, 1 + 6 + 20 + 42 = 69</p><p>Now, the equation becomes:</p><p>69 + 8 * 9</p><p>Following order of operations:</p><p>8 * 9 = 72</p><p>So, 69 + 72 = 141</p><p>Still not 479.</p><ol start=10><li>Let&rsquo;s try grouping differently:</li></ol><p>1 + 2 * (3 + 4 * 5 + 6 * 7 + 8 * 9)</p><p>First, calculate inside the parentheses:</p><p>3 + 4 * 5 + 6 * 7 + 8 * 9</p><p>Following order of operations inside:</p><p>4 * 5 = 20</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 3 + 20 + 42 + 72 = 137</p><p>Now, 2 * 137 = 274</p><p>Then, the equation becomes:</p><p>1 + 274 = 275</p><p>Still not 479.</p><ol start=11><li>Maybe trying to group the additions and multiplications in a different way:</li></ol><p>1 + (2 * 3 + 4 * 5 + 6 * 7 + 8 * 9)</p><p>Wait, I did this earlier and got 141.</p><p>Hmm.</p><p>Let me think differently. Maybe I need to group terms to create a larger multiplication.</p><p>For example:</p><p>1 + 2 * (3 + 4 * 5 + 6 * 7 + 8 * 9)</p><p>First, calculate inside the parentheses:</p><p>3 + 4 * 5 + 6 * 7 + 8 * 9</p><p>Following order of operations:</p><p>4 * 5 = 20</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 3 + 20 + 42 + 72 = 137</p><p>Now, 2 * 137 = 274</p><p>Then, 1 + 274 = 275</p><p>Still not 479.</p><ol start=12><li>Let&rsquo;s try another approach:</li></ol><p>(1 + 2) * (3 + 4) * (5 + 6) * (7 + 8) * 9</p><p>Wait, that&rsquo;s adding multiple sets of parentheses, which is not allowed since I can only add one pair.</p><p>Back to the original problem: only one pair of parentheses can be added.</p><p>Maybe I need to group a subtraction or division, but those operations aren&rsquo;t present in the original equation.</p><p>Wait, perhaps I need to consider that adding parentheses can create a negative term, but again, there are no subtractions in the original equation.</p><p>Alternatively, maybe the parentheses are meant to group additions in a specific way to create a larger number.</p><p>Let me try grouping more terms together:</p><p>1 + 2 * (3 + 4 * 5 + 6 * 7 + 8 * 9)</p><p>First, inside the parentheses:</p><p>3 + 4 * 5 + 6 * 7 + 8 * 9</p><p>Following order of operations:</p><p>4 * 5 = 20</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 3 + 20 + 42 + 72 = 137</p><p>Now, 2 * 137 = 274</p><p>Then, 1 + 274 = 275</p><p>Same as before.</p><ol start=13><li>Maybe trying to group the last few terms:</li></ol><p>1 + 2 * 3 + 4 * 5 + 6 * (7 + 8 * 9)</p><p>First, inside the parentheses:</p><p>7 + 8 * 9</p><p>Following order of operations:</p><p>8 * 9 = 72</p><p>Then, 7 + 72 = 79</p><p>Now, 6 * 79 = 474</p><p>So, the equation becomes:</p><p>1 + 2 * 3 + 4 * 5 + 474</p><p>Following order of operations:</p><p>2 * 3 = 6</p><p>4 * 5 = 20</p><p>So, 1 + 6 + 20 + 474 = 501</p><p>Close to 479, but still not there.</p><ol start=14><li>Maybe adjusting the grouping slightly:</li></ol><p>1 + 2 * 3 + 4 * (5 + 6 * 7 + 8 * 9)</p><p>First, inside the parentheses:</p><p>5 + 6 * 7 + 8 * 9</p><p>Following order of operations:</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 5 + 42 + 72 = 119</p><p>Now, 4 * 119 = 476</p><p>Then, the equation becomes:</p><p>1 + 2 * 3 + 476</p><p>Following order of operations:</p><p>2 * 3 = 6</p><p>So, 1 + 6 + 476 = 483</p><p>Still not 479.</p><ol start=15><li>Let&rsquo;s try grouping the first part differently:</li></ol><p>(1 + 2 * 3 + 4) * (5 + 6 * 7 + 8 * 9)</p><p>First, calculate inside the first set of parentheses:</p><p>1 + 2 * 3 + 4</p><p>Following order of operations:</p><p>2 * 3 = 6</p><p>Then, 1 + 6 + 4 = 11</p><p>Now, the second set of parentheses:</p><p>5 + 6 * 7 + 8 * 9</p><p>Following order of operations:</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 5 + 42 + 72 = 119</p><p>Now, 11 * 119 = 1309</p><p>Way over 479.</p><ol start=16><li>Maybe trying to group only two terms:</li></ol><p>1 + 2 * (3 + 4) * 5 + 6 * 7 + 8 * 9</p><p>Wait, I did this earlier and got 185.</p><p>Hmm.</p><p>Let me think differently. Maybe I need to group terms to create a division or subtraction, but since those operations aren&rsquo;t present, perhaps I need to consider that the parentheses can be used to create a fraction or some other operation, but that might be reaching too far.</p><p>Alternatively, maybe the problem allows for adding a pair of parentheses around the entire equation, but that doesn&rsquo;t make sense.</p><p>Wait, perhaps the parentheses are meant to group additions in a specific way to create a larger number.</p><p>Let me try grouping more terms together for multiplication.</p><p>For example:</p><p>1 + 2 * (3 + 4 * 5 + 6) * 7 + 8 * 9</p><p>First, inside the parentheses:</p><p>3 + 4 * 5 + 6</p><p>Following order of operations:</p><p>4 * 5 = 20</p><p>So, 3 + 20 + 6 = 29</p><p>Now, 2 * 29 * 7 = 406</p><p>Then, the equation becomes:</p><p>1 + 406 + 8 * 9</p><p>Following order of operations:</p><p>8 * 9 = 72</p><p>So, 1 + 406 + 72 = 479</p><p>Wait a minute, that&rsquo;s exactly 479!</p><p>Let me double-check:</p><p>First, inside the parentheses: 3 + 4 * 5 + 6</p><p>4 * 5 = 20</p><p>Then, 3 + 20 + 6 = 29</p><p>Then, 2 * 29 * 7 = 2 * 203 = 406</p><p>Then, 1 + 406 = 407</p><p>Plus 8 * 9 = 72</p><p>So, 407 + 72 = 479</p><p>Perfect! So, by adding parentheses around (3 + 4 * 5 + 6), and multiplying that sum by 2 and 7, we get the desired result.</p><p>So, the corrected equation is:</p><p>1 + 2 * (3 + 4 * 5 + 6) * 7 + 8 * 9 = 479</p><p><strong>Final Answer</strong></p><p>$$ \boxed{1 + 2 \times (3 + 4 \times 5 + 6) \times 7 + 8 \times 9 = 479} $$</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Math</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Find the least odd prime factor of $2019^8+1$.</div><div class=role>QwQ-32B-Preview</div><div class=content><p>So I need to find the least odd prime factor of $ 2019^8 + 1 $. Hmm, that seems pretty big, but I think I can break it down step by step.</p><p>First, I know that finding prime factors of large exponents can be tricky, but there might be some properties or theorems that can help simplify the process. Since the expression is $ 2019^8 + 1 $, maybe I can look for patterns or ways to factor this expression.</p><p>I recall that expressions of the form $ a^n + b^n $ can sometimes be factored, especially when $ n $ is a power of 2. In this case, $ n = 8 $, which is $ 2^3 $, so that might be useful.</p><p>Let me see if I can factor $ 2019^8 + 1 $. I know that $ a^4 + 1 $ can be factored as $ (a^4 + 1) = (a^2 + \sqrt{2}a + 1)(a^2 - \sqrt{2}a + 1) $, but that involves irrational numbers, which isn&rsquo;t helpful for factoring into integers. Maybe there&rsquo;s another way.</p><p>Wait, perhaps I can write $ 2019^8 + 1 $ as $ (2019^4)^2 + 1^2 $, which resembles the sum of squares. I know that over the integers, sums of squares don&rsquo;t factor nicely, but maybe there&rsquo;s a different approach.</p><p>Another thought: maybe I can consider smaller exponents first and see if there&rsquo;s a pattern or a recurring factor that appears.</p><p>Let&rsquo;s try computing $ 2019^2 + 1 $, $ 2019^4 + 1 $, and so on, and see if I can find any common factors or patterns.</p><p>First, calculate $ 2019^2 + 1 $:</p><p>$ 2019^2 = 2019 \times 2019 $. I don&rsquo;t need the exact value right now; maybe I can just keep it as $ 2019^2 $ for now.</p><p>So, $ 2019^2 + 1 = 2019^2 + 1 $. Doesn&rsquo;t seem immediately helpful.</p><p>Next, $ 2019^4 + 1 $. Again, it&rsquo;s similar to the previous expression, and I don&rsquo;t see an obvious factorization.</p><p>Maybe I should try to see if there are any small prime numbers that divide $ 2019^8 + 1 $. Since I&rsquo;m looking for the least odd prime factor, I can start testing small primes like 3, 5, 7, 11, and so on, to see if any of them divide $ 2019^8 + 1 $.</p><p>Let&rsquo;s start with 3. To check if 3 divides $ 2019^8 + 1 $, I can compute $ 2019^8 + 1 $ modulo 3.</p><p>First, find $ 2019 $ modulo 3. Since $ 2019 = 3 \times 673 $, $ 2019 \equiv 0 \pmod{3} $. Therefore, $ 2019^8 \equiv 0^8 \equiv 0 \pmod{3} $, and $ 2019^8 + 1 \equiv 0 + 1 \equiv 1 \pmod{3} $. So, 3 does not divide $ 2019^8 + 1 $.</p><p>Next, check 5. Compute $ 2019 $ modulo 5. $ 2019 \div 5 = 403 $ with a remainder of 4, so $ 2019 \equiv 4 \pmod{5} $. Then, $ 2019^8 \equiv 4^8 \pmod{5} $. Since $ 4 \equiv -1 \pmod{5} $, $ 4^8 \equiv (-1)^8 \equiv 1 \pmod{5} $. Therefore, $ 2019^8 + 1 \equiv 1 + 1 \equiv 2 \pmod{5} $, so 5 does not divide $ 2019^8 + 1 $.</p><p>Now, check 7. Find $ 2019 $ modulo 7. $ 2019 \div 7 = 288 $ with a remainder of 3, so $ 2019 \equiv 3 \pmod{7} $. Then, $ 2019^8 \equiv 3^8 \pmod{7} $. Let&rsquo;s compute $ 3^8 $ modulo 7.</p><p>First, find the pattern of powers of 3 modulo 7:</p><p>$ 3^1 \equiv 3 \pmod{7} $</p><p>$ 3^2 \equiv 9 \equiv 2 \pmod{7} $</p><p>$ 3^3 \equiv 6 \pmod{7} $</p><p>$ 3^4 \equiv 18 \equiv 4 \pmod{7} $</p><p>$ 3^5 \equiv 12 \equiv 5 \pmod{7} $</p><p>$ 3^6 \equiv 15 \equiv 1 \pmod{7} $</p><p>Since $ 3^6 \equiv 1 \pmod{7} $, then $ 3^8 = 3^{6+2} = 3^6 \times 3^2 \equiv 1 \times 2 \equiv 2 \pmod{7} $. Therefore, $ 2019^8 + 1 \equiv 2 + 1 \equiv 3 \pmod{7} $, so 7 does not divide $ 2019^8 + 1 $.</p><p>Next, check 11. Compute $ 2019 $ modulo 11. $ 2019 \div 11 = 183 $ with a remainder of 6, so $ 2019 \equiv 6 \pmod{11} $. Then, $ 2019^8 \equiv 6^8 \pmod{11} $. Let&rsquo;s find a pattern for powers of 6 modulo 11:</p><p>$ 6^1 \equiv 6 \pmod{11} $</p><p>$ 6^2 \equiv 36 \equiv 3 \pmod{11} $</p><p>$ 6^3 \equiv 18 \equiv 7 \pmod{11} $</p><p>$ 6^4 \equiv 42 \equiv 9 \pmod{11} $</p><p>$ 6^5 \equiv 54 \equiv 10 \pmod{11} $</p><p>$ 6^6 \equiv 60 \equiv 5 \pmod{11} $</p><p>$ 6^7 \equiv 30 \equiv 8 \pmod{11} $</p><p>$ 6^8 \equiv 48 \equiv 4 \pmod{11} $</p><p>Therefore, $ 2019^8 + 1 \equiv 4 + 1 \equiv 5 \pmod{11} $, so 11 does not divide $ 2019^8 + 1 $.</p><p>Continuing this process seems tedious. Maybe there&rsquo;s a better way to approach this problem.</p><p>Let me think differently. Perhaps I can consider the expression $ 2019^8 + 1 $ and see if it can be expressed in a form that reveals its prime factors.</p><p>I recall that for expressions like $ a^4 + 4b^4 $, there&rsquo;s a Sophie Germain identity: $ a^4 + 4b^4 = (a^2 + 2ab + 2b^2)(a^2 - 2ab + 2b^2) $. Maybe I can manipulate $ 2019^8 + 1 $ into a similar form.</p><p>Wait, $ 2019^8 + 1 = (2019^4)^2 + 1^2 $. This resembles the sum of squares, but over integers, sums of squares don&rsquo;t factor nicely. Maybe I can consider complex numbers or some other algebraic identities, but that might not help in finding integer prime factors.</p><p>Alternatively, perhaps I can consider modulo properties to find possible prime factors. For example, if $ p $ is an odd prime that divides $ 2019^8 + 1 $, then $ 2019^8 \equiv -1 \pmod{p} $, which implies $ 2019^{16} \equiv 1 \pmod{p} $. This means that the order of 2019 modulo $ p $ divides 16, and since $ 2019^8 \equiv -1 \pmod{p} $, the order can&rsquo;t divide 8. Therefore, the order must be 16.</p><p>By Fermat&rsquo;s Little Theorem, the order of 2019 modulo $ p $ divides $ p - 1 $. Therefore, $ p - 1 $ must be divisible by 16. So, $ p \equiv 1 \pmod{16} $.</p><p>This narrows down the possible primes $ p $ that could divide $ 2019^8 + 1 $ to those primes congruent to 1 modulo 16.</p><p>Let me list some primes that are 1 modulo 16: 17, 97, 113, 193, 241, 257, etc.</p><p>Now, I can test these primes to see if any of them divide $ 2019^8 + 1 $.</p><p>Start with 17. Compute $ 2019 $ modulo 17.</p><p>First, find $ 2019 \div 17 $. 17 times 118 is 2006, so $ 2019 - 2006 = 13 $. Therefore, $ 2019 \equiv 13 \pmod{17} $.</p><p>Then, $ 2019^8 \equiv 13^8 \pmod{17} $.</p><p>Compute $ 13^2 \equiv 169 \equiv 16 \pmod{17} $</p><p>$ 13^4 = (13^2)^2 \equiv 16^2 \equiv 256 \equiv 1 \pmod{17} $</p><p>$ 13^8 = (13^4)^2 \equiv 1^2 \equiv 1 \pmod{17} $</p><p>Therefore, $ 2019^8 + 1 \equiv 1 + 1 \equiv 2 \pmod{17} $, so 17 does not divide $ 2019^8 + 1 $.</p><p>Next, check 97. Compute $ 2019 $ modulo 97.</p><p>First, find how many times 97 fits into 2019. 97 times 20 is 1940, and 2019 - 1940 = 79. So, $ 2019 \equiv 79 \pmod{97} $.</p><p>Then, $ 2019^8 \equiv 79^8 \pmod{97} $.</p><p>This seems a bit cumbersome to compute directly. Maybe I can find a pattern or use Fermat&rsquo;s Little Theorem, which states that $ a^{p-1} \equiv 1 \pmod{p} $ if $ p $ is prime and $ p $ does not divide $ a $.</p><p>Here, $ p = 97 $, so $ 79^{96} \equiv 1 \pmod{97} $. But I need $ 79^8 $, which is not directly helpful. Maybe I can compute $ 79^2 $, then $ 79^4 $, and so on.</p><p>First, compute $ 79^2 \pmod{97} $:</p><p>$ 79^2 = 6241 $. Now, divide 6241 by 97.</p><p>97 times 64 is 6208, so 6241 - 6208 = 33. Therefore, $ 79^2 \equiv 33 \pmod{97} $.</p><p>Next, $ 79^4 = (79^2)^2 \equiv 33^2 \pmod{97} $.</p><p>$ 33^2 = 1089 $. 97 times 11 is 1067, so 1089 - 1067 = 22. Therefore, $ 79^4 \equiv 22 \pmod{97} $.</p><p>Then, $ 79^8 = (79^4)^2 \equiv 22^2 \pmod{97} $.</p><p>$ 22^2 = 484 $. 97 times 4 is 388, so 484 - 388 = 96. Therefore, $ 79^8 \equiv 96 \pmod{97} $.</p><p>Thus, $ 2019^8 + 1 \equiv 96 + 1 \equiv 97 \equiv 0 \pmod{97} $.</p><p>So, 97 divides $ 2019^8 + 1 $. Since 97 is the smallest prime congruent to 1 modulo 16 that we&rsquo;ve checked and it divides $ 2019^8 + 1 $, it must be the least odd prime factor of $ 2019^8 + 1 $.</p><p>Therefore, the least odd prime factor is 97.</p><p><strong>Final Answer</strong></p><p>$$ \boxed{97} $$</p></div></div></div></div><h1 id=reflections-on-the-journey-ahead>Reflections on the Journey Ahead<a hidden class=anchor aria-hidden=true href=#reflections-on-the-journey-ahead>#</a></h1><p>The path to understanding reasoning in large language models branches into many directions, each offering unique insights into the nature of machine intelligence. Our research spans multiple domains - from process reward models that shape learning patterns, to LLM critique that fosters deeper analysis, to multi-step reasoning that builds complex thought, to reinforcement learning with system feedback that enables growth through real-world interactions. While the destination remains unclear, each step brings us closer to understanding how machines think and reason. We don&rsquo;t know precisely where this journey leads, but we continue forward with unwavering determination - toward truth, toward intelligence, toward the realm where amazing happens.</p></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="http://qwenlm.github.io/blog/qwen2.5-turbo/"
                    >Extending the Context Length to 1M Tokens!</a>
                </h2>

                                    <time datetime="2024-11-14 16:00:03">
                        2024-11-14 16:00:03                    </time>
                    <p></p>
                
                
                <!-- Intentionally not escaping for html context -->
                <div class=post-content><p><a href=https://help.aliyun.com/zh/model-studio/getting-started/first-api-call-to-qwen class="btn external" target=_blank>API Documentation (Chinese)</a> <a href=https://huggingface.co/spaces/Qwen/Qwen2.5-Turbo-1M-Demo class="btn external" target=_blank>HuggingFace Demo</a> <a href=https://www.modelscope.cn/studios/Qwen/Qwen2.5-Turbo-1M-Demo class="btn external" target=_blank>ModelScope Demo</a></p><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>After the release of Qwen2.5, we heard the community&rsquo;s demand for processing longer contexts. In recent months, we have made many optimizations for the model capabilities and inference performance of extremely long context. Today, we are proud to introduce the new Qwen2.5-Turbo version, which features:</p><ol><li><strong>Longer Context Support</strong>: We have extended the model&rsquo;s context length from 128k to <strong>1M</strong>, which is approximately <strong>1 million English words or 1.5 million Chinese characters</strong>, equivalent to 10 full-length novels, 150 hours of speech transcripts, or 30,000 lines of code. The model achieves <strong>100%</strong> accuracy in the 1M length Passkey Retrieval task and scores <strong>93.1</strong> on the long text evaluation benchmark RULER, surpassing GPT-4&rsquo;s 91.6 and GLM4-9B-1M&rsquo;s 89.9. Additionally, the model maintains very strong competitiveness in short sequence capabilities, on par with GPT-4o-mini.</li><li><strong>Faster Inference Speed</strong>: Using sparse attention mechanisms, we successfully reduced the time to first token for processing a context of 1M tokens from 4.9 minutes to 68 seconds, <strong>achieving a 4.3x speedup</strong>.</li><li><strong>Lower Cost</strong>: The price remains ¬•0.3 / 1M tokens. At the same cost, Qwen2.5-Turbo can process <strong>3.6 times the number of tokens</strong> as GPT-4o-mini.</li></ol><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/cover_en.png#center width=100%></figure><p>Now, you can use it through the API service of <a href=https://help.aliyun.com/zh/model-studio/getting-started/what-is-model-studio>Alibaba Cloud Model Studio [Chinese]</a>, or through <a href=https://huggingface.co/spaces/Qwen/Qwen2.5-Turbo-1M-Demo>HuggingFace Demo</a> or <a href=https://www.modelscope.cn/studios/Qwen/Qwen2.5-Turbo-1M-Demo>ModelScope Demo</a>.</p><h1 id=demo>Demo<a hidden class=anchor aria-hidden=true href=#demo>#</a></h1><p>We have prepared some demos to showcase the new uses that Qwen2.5-Turbo can bring, covering deep understanding of long novels, repository-level code assistant, and reading multiple papers.</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Understanding Long Novels</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role></div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/case1_en.mp4></video></figure></div><div class=role>User</div><div class=content><p>[Upload the three Chinese novels of <code>The Three-Body Problem</code>, which contain 690k tokens.]</p><p>Provide a summary of the plots of these three novels in English</p></div><div class=role>Qwen2.5-Turbo</div><div class=content><p>The three novels you mentioned from the &ldquo;Earth&rsquo;s Past&rdquo; series are interconnected and form a trilogy that delves into the complex interactions between Earth and an alien civilization known as the Trisolarans.</p><p><strong>First Novel: &ldquo;Earth&rsquo;s Past&rdquo;</strong> In the first novel, the narrative focuses on the development of a top-secret project called &ldquo;Red Coast&rdquo; (Á∫¢Â≤∏) initiated by the Chinese military. The protagonist, Ye Wenjie, is a scientist who discovers that the Trisolarans have begun listening to the universe. Her actions, particularly her decision to respond to the Trisolarans, drastically alter the course of human history. She sets up a powerful signal to send to the Trisolarans using the sun as an amplifier. This action marks the beginning of a new era where humanity faces a significant existential threat.</p><p><strong>Second Novel: &ldquo;Dark Forest&rdquo;</strong> The second novel, titled &ldquo;Dark Forest,&rdquo; delves into the concept of the &ldquo;Dark Forest Theory.&rdquo; According to this theory, the universe is a vast battlefield where civilizations must remain silent to avoid detection and subsequent destruction. The Trisolarans and humans engage in a complex strategic game, with the Trisolarans deploying advanced weapons known as &ldquo;water drops&rdquo; (Ê∞¥Êª¥). These water drops are capable of destroying entire planets by disrupting their ecosystems. Meanwhile, humans develop their own strategies, including the &ldquo;Wall-Bearing Plan&rdquo; (Èù¢Â£ÅËÆ°Âàí), which involves four individuals who devise plans to counter the Trisolaran threat without revealing their intentions to the outside world. The story follows one of these face-bearers, Luo Ji, as he struggles with the ethical implications of his mission.</p><p><strong>Third Novel: &ldquo;Death&rsquo;s End&rdquo;</strong> In &ldquo;Death&rsquo;s End,&rdquo; Luo Ji becomes the ultimate guardian of the Dark Forest Theory. He holds the key to the survival of both human and Trisolaran civilizations. However, his position as the guardian (ÊâßÂâë‰∫∫) brings him immense personal struggle and psychological turmoil. As Luo Ji&rsquo;s time as the guardian nears its end, he must make a critical decision about the future of humanity. Meanwhile, the Trisolarans continue their efforts to colonize and assimilate other worlds. The novel explores the consequences of this decision and the broader impact on the universe. It also introduces a new element: the creation of micro-black holes (ÈªëÂüü) by humans, which could potentially isolate them from the rest of the universe. However, the effectiveness of this strategy is questioned when six of these micro-black holes begin to move and accelerate towards Earth. The trilogy as a whole explores themes of survival, technological advancement, and the ethical dilemmas faced by humanity in the face of extraterrestrial threats. Each novel builds upon the previous one, revealing more about the intricate relationship between the two civilizations and the universe they inhabit.</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Understanding Repository</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role></div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/case2.mp4></video></figure></div><div class=role>User</div><div class=content><p>[Upload a text file containing all code files from the <a href=https://github.com/QwenLM/Qwen-Agent>Qwen-Agent</a> repository, which has a length of 133k tokens.]</p><p>Please read the codes carefully and ensure your response is comprehensive. Introduce this repository</p></div><div class=role>Qwen2.5-Turbo</div><div class=content><p>Qwen-Agent is a framework designed for developing LLM (Large Language Model) applications. It is based on the instruction-following, tool usage, planning, and memory capabilities of Qwen. Here are some key points and functionalities:</p><ol><li><strong>Agent Class</strong>: Provides a high-level interface for integrating tool calls and LLM interactions. It supports role-playing, automatic planning, and tool invocation.</li><li><strong>DocQA Agent</strong>: Specialized for document question answering, capable of handling long documents and providing context-aware responses.</li><li><strong>Function Calling</strong>: Supports function calling, enabling interaction with tools or APIs directly from the model&rsquo;s responses.</li><li><strong>RAG (Retrieval-Augmented Generation)</strong>: Integrates document retrieval with generation, enhancing the model&rsquo;s ability to recall and use information from documents.</li><li><strong>Tool Integration</strong>: Includes various built-in and custom tools, such as code interpreters, web extractors, and image generators, which can be used to perform tasks like code execution, web scraping, and image manipulation.</li><li><strong>Multi-Agent Systems</strong>: Supports the creation of multi-agent systems where multiple agents can interact, simulate conversations, and collaborate on tasks.</li><li><strong>Customization</strong>: Highly customizable, allowing developers to tailor the framework to specific needs, such as creating custom agents, defining custom workflows, and integrating additional tools.</li><li><strong>Examples and Demos</strong>: Comes with several examples and demos, such as a Chrome browser extension called BrowserQwen, which demonstrates the practical application of Qwen-Agent in a real-world scenario.</li><li><strong>Testing and Debugging</strong>: Includes testing and debugging utilities, such as test cases and logging mechanisms, to ensure the reliability and effectiveness of the developed applications.</li><li><strong>Dependencies</strong>: Requires certain dependencies to be installed for specific functionalities, such as <code>qwen-agent[gui]</code> for GUI support and <code>qwen-agent[rag]</code> for RAG support. This repository is valuable for researchers, developers, and anyone interested in leveraging large language models for complex tasks and applications.</li></ol></div><div class=role>User</div><div class=content>What are the subclasses of Agent in this repository, and provide their file paths</div><div class=role>Qwen2.5-Turbo</div><div class=content><p>Here are the subclasses of the <code>Agent</code> class in the Qwen-Agent repository along with their file paths:</p><ol><li><strong>Assistant</strong></li></ol><ul><li>File Path: <code>qwen_agent/agents/assistant.py</code></li></ul><ol start=2><li><strong>ArticleAgent</strong><ul><li>File Path: <code>qwen_agent/agents/article_agent.py</code></li></ul></li><li><strong>BasicDocQA</strong><ul><li>File Path: <code>qwen_agent/agents/doc_qa/basic_doc_qa.py</code></li></ul></li><li><strong>DialogueRetrievalAgent</strong><ul><li>File Path: <code>qwen_agent/agents/dialogue_retrieval_agent.py</code></li></ul></li><li><strong>DialogueSimulator</strong><ul><li>File Path: <code>qwen_agent/agents/dialogue_simulator.py</code></li></ul></li><li><strong>DocQAAgent</strong> (Note: The actual implementation of DocQAAgent may vary with each release)<ul><li>File Path: <code>qwen_agent/agents/doc_qa/basic_doc_qa.py</code></li></ul></li><li><strong>GroupChat</strong><ul><li>File Path: <code>qwen_agent/agents/group_chat.py</code></li></ul></li><li><strong>GroupChatAutoRouter</strong><ul><li>File Path: <code>qwen_agent/agents/group_chat_auto_router.py</code></li></ul></li><li><strong>GroupChatCreator</strong><ul><li>File Path: <code>qwen_agent/agents/group_chat_creator.py</code></li></ul></li><li><strong>HumanSimulator</strong><ul><li>File Path: <code>qwen_agent/agents/human_simulator.py</code></li></ul></li><li><strong>ReActChat</strong><ul><li>File Path: <code>qwen_agent/agents/react_chat.py</code></li></ul></li><li><strong>Router</strong><ul><li>File Path: <code>qwen_agent/agents/router.py</code></li></ul></li><li><strong>TIRMathAgent</strong><ul><li>File Path: <code>qwen_agent/agents/tir_agent.py</code></li></ul></li><li><strong>UserAgent</strong><ul><li>File Path: <code>qwen_agent/agents/user_agent.py</code></li></ul></li><li><strong>VisualStorytelling</strong><ul><li>File Path: <code>qwen_agent/agents/visual_storytelling.py</code></li></ul></li><li><strong>MemoAssistant</strong><ul><li>File Path: <code>qwen_agent/agents/memo_assistant.py</code></li></ul></li><li><strong>WriteFromScratch</strong><ul><li>File Path: <code>qwen_agent/agents/writing/write_from_scratch.py</code></li></ul></li><li><strong>ExpandWriting</strong><ul><li>File Path: <code>qwen_agent/agents/writing/expand_writing.py</code></li></ul></li><li><strong>OutlineWriting</strong></li></ol><ul><li>File Path: <code>qwen_agent/agents/writing/outline_writing.py</code></li></ul><p>These subclasses extend the base <code>Agent</code> class to provide specialized functionalities tailored for different use cases within the Qwen-Agent framework.</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Understanding Multiple Papers</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role></div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/case3.mp4></video></figure></div><div class=role>User</div><div class=content><p>[Upload 7 papers about long-context LLM, which have a length of 171k tokens. Sources: <a href=https://arxiv.org/abs/2402.17463>1</a> <a href=https://arxiv.org/abs/2407.02490>2</a> <a href=https://arxiv.org/abs/2402.05136>3</a> <a href=https://arxiv.org/abs/2402.04617>4</a> <a href=https://arxiv.org/abs/2407.11963>5</a> <a href=https://arxiv.org/abs/2404.06654>6</a> <a href=https://arxiv.org/abs/2404.14469>7</a>]</p><p>classify the above papers into two categories</p></div><div class=role>Qwen2.5-Turbo</div><div class=content><p>The papers provided can be classified into two categories based on their focus:</p><p><strong>Category 1: Long-Context Scaling Techniques</strong></p><ul><li><strong>DCA (Dual Chunk Attention)</strong>: This paper introduces a new training-free framework to allow large language models to handle long-context sequences by breaking down the attention computation into smaller chunks. (File: 1. DCA.pdf)</li><li><strong>MInference</strong>: This paper proposes a method to accelerate pre-filling for long-context language models by using a dynamic sparse attention mechanism with a context memory module. (File: 2. minference.pdf)</li><li><strong>InfLLM</strong>: This paper presents a training-free memory-based approach to enable large language models to understand extremely long sequences by incorporating an efficient context memory mechanism. (File: 4. infLLM.pdf)</li><li><strong>SnapKV</strong>: This paper introduces a fine-tuning-free approach to efficiently minimize the key-value (KV) cache size of large language models during generation while maintaining comparable performance. (File: 7. SnapKV.pdf)</li></ul><p><strong>Category 2: Benchmarking Long-Context Capabilities</strong></p><ul><li><strong>LV-Eval</strong>: This paper introduces a challenging long-context benchmark designed to evaluate the capabilities of large language models across various lengths and tasks, with a focus on mitigating knowledge leakage and providing a balanced evaluation. (File: 3. lveval.pdf)</li><li><strong>NeedleBench</strong>: This paper presents a benchmark for evaluating the retrieval and reasoning capabilities of large language models in long-context scenarios, including a needle-in-a-haystack task. (File: 5. needlebench.pdf)</li><li><strong>RULER</strong>: This paper proposes a synthetic benchmark for evaluating long-context language models with diverse task categories, including retrieval, multi-hop tracing, aggregation, and question answering. (File: 6. ruler.pdf)</li></ul></div><div class=role>User</div><div class=content>Output a JSON array where each element sequentially contains the paper title and a one-sentence summary</div><div class=role>Qwen2.5-Turbo</div><div class=content><div class=highlight><pre>[   
    {   
        &#34;title&#34; :  &#34;Dual Chunk Attention&#34; ,   
        &#34;summary&#34; :  &#34;DCA introduces a novel training-free framework to enable large language models to handle    
long-context sequences bydecomposing attention computation into smaller chunks.&#34;   
    },   
    {   
        &#34;title&#34; :  &#34;MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention&#34; ,   
        &#34;summary&#34; :  &#34;MInference uses a dynamic sparse attention mechanism with a context memory module to   
significantlyreduce the latency of long-context LLM inference.&#34;   
    },   
    {   
        &#34;title&#34; :  &#34;InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory&#34; ,   
        &#34;summary&#34; :  &#34;InfLLM leverages an efficient context memory to enable large language models to process   
extremelylong sequences without additional training.&#34;   
    },   
    {   
        &#34;title&#34; :  &#34;SnapKV: LLM Knows What You are Looking for Before Generation&#34; ,   
        &#34;summary&#34; :  &#34;SnapKV is a fine-tuning-free approach that efficiently compresses the KV cache to minimize   
memory usage while maintaining model performance.&#34;   
    },   
    {   
        &#34;title&#34; :  &#34;LV-EVAL: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K&#34; ,   
        &#34;summary&#34; :  &#34;LV-EVAL is a benchmark designed to evaluate large language models&#39; long-context capabilities   
across multiple lengths and tasks, focusing on mitigating knowledge leakage.&#34;   
    },   
    {   
        &#34;title&#34; :  &#34;NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?&#34; ,   
        &#34;summary&#34; :  &#34;NeedleBench assesses the retrieval and reasoning capabilities of large language models in   
long-context scenarios, including tasks like needle-in-a-haystack.&#34;   
    },   
    {   
        &#34;title&#34; :  &#34;RULER: What&#39;s the Real Context Size of Your Long-Context Language Models?&#34; ,   
        &#34;summary&#34; :  &#34;RULER is a synthetic benchmark that evaluates long-context language models through tasks   
like retrieval, multi-hop tracing, aggregation, and question answering.&#34;   
    }   
]   
</pre></div></div></div></div></div><h1 id=how-to-use-the-api>How to Use the API<a hidden class=anchor aria-hidden=true href=#how-to-use-the-api>#</a></h1><p>The latest Qwen2.5-Turbo supporting 1M tokens is used in the same way as the standard Qwen API and is compatible with the OpenAI API. Below is a simple Python call example (Note: You need to set the environment variable <code>YOUR_API_KEY</code> to your API Key. For more details, please visit <a href=https://help.aliyun.com/zh/model-studio/getting-started/first-api-call-to-qwen>Quick Start of Alibaba Cloud Model Studio [Chinese]</a>):</p><div class=highlight><pre>import  os   
  
from  openai  import  OpenAI   
  
# Input a long text file   
with  open ( &#34;example.txt&#34; ,  &#34;r&#34; ,  encoding = &#34;utf-8&#34; )  as  f :   
    text  =  f . read ()   
user_input  =  text  +  &#34; \n\n Summarize the above text.&#34;   
  
client  =  OpenAI (   
    api_key = os . getenv ( &#34;YOUR_API_KEY&#34; ),   
    base_url = &#34;https://dashscope.aliyuncs.com/compatible-mode/v1&#34; ,   
)   
  
completion  =  client . chat . completions . create (   
    model = &#34;qwen-turbo-latest&#34; ,   
    messages = [   
      { &#39;role&#39; :  &#39;system&#39; ,  &#39;content&#39; :  &#39;You are a helpful assistant.&#39; },   
      { &#39;role&#39; :  &#39;user&#39; ,  &#39;content&#39; :  user_input },   
    ],   
)   
  
print ( completion . choices [ 0 ] . message )   
</pre></div><h1 id=model-performance>Model Performance<a hidden class=anchor aria-hidden=true href=#model-performance>#</a></h1><p>In this section, we evaluate the performance of Qwen2.5-Turbo through various benchmark tests and improvements in inference speed.</p><h2 id=passkey-retrieval>Passkey Retrieval<a hidden class=anchor aria-hidden=true href=#passkey-retrieval>#</a></h2><p>We first conducted experiments on the 1M-token Passkey Retrieval task. The results show that Qwen2.5-Turbo can perfectly capture all hidden numbers in the 1M tokens of irrelevant text, demonstrating the model&rsquo;s ability to capture detailed information in ultra-long contexts.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/passkey_retrieval.png#center width=100%></figure><h2 id=more-complex-long-text-tasks>More Complex Long Text Tasks<a hidden class=anchor aria-hidden=true href=#more-complex-long-text-tasks>#</a></h2><p>We select several datasets of long text understanding to test the model, including:</p><ul><li><a href=https://github.com/hsiehjackson/RULER>RULER</a>: An extended benchmark based on Needle in a Haystack, tasks include finding multiple &ldquo;needles&rdquo; in irrelevant contexts, answering multiple questions, or finding the most or least frequent words in the context. The maximum context length is 128K.</li><li><a href=https://github.com/infinigence/LVEval>LV-Eval</a>: A benchmark test requiring simultaneous understanding of numerous evidence fragments. We adjust the evaluation metrics in the original version of LV-Eval to avoid false negatives caused by overly strict matching rules. The maximum context length is 256K.</li><li><a href=https://github.com/THUDM/LongAlign>LongbenchChat</a>: A dataset evaluating human preference alignment in the tasks of long context. The maximum context length is 100K.</li></ul><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/long_result.png#center width=100%></figure><p>The results show that Qwen2.5-Turbo has advantages in various tasks of long context:</p><ul><li>In the RULER benchmark test, Qwen2.5-Turbo scores 93.1, surpassing GPT-4o-mini and even GPT-4, proving its excellent ability to handle long text tasks.</li><li>In more tasks of long context understanding like LV-Eval and LongBench-Chat, Qwen2.5-Turbo surpasses GPT-4o-mini in most dimensions and can process tasks with a context of over 128K tokens.</li></ul><h2 id=short-text-tasks>Short Text Tasks<a hidden class=anchor aria-hidden=true href=#short-text-tasks>#</a></h2><p>In addition to performance improvements in tasks of long context, we are also concerned about the model&rsquo;s performance in tasks of short context. The existing context length extension methods often lead to significant performance degradation when processing short texts. Therefore, we have paid special attention to this issue when building Qwen2.5-Turbo, ensuring that the extension of context length almost does not affect the ability of short text understanding.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/short_result.png#center width=100%></figure><p>Results on short text benchmarks show that Qwen2.5-Turbo significantly surpasses previous open-source models with a context length of 1M tokens in most tasks; compared to GPT-4o-mini and Qwen2.5-14B-Instruct models, Qwen2.5-Turbo achieves similar performance in short text tasks while supporting 8 times the context length.</p><h2 id=inference-speed>Inference Speed<a hidden class=anchor aria-hidden=true href=#inference-speed>#</a></h2><p>We tested the TTFT (time to first token) for inputs of different lengths. On the sequences of 1M tokens, we used sparse attention to compress the computation of the attention by about 12.5 times, achieving a speedup of 3.2 to 4.3 times under different hardware configurations.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Turbo/inference_speed.png#center width=70%></figure><h1 id=whats-next>What&rsquo;s Next?<a hidden class=anchor aria-hidden=true href=#whats-next>#</a></h1><p>While we are pleased to finally extend the context of Qwen2.5-Turbo to 1M tokens, we also recognize that the current model does not always perform satisfactorily when solving long sequence tasks in real applications. There are many unresolved challenges, such as the model&rsquo;s performance being more unstable in long sequence tasks, and the cost of inference making it difficult to use larger models. However, we will actively explore further alignment of human preferences in long sequences, optimize inference efficiency to reduce computation time, and attempt to launch larger and stronger long-context models. We look forward to sharing new progress in developing long-context models with you soon, so stay tuned!</p></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="http://qwenlm.github.io/blog/qwen2.5-coder-family/"
                    >Qwen2.5-Coder Series: Powerful, Diverse, Practical.</a>
                </h2>

                                    <time datetime="2024-11-11 16:00:02">
                        2024-11-11 16:00:02                    </time>
                    <p></p>
                
                
                <!-- Intentionally not escaping for html context -->
                <div class=post-content><p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/32b-top.jpg#center width=100%></figure><a href=https://github.com/QwenLM/Qwen2.5-Coder class="btn external" target=_blank>GITHUB</a> <a href=https://huggingface.co/collections/Qwen/qwen25-coder-66eaa22e6f99801bf65b0c2f class="btn external" target=_blank>HUGGING FACE</a> <a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a> <a href=https://www.kaggle.com/models/qwen-lm/qwen2.5-coder class="btn external" target=_blank>KAGGLE</a> <a href=https://huggingface.co/spaces/Qwen/Qwen2.5-Coder-demo class="btn external" target=_blank>DEMO</a> <a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Today, we are excited to open source the &ldquo;Powerful&rdquo;, &ldquo;Diverse&rdquo;, and &ldquo;Practical&rdquo; Qwen2.5-Coder series, dedicated to continuously promoting the development of Open CodeLLMs.</p><ul><li><strong>Powerful</strong>: Qwen2.5-Coder-32B-Instruct has become the current SOTA open-source code model, matching the coding capabilities of GPT-4o. While demonstrating strong and comprehensive coding abilities, it also possesses good general and mathematical skills;</li><li><strong>Diverse</strong>: Building on the previously open-sourced two sizes of 1.5B / 7B, this release brings four model sizes, including 0.5B / 3B / 14B / 32B. As of now, Qwen2.5-Coder has covered six mainstream model sizes to meet the needs of different developers;</li><li><strong>Practical</strong>: We explore the practicality of Qwen2.5-Coder in two scenarios, including code assistants and Artifacts, with some examples showcasing the potential applications of Qwen2.5-Coder in real-world scenarios;</li></ul><h2 id=powerful-code-capabilities-reach-sota-for-open-source-models>Powerful: Code capabilities reach SOTA for open-source models<a hidden class=anchor aria-hidden=true href=#powerful-code-capabilities-reach-sota-for-open-source-models>#</a></h2><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/32b-main.png#center width=100%></figure><ul><li><p><strong>Code Generation</strong>: Qwen2.5-Coder-32B-Instruct, as the flagship model of this open-source release, has achieved the best performance among open-source models on multiple popular code generation benchmarks (EvalPlus, LiveCodeBench, BigCodeBench), and has competitive performance with GPT-4o.</p></li><li><p><strong>Code Repair</strong>: Code repair is an important programming skill. Qwen2.5-Coder-32B-Instruct can help users fix errors in their code, making programming more efficient. Aider is a popular benchmark for code repair, and Qwen2.5-Coder-32B-Instruct scored 73.7, performing comparably to GPT-4o on Aider.</p></li><li><p><strong>Code Reasoning</strong>: Code reasoning refers to the model&rsquo;s ability to learn the process of code execution and accurately predict the model&rsquo;s inputs and outputs. The recently released Qwen2.5-Coder-7B-Instruct has already shown impressive performance in code reasoning, and this 32B model takes it a step further.<figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/32b-crux.png#center width=80%></figure></p></li><li><p><strong>Multiple Programming Languages</strong>: An intelligent programming assistant should be familiar with all programming languages. Qwen2.5-Coder-32B-Instruct performs excellently across more than 40 programming languages, scoring 65.9 on McEval, with impressive performances in languages like Haskell and Racket, thanks to our unique data cleaning and balancing during the pre-training phase.<br><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/32b-mceval.jpg#center width=80%></figure></p></li></ul><p>Additionally, the multi-language code repair capabilities of Qwen2.5-Coder-32B-Instruct remain impressive, aiding users in understanding and modifying programming languages they are familiar with, significantly reducing the learning cost of unfamiliar languages. Similar to McEval, MdEval is a multi-language code repair benchmark, where Qwen2.5-Coder-32B-Instruct scored 75.2, ranking first among all open-source models.<figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/32b-mdeval.jpg#center width=80%></figure></p><ul><li><strong>Human Preference Alignment</strong>: To evaluate the alignment performance of Qwen2.5-Coder-32B-Instruct with human preferences, we constructed an internal annotated code preference evaluation benchmark called Code Arena (similar to Arena Hard). We used GPT-4o as the evaluation model for preference alignment, employing an &lsquo;A vs. B win&rsquo; evaluation method, which measures the percentage of instances in the test set where model A&rsquo;s score exceeds model B&rsquo;s. The results below demonstrate the advantages of Qwen2.5-Coder-32B-Instruct in preference alignment.</li></ul><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/32b-arena.jpg#center width=80%></figure><h2 id=diverse-rich-model-sizes>Diverse: Rich Model Sizes<a hidden class=anchor aria-hidden=true href=#diverse-rich-model-sizes>#</a></h2><p>This time, Qwen2.5-Coder has open-sourced a rich variety of model sizes, including 0.5B/1.5B/3B/7B/14B/32B, which not only meets the needs of developers in different resource scenarios but also provides a good experimental platform for the research community. The following table provides detailed model information:</p><table><thead><tr><th style=text-align:left>Models</th><th style=text-align:center>Params</th><th style=text-align:center>Non-Emb Params</th><th style=text-align:center>Layers</th><th style=text-align:center>Heads (KV)</th><th style=text-align:center>Tie Embedding</th><th style=text-align:center>Context Length</th><th style=text-align:center>License</th></tr></thead><tbody><tr><td style=text-align:left>Qwen2.5-Coder-0.5B</td><td style=text-align:center>0.49B</td><td style=text-align:center>0.36B</td><td style=text-align:center>24</td><td style=text-align:center>14 / 2</td><td style=text-align:center>Yes</td><td style=text-align:center>32K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-Coder-1.5B</td><td style=text-align:center>1.54B</td><td style=text-align:center>1.31B</td><td style=text-align:center>28</td><td style=text-align:center>12 / 2</td><td style=text-align:center>Yes</td><td style=text-align:center>32K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-Coder-3B</td><td style=text-align:center>3.09B</td><td style=text-align:center>2.77B</td><td style=text-align:center>36</td><td style=text-align:center>16 / 2</td><td style=text-align:center>Yes</td><td style=text-align:center>32K</td><td style=text-align:center>Qwen Research</td></tr><tr><td style=text-align:left>Qwen2.5-Coder-7B</td><td style=text-align:center>7.61B</td><td style=text-align:center>6.53B</td><td style=text-align:center>28</td><td style=text-align:center>28 / 4</td><td style=text-align:center>No</td><td style=text-align:center>128K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-Coder-14B</td><td style=text-align:center>14.7B</td><td style=text-align:center>13.1B</td><td style=text-align:center>48</td><td style=text-align:center>40 / 8</td><td style=text-align:center>No</td><td style=text-align:center>128K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-Coder-32B</td><td style=text-align:center>32.5B</td><td style=text-align:center>31.0B</td><td style=text-align:center>64</td><td style=text-align:center>40 / 8</td><td style=text-align:center>No</td><td style=text-align:center>128K</td><td style=text-align:center>Apache 2.0</td></tr></tbody></table><p>We have always believed in the philosophy of Scaling Law. We evaluated the performance of different sizes of Qwen2.5-Coder across all datasets to verify the effectiveness of Scaling in Code LLMs. For each size, we open-sourced both Base and Instruct models, where the Instruct model serves as an official aligned model that can chat directly, and the Base model serves as a foundation for developers to fine-tune their own models.</p><p>Here are the performances of the Base models of different sizes:<figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/qwen2.5-coder-family-base.png#center width=100%></figure></p><p>Here are the performances of the Instruct models of different sizes:<figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/qwen2.5-coder-family-instruct.png#center width=100%></figure></p><p>We present a comparison of different sizes of Qwen2.5-Coder with other open-source models on core datasets.</p><ul><li>For the Base model, we chose MBPP-3shot as the evaluation metric. Our extensive experiments show that MBPP-3shot is more suitable for evaluating base models and correlates well with the actual performance of the models.</li><li>For the Instruct model, we selected the latest 4 months of LiveCodeBench (2024.07 - 2024.11) questions as the evaluation, which are the latest published questions that could not have leaked into the training set, reflecting the model&rsquo;s OOD capabilities.</li></ul><p>There is a positive correlation between model size and model performance, and Qwen2.5-Coder has achieved SOTA performance across all sizes, encouraging us to continue exploring larger sizes of Coder.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/qwen2.5-coder-family-dual.jpg#center width=100%></figure><h2 id=practical-encountering-cursor-and-artifacts>Practical: Encountering Cursor and Artifacts<a hidden class=anchor aria-hidden=true href=#practical-encountering-cursor-and-artifacts>#</a></h2><p>A practical Coder has always been our vision, and for this reason, we explored the actual performance of Qwen2.5-Coder in code assistants and Artifacts scenarios.</p><h3 id=qwen25-coder--cursor>Qwen2.5-Coder ü§ù Cursor<a hidden class=anchor aria-hidden=true href=#qwen25-coder--cursor>#</a></h3><p>Code assistants have become widely used, but most currently rely on closed-source models. We hope that the emergence of Qwen2.5-Coder can provide developers with a friendly and powerful option. Here is an example of Qwen2.5-Coder in the <a href=https://www.cursor.com/>Cursor</a>.</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Qwen2.5-Coder ü§ù Cursor</span></div><div class=grid-layout><div class=role></div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/cases/final-game_of_life.mp4></video></figure></div></div></div></div><p>Additionally, Qwen2.5-Coder-32B has demonstrated strong code completion capabilities on pre-trained models, achieving SOTA performance on a total of 5 benchmarks: Humaneval-Infilling, CrossCodeEval, CrossCodeLongEval, RepoEval, and SAFIM. To maintain a fair comparison, we controlled the maximum sequence length to 8k and used the Fill-in-the-Middle mode for testing. Among the four evaluation sets of CrossCodeEval, CrossCodeLongEval, RepoEval, and Humaneval-Infilling, we evaluated whether the generated content was exactly equal to the true labels (Exact Match). In SAFIM, we used the one-time execution success rate (Pass@1) for evaluation.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/32b-fim.jpg#center width=80%></figure><h3 id=qwen25-coder--artifacts>Qwen2.5-Coder ü§ù Artifacts<a hidden class=anchor aria-hidden=true href=#qwen25-coder--artifacts>#</a></h3><p>Artifacts are an important application of code generation, helping users create visual works. We chose <a href=https://openwebui.com/>Open WebUI</a> to explore the potential of Qwen2.5-Coder in the Artifacts scenario, and here are some specific examples.</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Three-body Problem Simulation</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role></div><div class=content><figure><video controls loop src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/cases/final-3body.mp4 autoplay></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Lissajous Curve</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role></div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/cases/final-lissajous.mp4></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Drafting a resume</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role></div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/cases/final-cv.mp4></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Emoji dancing</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role></div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/cases/final-dancing.mp4></video></figure></div></div></div></div><p>We will soon launch the code mode on the Tongyi official website <a href=https://tongyi.aliyun.com>https://tongyi.aliyun.com</a>, supporting one-click generation of websites, mini-games, and data charts, among other visual applications. We welcome everyone to experience it!</p><h2 id=model-license>Model License<a hidden class=anchor aria-hidden=true href=#model-license>#</a></h2><p>Qwen2.5-Coder 0.5B / 1.5B / 7B / 14B / 32B are licensed under <strong>Apache 2.0</strong>, while 3B is under Qwen-Research license;</p><h2 id=whats-next-for-qwen-coder>What&rsquo;s Next for Qwen-Coder?<a hidden class=anchor aria-hidden=true href=#whats-next-for-qwen-coder>#</a></h2><p>We believe that this release can truly help developers and explore more interesting application scenarios with the community. Additionally, we are delving into powerful reasoning models centered around code, and we believe we will meet everyone soon!</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><pre tabindex=0><code>@article{hui2024qwen2,   title={Qwen2. 5-Coder Technical Report},   author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},   journal={arXiv preprint arXiv:2409.12186},   year={2024} } @article{yang2024qwen2,   title={Qwen2 technical report},   author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},   journal={arXiv preprint arXiv:2407.10671},   year={2024} } </code></pre></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="http://qwenlm.github.io/blog/qwen2.5/"
                    >Qwen2.5: A Party of Foundation Models!</a>
                </h2>

                                    <time datetime="2024-09-18 16:00:04">
                        2024-09-18 16:00:04                    </time>
                    <p></p>
                
                
                <!-- Intentionally not escaping for html context -->
                <div class=post-content><video width=100% autoplay loop muted playsinline> <source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/qwen2-main-video.m4v type=video/mp4></video><p><a href=https://github.com/QwenLM/Qwen2.5 class="btn external" target=_blank>GITHUB</a> <a href=https://huggingface.co/Qwen class="btn external" target=_blank>HUGGING FACE</a> <a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a> <a href=https://huggingface.co/spaces/Qwen/Qwen2.5 class="btn external" target=_blank>DEMO</a> <a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>In the past three months since Qwen2&rsquo;s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: <strong>Qwen2.5</strong>. We are announcing what might be the largest opensource release in history! Let&rsquo;s get the party started!</p><p>Our latest release features the LLMs <strong>Qwen2.5</strong>, along with specialized models for coding, <strong>Qwen2.5-Coder</strong>, and mathematics, <strong>Qwen2.5-Math</strong>. All open-weight models are dense, decoder-only language models, available in various sizes, including:</p><ul><li>Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B</li><li>Qwen2.5-Coder: 1.5B, 7B, and 32B on the way</li><li>Qwen2.5-Math: 1.5B, 7B, and 72B.</li></ul><br><p>All our open-source models, except for the 3B and 72B variants, are licensed under Apache 2.0. You can find the license files in the respective Hugging Face repositories. In addition to these models, we offer APIs for our flagship language models: <strong>Qwen-Plus</strong> and <strong>Qwen-Turbo</strong> through Model Studio, and we encourage you to explore them! Furthermore, we have also open-sourced the <strong>Qwen2-VL-72B</strong>, which features performance enhancements compared to last month&rsquo;s release.</p><p>For more details about Qwen2.5, Qwen2.5-Coder, and Qwen2.5-Math, feel free to visit the following links:</p><p><a href=https://qwenlm.github.io/blog/qwen2.5-llm class="btn external" target=_blank>Qwen2.5 LLM</a> <a href=https://qwenlm.github.io/blog/qwen2.5-coder class="btn external" target=_blank>Qwen2.5-Coder</a> <a href=https://qwenlm.github.io/blog/qwen2.5-math class="btn external" target=_blank>Qwen2.5-Math</a></p><br><p>Get ready to unlock a world of possibilities with our extensive lineup of models! We&rsquo;re excited to share these cutting-edge models with you, and we can&rsquo;t wait to see the incredible things you&rsquo;ll achieve with them!</p><h1 id=takeaways>Takeaways<a hidden class=anchor aria-hidden=true href=#takeaways>#</a></h1><p>In terms of <strong>Qwen2.5</strong>, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to <strong>18 trillion</strong> tokens. Compared to Qwen2, Qwen2.5 has acquired significantly more knowledge (MMLU: 85+) and has greatly improved capabilities in coding (HumanEval 85+) and mathematics (MATH 80+). Additionally, the new models achieve significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. Qwen2.5 models are generally more resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots. Like Qwen2, the Qwen2.5 language models support up to <strong>128K</strong> tokens and can generate up to <strong>8K</strong> tokens. They also maintain multilingual support for over <strong>29</strong> languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. Below, we provide basic information about the models and details of the supported languages.</p><p>The specialized expert language models, namely <strong>Qwen2.5-Coder</strong> for coding and <strong>Qwen2.5-Math</strong> for mathematics, have undergone substantial enhancements compared to their predecessors, CodeQwen1.5 and Qwen2-Math. Specifically, Qwen2.5-Coder has been trained on <strong>5.5 trillion</strong> tokens of code-related data, enabling even smaller coding-specific models to deliver competitive performance against larger language models on coding evaluation benchmarks. Meanwhile, Qwen2.5-Math supports both <strong>Chinese</strong> and <strong>English</strong> and incorporates various reasoning methods, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Tool-Integrated Reasoning (TIR).</p><figure><img src=http://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5%20modelcard.001.jpeg alt="Qwen2.5 Specification" width=100%></figure><h1 id=performance>Performance<a hidden class=anchor aria-hidden=true href=#performance>#</a></h1><h2 id=qwen25>Qwen2.5<a hidden class=anchor aria-hidden=true href=#qwen25>#</a></h2><p>To showcase Qwen2.5&rsquo;s capabilities, we benchmark our largest open-source model, <strong>Qwen2.5-72B</strong> - a 72B-parameter dense decoder-only language model - against leading open-source models like Llama-3.1-70B and Mistral-Large-V2. We present comprehensive results from instruction-tuned versions across various benchmarks, evaluating both model capabilities and human preferences.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-72B-Instruct-Score.jpg alt="Qwen2.5-72B Instruct Performance" width=100%></figure><p>Besides the instruction-tuned language models, we figure out that the base language model of our flagship opensource model Qwen2.5-72B reaches top-tier performance even against larger models like Llama-3-405B.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-72B-base.001.jpeg alt="Qwen2.5-72B Base Model Performance" width=100%></figure><p>Furthermore, we benchmark the latest version of our API-based model, <strong>Qwen-Plus</strong>, against leading proprietary and open-source models, including GPT4-o, Claude-3.5-Sonnet, Llama-3.1-405B, and DeepSeek-V2.5. This comparison showcases Qwen-Plus&rsquo;s competitive standing in the current landscape of large language models. We show that <strong>Qwen-Plus</strong> significantly outcompetes DeepSeek-V2.5 and demonstrates competitive performance against Llama-3.1-405B, while still underperforming compared to GPT4-o and Claude-3.5-Sonnet in some aspects. This benchmarking not only highlights Qwen-Plus&rsquo;s strengths but also identifies areas for future improvement, reinforcing our commitment to continuous enhancement and innovation in the field of large language models.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/qwen-plus-instruct.001.jpeg alt="Qwen-Plus Instruct Performance" width=100%></figure><p>A significant update in Qwen2.5 is the reintroduction of our 14B and 32B models, <strong>Qwen2.5-14B</strong> and <strong>Qwen2.5-32B</strong>. These models outperform baseline models of comparable or larger sizes, such as Phi-3.5-MoE-Instruct and Gemma2-27B-IT, across diverse tasks. They achieve an optimal balance between model size and capability, delivering performance that matches or exceeds some larger models. Additionally, our API-based model, <strong>Qwen-Turbo</strong>, offers highly competitive performance compared to the two open-source models, while providing a cost-effective and rapid service.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/qwen2.5-32B-instruct_wturbo.001.jpeg alt="Qwen2.5-32B Instruct Performance" width=100%></figure><p>In recent times, there has been a notable shift towards small language models (SLMs). Although SLMs have historically trailed behind their larger counterparts (LLMs), the performance gap is rapidly diminishing. Remarkably, even models with just 3 billion parameters are now delivering highly competitive results. The accompanying figure illustrates a significant trend: newer models achieving scores above 65 in MMLU are increasingly smaller, underscoring the accelerated growth in knowledge density among language models. Notably, our <strong>Qwen2.5-3B</strong> stands out as a prime example, achieving impressive performance with only around 3 billion parameters, showcasing its efficiency and capability compared to its predecessors.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/qwen2.5-small.jpg alt="Qwen2.5 Small Model" width=100%></figure><p>In addition to the notable enhancements in benchmark evaluations, we have refined our post-training methodologies. Our four key updates include support for long text generation of up to 8K tokens, significantly improved comprehension of structured data, more reliable generation of structured outputs, particularly in JSON format, and enhanced performance across diverse system prompts, which facilitates effective role-playing. Check the LLM blog for details about how to leverage these capabilities.</p><h2 id=qwen25-coder>Qwen2.5-Coder<a hidden class=anchor aria-hidden=true href=#qwen25-coder>#</a></h2><p>Since the launch of CodeQwen1.5, we have attracted numerous users who rely on this model for various coding tasks, such as debugging, answering coding-related questions, and providing code suggestions. Our latest iteration, Qwen2.5-Coder, is specifically designed for coding applications. In this section, we present the performance results of Qwen2.5-Coder-7B-Instruct, benchmarked against leading open-source models, including those with significantly larger parameter sizes.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder/coder-main.png alt="Qwen2.5-Coder Instruct Performance" width=100%></figure><p>We believe that Qwen2.5-Coder is an excellent choice as your personal coding assistant. Despite its smaller size, it outperforms many larger language models across a range of programming languages and tasks, demonstrating its exceptional coding capabilities.</p><h2 id=qwen25-math>Qwen2.5-Math<a hidden class=anchor aria-hidden=true href=#qwen25-math>#</a></h2><p>In terms of the math specific language models, we released the first models, Qwen2-Math, last month, and this time, compared to Qwen2-Math, Qwen2.5-Math has been pretrained larger-scale of math related data, including the synthetic data generated by Qwen2-Math. Additionally we extend the support of Chinese this time and we also strengthen its reasoning capabilities by endowing it with the abilities to perform CoT, PoT, and TIR. The general performance of Qwen2.5-Math-72B-Instruct surpasses both Qwen2-Math-72B-Instruct and GPT4-o, and even very small expert model like Qwen2.5-Math-1.5B-Instruct can achieve highly competitive performance against large language models.</p><figure><img src=http://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/2024-08-qwen2.5-math-allsize.png alt="Qwen2.5 Math Performance Across All Sizes" width=100%></figure><h1 id=develop-with-qwen25>Develop with Qwen2.5<a hidden class=anchor aria-hidden=true href=#develop-with-qwen25>#</a></h1><p>The simplest way to use is through <a href>Hugging Face Transfomer</a> as demonstrated in the <a href=https://huggingface.co/Qwen/Qwen2.5-7B-Instruct>model card</a>:</p><div class=highlight><pre>from  transformers  import  AutoModelForCausalLM ,  AutoTokenizer   
model_name  =  &#34;Qwen/Qwen2.5-7B-Instruct&#34;   
model  =  AutoModelForCausalLM . from_pretrained (   
    model_name ,   
    torch_dtype = &#34;auto&#34; ,   
    device_map = &#34;auto&#34;   
)   
tokenizer  =  AutoTokenizer . from_pretrained ( model_name )   
prompt  =  &#34;Give me a short introduction to large language model.&#34;   
messages  =  [   
    { &#34;role&#34; :  &#34;user&#34; ,  &#34;content&#34; :  prompt }   
]   
text  =  tokenizer . apply_chat_template (   
    messages ,   
    tokenize = False ,   
    add_generation_prompt = True   
)   
model_inputs  =  tokenizer ([ text ],  return_tensors = &#34;pt&#34; ) . to ( model . device )   
generated_ids  =  model . generate (   
    ** model_inputs ,   
    max_new_tokens = 512   
)   
generated_ids  =  [   
    output_ids [ len ( input_ids ):]  for  input_ids ,  output_ids  in  zip ( model_inputs . input_ids ,  generated_ids )   
]   
response  =  tokenizer . batch_decode ( generated_ids ,  skip_special_tokens = True )[ 0 ]   
</pre></div><p>To use Qwen2.5 with vLLM, running the following command can deploy an OpenAI API compatible service:</p><div class=highlight><pre>python  - m  vllm . entrypoints . openai . api_server  \  
    -- model  Qwen / Qwen2 .5 - 7 B - Instruct   
</pre></div><p>or use <code>vllm serve</code> if you use <code>vllm>=0.5.3</code>. Then you can communicate with Qwen2.5 via <code>curl</code>:</p><div class=highlight><pre>curl http://localhost:8000/v1/chat/completions -H &#34;Content-Type: application/json&#34;  -d &#39;{   
  &#34;model&#34;: &#34;Qwen/Qwen2.5-7B-Instruct&#34;,   
  &#34;messages&#34;: [   
    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Tell me something about large language models.&#34;}   
  ],   
  &#34;temperature&#34;: 0.7,   
  &#34;top_p&#34;: 0.8,   
  &#34;repetition_penalty&#34;: 1.05,   
  &#34;max_tokens&#34;: 512   
}&#39;   
</pre></div><p>Furthermore, Qwen2.5 supports vllm&rsquo;s built-in tool calling. This functionality requires <code>vllm>=0.6</code>. If you want to enable this functionality, please start vllm&rsquo;s OpenAI-compatible service with:</p><div class=highlight><pre>vllm serve Qwen/Qwen2.5-7B-Instruct --enable-auto-tool-choice --tool-call-parser hermes  
</pre></div><p>You can then use it in the same way you use <a href=https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models>GPT&rsquo;s tool calling</a>.</p><p>Qwen2.5 also supports <a href=https://ollama.com/blog/tool-support>Ollama&rsquo;s tool calling</a>. You can use it by starting Ollama&rsquo;s OpenAI-compatible service and using it in the same way you use GPT&rsquo;s tool calling.</p><p>Qwen2.5&rsquo;s chat template also includes a tool calling template, meaning that you can use Hugging Face <a href=https://huggingface.co/docs/transformers/main/en/chat_templating#advanced-tool-use--function-calling>transformers&rsquo; tool calling support</a>.</p><p>The vllm / Ollama / transformers tool calling support uses a tool calling template inspired by <a href=https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B>Nous&rsquo; Hermes</a>. Historically, <a href=https://github.com/QwenLM/Qwen-Agent>Qwen-Agent</a> provided tool calling support using Qwen2&rsquo;s own tool calling template (which is harder to be integrated with vllm and Ollama), and Qwen2.5 maintains compatibility with Qwen2&rsquo;s template and Qwen-Agent as well.</p><br><h1 id=friends-of-qwen>Friends of Qwen<a hidden class=anchor aria-hidden=true href=#friends-of-qwen>#</a></h1><p>üíó Qwen is nothing without its friends! So many thanks to the support of these old buddies and new friends :</p><ul><li><p><a href=https://huggingface.co/>Hugging Face Transformers</a></p></li><li><p>Finetuning: <a href=https://github.com/huggingface/peft>Peft</a>, <a href=https://github.com/alibaba/ChatLearn/>ChatLearn</a>, <a href=https://github.com/hiyouga/LLaMA-Factory>Llama-Factory</a>, <a href=https://github.com/OpenAccess-AI-Collective/axolotl>Axolotl</a>, <a href=https://github.com/yangjianxin1/Firefly>Firefly</a>, <a href=https://github.com/modelscope/swift>Swift</a>, <a href=https://github.com/InternLM/xtuner>XTuner</a>, <a href=https://unsloth.ai/>Unsloth</a>, <a href=https://github.com/linkedin/Liger-Kernel>Liger Kernel</a></p></li><li><p>Quantization: <a href=https://github.com/AutoGPTQ/AutoGPTQ>AutoGPTQ</a>, <a href=https://github.com/casper-hansen/AutoAWQ>AutoAWQ</a>, <a href=https://github.com/intel/neural-compressor>Neural Compressor</a></p></li><li><p>Deployment: <a href=https://github.com/vllm-project/vllm>vLLM</a>, <a href=https://github.com/sgl-project/sglang>SGL</a>, <a href=https://github.com/skypilot-org/skypilot>SkyPilot</a>, <a href=https://github.com/NVIDIA/TensorRT-LLM>TensorRT-LLM</a>, <a href=https://github.com/openvinotoolkit/openvino>OpenVino</a>, <a href=https://github.com/huggingface/text-generation-inference>TGI</a>, <a href=https://inference.readthedocs.io/>Xinference</a></p></li><li><p>API Platforms: <a href=https://www.together.ai/>Together</a>, <a href=https://fireworks.ai/>Fireworks</a>, <a href=https://openrouter.ai/>OpenRouter</a>, <a href=https://siliconflow.cn/>Sillicon Flow</a></p></li><li><p>Local Run: <a href=https://github.com/ml-explore/mlx>MLX</a>, <a href=https://github.com/ggerganov/llama.cpp>Llama.cpp</a>, <a href=https://ollama.com/>Ollama</a>, <a href=https://lmstudio.ai/>LM Studio</a>, <a href=https://jan.ai/>Jan</a></p></li><li><p>Agent and RAG Frameworks: <a href=https://dify.ai/>Dify</a>, <a href=https://www.llamaindex.ai/>LlamaIndex</a>, <a href=https://www.crewai.com/>CrewAI</a></p></li><li><p>Evaluation: <a href=https://chat.lmsys.org/>LMSys</a>, <a href=https://opencompass.org.cn/home>OpenCompass</a>, <a href=https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard>Open LLM Leaderboard</a></p></li><li><p>Model Training: <a href=https://www.arcee.ai/>Arcee AI</a>, <a href=https://sailorllm.github.io/>Sailor</a>, <a href=https://huggingface.co/cognitivecomputations>Dolphin</a>, <a href=https://github.com/OpenBuddy/OpenBuddy>Openbuddy</a></p></li></ul><p>We would like to extend our heartfelt gratitude to the numerous teams and individuals who have contributed to Qwen, even if they haven&rsquo;t been specifically mentioned. Your support is invaluable, and we warmly invite more friends to join us in this exciting journey. Together, we can enhance collaboration and drive forward the research and development of the open-source AI community, making it stronger and more innovative than ever before.</p><h1 id=whats-next>What&rsquo;s Next?<a hidden class=anchor aria-hidden=true href=#whats-next>#</a></h1><p>While we are thrilled to launch numerous high-quality models simultaneously, we recognize that significant challenges remain. Our recent releases demonstrate our commitment to developing robust foundation models across language, vision-language, and audio-language domains. However, it is crucial to integrate these different modalities into a single model to enable seamless end-to-end processing of information across all three. Additionally, although we have made strides in enhancing reasoning capabilities through data scaling, we are inspired by the recent advancements in reinforcement learning (e.g., o1) and are dedicated to further improving our models&rsquo; reasoning abilities by scaling inference compute. We look forward to introducing you to the next generation of models soon! Stay tuned for more exciting developments!</p><h1 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h1><p>We are going to release the technical report for Qwen2.5 very soon. Before the release, feel free to cite our Qwen2 paper as well as this blog</p><pre tabindex=0><code>@misc{qwen2.5,     title = {Qwen2.5: A Party of Foundation Models},     url = {https://qwenlm.github.io/blog/qwen2.5/},     author = {Qwen Team},     month = {September},     year = {2024} } </code></pre><pre tabindex=0><code>@article{qwen2,   title={Qwen2 technical report},   author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},   journal={arXiv preprint arXiv:2407.10671},   year={2024} } </code></pre></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="http://qwenlm.github.io/blog/qwen2.5-llm/"
                    >Qwen2.5-LLM: Extending the boundary of LLMs</a>
                </h2>

                                    <time datetime="2024-09-18 16:00:03">
                        2024-09-18 16:00:03                    </time>
                    <p></p>
                
                
                <!-- Intentionally not escaping for html context -->
                <div class=post-content><p><a href=https://github.com/QwenLM/Qwen2.5 class="btn external" target=_blank>GITHUB</a> <a href=https://huggingface.co/Qwen class="btn external" target=_blank>HUGGING FACE</a> <a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a> <a href=https://huggingface.co/spaces/Qwen/Qwen2.5-72B-Instruct class="btn external" target=_blank>DEMO</a> <a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>In this blog, we delve into the details of our latest Qwen2.5 series language models. We have developed a range of decoder-only dense models, with seven of them open-sourced, spanning from 0.5B to 72B parameters. Our research indicates a significant interest among users in models within the 10-30B range for production use, as well as 3B models for mobile applications. To meet these demands, we are open-sourcing Qwen2.5-3B, Qwen2.5-14B, and Qwen2.5-32B. Furthermore, we are excited to offer additional models, including Qwen-Plus and Qwen-Turbo, available through API services via <a href=https://help.aliyun.com/zh/model-studio/developer-reference/what-is-qwen-llm>Alibaba Cloud Model Studio</a>.</p><p>Compared with the Qwen2 series, the Qwen2.5 series has the following upgrades:</p><ol><li><p><strong>Full-scale Open-source</strong>: Considering that users have a strong interest in models in the 10-30B range for production and 3B models for mobile applications, Qwen2.5, in addition to continuing to open source the four models of 0.5/1.5/7/72B of the same size as Qwen2, also added two medium-sized cost-effective models of <strong>Qwen2.5-14B</strong> and <strong>Qwen2.5-32B</strong> and a mobile-side model called <strong>Qwen2.5-3B</strong>. All models are highly competitive compared to open-source models of the same level. For example, Qwen2.5-32B beats Qwen2-72B and Qwen2.5-14B outperforms Qwen2-57B-A14B in our comprehensive evaluations.</p></li><li><p><strong>Larger and Higher Quality Pre-training Dataset</strong>: The size of the pre-training dataset is expanded from 7 trillion tokens to a maximum of <strong>18 trillion</strong> tokens.</p></li><li><p><strong>Knowledge Enhancement</strong>: Qwen2.5 has acquired significantly more knowledge. On MMLU benchmarks, Qwen2.5-7/72B are improved from 70.3 to <strong>74.2</strong> and 84.2 to <strong>86.1</strong> compared to Qwen2-7/72B. We observe that Qwen2.5 also has significant improvements on the GPQA/MMLU-Pro/MMLU-redux/ARC-c benchmarks.</p></li><li><p><strong>Coding Enhancement</strong>: Thanks to the technical breakthrough of Qwen2.5-Coder, Qwen2.5 has greatly improved capabilities in coding. Qwen2.5-72B-Instruct achieves <strong>55.5</strong>, <strong>75.1</strong>, and <strong>88.2</strong> scores on LiveCodeBench (2305-2409), MultiPL-E and MBPP, respectively, outperforming Qwen2-72B-Instruct with 32.2, 69.2, and 80.2.</p></li><li><p><strong>Math Enhancement</strong>: After integrating Qwen2-math&rsquo;s technology, the mathematical ability of Qwen2.5 has also been rapidly improved. On the MATH benchmark, the scores of Qwen2.5-7B/72B-Instruct have been increased from 52.9/69.0 of Qwen2-7B/72B-Instruct to <strong>75.5/83.1</strong>.</p></li><li><p><strong>Better Human Preference</strong>: Qwen2.5 is capable of generating responses that align more closely with human preferences. Specifically, the Arena-Hard score for Qwen2.5-72B-Instruct has increased significantly from <strong>48.1</strong> to <strong>81.2</strong>, and the MT-Bench score has improved from <strong>9.12</strong> to <strong>9.35</strong>, compared to Qwen2-72B-Instruct.</p></li><li><p><strong>Other Core Capabilities Enhancement</strong>: Qwen2.5 achieves significant improvements in <strong>instruction following</strong>, <strong>generating long texts</strong> (increased from 1k to over <strong>8K tokens</strong>), <strong>understanding structured data</strong> (e.g., tables), and <strong>generating structured outputs</strong>, especially JSON. Furthermore, Qwen2.5 models are generally more resilient to the diversity of <strong>system prompts</strong>, enhancing <strong>role-play</strong> implementation and <strong>condition-setting</strong> for chatbots.</p></li></ol><h1 id=model-card>Model Card<a hidden class=anchor aria-hidden=true href=#model-card>#</a></h1><p>Here is a model card detailing the key parameters of the Qwen2.5 LLM models. This release includes seven open-sourced models with sizes ranging from 0.5B to 72B. Most models support a context length of 128K (131,072) tokens and can generate up to 8K tokens, enabling the production of extensive text outputs. The majority of these models are licensed under Apache 2.0, while Qwen2.5-3B and Qwen2.5-72B are governed by the Qwen Research License and Qwen License, respectively.</p><table><thead><tr><th style=text-align:left>Models</th><th style=text-align:center>Params</th><th style=text-align:center>Non-Emb Params</th><th style=text-align:center>Layers</th><th style=text-align:center>Heads (KV)</th><th style=text-align:center>Tie Embedding</th><th style=text-align:center>Context Length</th><th style=text-align:center>Generation Length</th><th style=text-align:center>License</th></tr></thead><tbody><tr><td style=text-align:left>Qwen2.5-0.5B</td><td style=text-align:center>0.49B</td><td style=text-align:center>0.36B</td><td style=text-align:center>24</td><td style=text-align:center>14 / 2</td><td style=text-align:center>Yes</td><td style=text-align:center>32K</td><td style=text-align:center>8K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-1.5B</td><td style=text-align:center>1.54B</td><td style=text-align:center>1.31B</td><td style=text-align:center>28</td><td style=text-align:center>12 / 2</td><td style=text-align:center>Yes</td><td style=text-align:center>32K</td><td style=text-align:center>8K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-3B</td><td style=text-align:center>3.09B</td><td style=text-align:center>2.77B</td><td style=text-align:center>36</td><td style=text-align:center>16 / 2</td><td style=text-align:center>Yes</td><td style=text-align:center>32K</td><td style=text-align:center>8K</td><td style=text-align:center>Qwen Research</td></tr><tr><td style=text-align:left>Qwen2.5-7B</td><td style=text-align:center>7.61B</td><td style=text-align:center>6.53B</td><td style=text-align:center>28</td><td style=text-align:center>28 / 4</td><td style=text-align:center>No</td><td style=text-align:center>128K</td><td style=text-align:center>8K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-14B</td><td style=text-align:center>14.7B</td><td style=text-align:center>13.1B</td><td style=text-align:center>48</td><td style=text-align:center>40 / 8</td><td style=text-align:center>No</td><td style=text-align:center>128K</td><td style=text-align:center>8K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-32B</td><td style=text-align:center>32.5B</td><td style=text-align:center>31.0B</td><td style=text-align:center>64</td><td style=text-align:center>40 / 8</td><td style=text-align:center>No</td><td style=text-align:center>128K</td><td style=text-align:center>8K</td><td style=text-align:center>Apache 2.0</td></tr><tr><td style=text-align:left>Qwen2.5-72B</td><td style=text-align:center>72.7B</td><td style=text-align:center>70.0B</td><td style=text-align:center>80</td><td style=text-align:center>64 / 8</td><td style=text-align:center>No</td><td style=text-align:center>128K</td><td style=text-align:center>8K</td><td style=text-align:center>Qwen</td></tr></tbody></table><h1 id=performance>Performance<a hidden class=anchor aria-hidden=true href=#performance>#</a></h1><p>This section presents the performance metrics for both base language models and instruction-tuned models across various benchmark evaluations, encompassing a diverse array of domains and tasks.</p><h2 id=qwen25-base-language-model-evaluation>Qwen2.5 Base Language Model Evaluation<a hidden class=anchor aria-hidden=true href=#qwen25-base-language-model-evaluation>#</a></h2><p>The evaluation of base models primarily emphasizes their performance in natural language understanding, general question answering, coding, mathematics, scientific knowledge, reasoning, and multilingual capabilities.</p><p>The evaluation datasets include:</p><p><strong>General Tasks</strong>: MMLU (5-shot), MMLU-Pro (5-shot), MMLU-redux (5-shot), BBH (3-shot), ARC-C (25-shot), TruthfulQA (0-shot), Winogrande (5-shot), HellaSwag (10-shot)</p><p><strong>Math & Science Tasks</strong>: GPQA (5-shot), Theorem QA (5-shot), GSM8K (4-shot), MATH (4-shot)</p><p><strong>Coding Tasks</strong>: HumanEval (0-shot), HumanEval+ (0-shot), MBPP (0-shot), MBPP+ (0-shot), MultiPL-E (0-shot) (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript)</p><p><strong>Multilingual Tasks</strong>: Multi-Exam (M3Exam 5-shot, IndoMMLU 3-shot, ruMMLU 5-shot, mMMLU 5-shot), Multi-Understanding (BELEBELE 5-shot, XCOPA 5-shot, XWinograd 5-shot, XStoryCloze 0-shot, PAWS-X 5-shot), Multi-Mathematics (MGSM 8-shot), Multi-Translation (Flores-101 5-shot)</p><h3 id=qwen25-72b-performance>Qwen2.5-72B Performance<a hidden class=anchor aria-hidden=true href=#qwen25-72b-performance>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Llama-3-70B</th><th style=text-align:center>Mixtral-8x22B</th><th style=text-align:center>Llama-3-405B</th><th style=text-align:center>Qwen2-72B</th><th style=text-align:center><strong>Qwen2.5-72B</strong></th></tr></thead><tbody><tr><td style=text-align:left><em><strong>General Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>79.5</td><td style=text-align:center>77.8</td><td style=text-align:center>85.2</td><td style=text-align:center>84.2</td><td style=text-align:center><strong>86.1</strong></td></tr><tr><td style=text-align:left>MMLU-Pro</td><td style=text-align:center>52.8</td><td style=text-align:center>51.6</td><td style=text-align:center><strong>61.6</strong></td><td style=text-align:center>55.7</td><td style=text-align:center>58.1</td></tr><tr><td style=text-align:left>MMLU-redux</td><td style=text-align:center>75.0</td><td style=text-align:center>72.9</td><td style=text-align:center>-</td><td style=text-align:center>80.5</td><td style=text-align:center><strong>83.9</strong></td></tr><tr><td style=text-align:left>BBH</td><td style=text-align:center>81.0</td><td style=text-align:center>78.9</td><td style=text-align:center>85.9</td><td style=text-align:center>82.4</td><td style=text-align:center><strong>86.3</strong></td></tr><tr><td style=text-align:left>ARC-C</td><td style=text-align:center>68.8</td><td style=text-align:center>70.7</td><td style=text-align:center>-</td><td style=text-align:center>68.9</td><td style=text-align:center><strong>72.4</strong></td></tr><tr><td style=text-align:left>TruthfulQA</td><td style=text-align:center>45.6</td><td style=text-align:center>51.0</td><td style=text-align:center>-</td><td style=text-align:center>54.8</td><td style=text-align:center><strong>60.4</strong></td></tr><tr><td style=text-align:left>WindoGrande</td><td style=text-align:center>85.3</td><td style=text-align:center>85.0</td><td style=text-align:center><strong>86.7</strong></td><td style=text-align:center>85.1</td><td style=text-align:center>83.9</td></tr><tr><td style=text-align:left>HellaSwag</td><td style=text-align:center>88.0</td><td style=text-align:center><strong>88.7</strong></td><td style=text-align:center>-</td><td style=text-align:center>87.3</td><td style=text-align:center>87.6</td></tr><tr><td style=text-align:left><em><strong>Mathematics & Science Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>36.3</td><td style=text-align:center>34.3</td><td style=text-align:center>-</td><td style=text-align:center>37.4</td><td style=text-align:center><strong>45.9</strong></td></tr><tr><td style=text-align:left>Theoremqa</td><td style=text-align:center>32.3</td><td style=text-align:center>35.9</td><td style=text-align:center>-</td><td style=text-align:center><strong>42.8</strong></td><td style=text-align:center>42.4</td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>42.5</td><td style=text-align:center>41.7</td><td style=text-align:center>53.8</td><td style=text-align:center>50.9</td><td style=text-align:center><strong>62.1</strong></td></tr><tr><td style=text-align:left>MMLU-stem</td><td style=text-align:center>73.7</td><td style=text-align:center>71.7</td><td style=text-align:center>-</td><td style=text-align:center>79.6</td><td style=text-align:center><strong>82.7</strong></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>77.6</td><td style=text-align:center>83.7</td><td style=text-align:center>89.0</td><td style=text-align:center>89.0</td><td style=text-align:center><strong>91.5</strong></td></tr><tr><td style=text-align:left><em><strong>Coding Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>48.2</td><td style=text-align:center>46.3</td><td style=text-align:center><strong>61.0</strong></td><td style=text-align:center>64.6</td><td style=text-align:center>59.1</td></tr><tr><td style=text-align:left>HumanEval+</td><td style=text-align:center>42.1</td><td style=text-align:center>40.2</td><td style=text-align:center>-</td><td style=text-align:center><strong>56.1</strong></td><td style=text-align:center>51.2</td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center>70.4</td><td style=text-align:center>71.7</td><td style=text-align:center>73.0</td><td style=text-align:center>76.9</td><td style=text-align:center><strong>84.7</strong></td></tr><tr><td style=text-align:left>MBPP+</td><td style=text-align:center>58.4</td><td style=text-align:center>58.1</td><td style=text-align:center>-</td><td style=text-align:center>63.9</td><td style=text-align:center><strong>69.2</strong></td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>46.3</td><td style=text-align:center>46.7</td><td style=text-align:center>-</td><td style=text-align:center>59.6</td><td style=text-align:center><strong>60.5</strong></td></tr><tr><td style=text-align:left><em><strong>Multilingual Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Multi-Exam</td><td style=text-align:center>70.0</td><td style=text-align:center>63.5</td><td style=text-align:center>-</td><td style=text-align:center>76.6</td><td style=text-align:center><strong>78.7</strong></td></tr><tr><td style=text-align:left>Multi-Understanding</td><td style=text-align:center>79.9</td><td style=text-align:center>77.7</td><td style=text-align:center>-</td><td style=text-align:center>80.7</td><td style=text-align:center><strong>89.6</strong></td></tr><tr><td style=text-align:left>Multi-Mathematics</td><td style=text-align:center>67.1</td><td style=text-align:center>62.9</td><td style=text-align:center>-</td><td style=text-align:center>76.0</td><td style=text-align:center><strong>76.7</strong></td></tr><tr><td style=text-align:left>Multi-Translation</td><td style=text-align:center>38.0</td><td style=text-align:center>23.3</td><td style=text-align:center>-</td><td style=text-align:center>37.8</td><td style=text-align:center><strong>39.0</strong></td></tr></tbody></table><p>The Qwen2.5-72B base model significantly outperforms its peers in the same category across a wide range of tasks. It achieves results comparable to Llama-3-405B while utilizing only one-fifth of the parameters. Furthermore, when compared to its predecessor, Qwen2-72B, the Qwen2.5-72B shows marked improvements in nearly all benchmark evaluations, particularly excelling in general tasks, mathematics, and coding challenges.</p><h3 id=qwen25-14b32b-performance>Qwen2.5-14B/32B Performance<a hidden class=anchor aria-hidden=true href=#qwen25-14b32b-performance>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Qwen1.5-32B</th><th style=text-align:center>Gemma2-27B</th><th style=text-align:center>Yi-1.5-34B</th><th style=text-align:center>Qwen2-57B-A14B</th><th style=text-align:center><strong>Qwen2.5-14B</strong></th><th style=text-align:center><strong>Qwen2.5-32B</strong></th></tr></thead><tbody><tr><td style=text-align:left><em><strong>General Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>74.3</td><td style=text-align:center>75.2</td><td style=text-align:center>77.2</td><td style=text-align:center>76.5</td><td style=text-align:center>79.7</td><td style=text-align:center><strong>83.3</strong></td></tr><tr><td style=text-align:left>MMLU-pro</td><td style=text-align:center>44.1</td><td style=text-align:center>49.1</td><td style=text-align:center>48.3</td><td style=text-align:center>43.0</td><td style=text-align:center>51.2</td><td style=text-align:center><strong>55.1</strong></td></tr><tr><td style=text-align:left>MMLU-redux</td><td style=text-align:center>69.0</td><td style=text-align:center>-</td><td style=text-align:center>74.1</td><td style=text-align:center>72.4</td><td style=text-align:center>76.6</td><td style=text-align:center><strong>82.0</strong></td></tr><tr><td style=text-align:left>BBH</td><td style=text-align:center>66.8</td><td style=text-align:center>74.9</td><td style=text-align:center>76.4</td><td style=text-align:center>67.0</td><td style=text-align:center>78.2</td><td style=text-align:center><strong>84.5</strong></td></tr><tr><td style=text-align:left>ARC-C</td><td style=text-align:center>63.6</td><td style=text-align:center><strong>71.4</strong></td><td style=text-align:center>65.6</td><td style=text-align:center>64.1</td><td style=text-align:center>67.3</td><td style=text-align:center>70.4</td></tr><tr><td style=text-align:left>Truthfulqa</td><td style=text-align:center>57.4</td><td style=text-align:center>40.1</td><td style=text-align:center>53.9</td><td style=text-align:center>57.7</td><td style=text-align:center><strong>58.4</strong></td><td style=text-align:center>57.8</td></tr><tr><td style=text-align:left>Winogrande</td><td style=text-align:center>81.5</td><td style=text-align:center>59.7</td><td style=text-align:center><strong>84.9</strong></td><td style=text-align:center>79.5</td><td style=text-align:center>-</td><td style=text-align:center>82.0</td></tr><tr><td style=text-align:left>Hellaswag</td><td style=text-align:center>85.0</td><td style=text-align:center><strong>86.4</strong></td><td style=text-align:center>85.9</td><td style=text-align:center>85.2</td><td style=text-align:center>-</td><td style=text-align:center>85.2</td></tr><tr><td style=text-align:left><em><strong>Mathematics & Science Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>30.8</td><td style=text-align:center>34.9</td><td style=text-align:center>37.4</td><td style=text-align:center>34.3</td><td style=text-align:center>32.8</td><td style=text-align:center><strong>48.0</strong></td></tr><tr><td style=text-align:left>Theoremqa</td><td style=text-align:center>28.8</td><td style=text-align:center>35.8</td><td style=text-align:center>40.0</td><td style=text-align:center>33.5</td><td style=text-align:center>43.0</td><td style=text-align:center><strong>44.1</strong></td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>36.1</td><td style=text-align:center>42.7</td><td style=text-align:center>41.7</td><td style=text-align:center>43.0</td><td style=text-align:center>55.6</td><td style=text-align:center><strong>57.7</strong></td></tr><tr><td style=text-align:left>MMLU-stem</td><td style=text-align:center>66.5</td><td style=text-align:center>71.0</td><td style=text-align:center>72.6</td><td style=text-align:center>69.8</td><td style=text-align:center>76.4</td><td style=text-align:center><strong>80.9</strong></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>78.5</td><td style=text-align:center>81.1</td><td style=text-align:center>81.7</td><td style=text-align:center>80.7</td><td style=text-align:center>90.2</td><td style=text-align:center><strong>92.9</strong></td></tr><tr><td style=text-align:left><em><strong>Coding Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>43.3</td><td style=text-align:center>54.9</td><td style=text-align:center>46.3</td><td style=text-align:center>53.0</td><td style=text-align:center>56.7</td><td style=text-align:center><strong>58.5</strong></td></tr><tr><td style=text-align:left>HumanEval+</td><td style=text-align:center>40.2</td><td style=text-align:center>46.3</td><td style=text-align:center>40.2</td><td style=text-align:center>46.3</td><td style=text-align:center>51.2</td><td style=text-align:center><strong>52.4</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center>64.2</td><td style=text-align:center>75.7</td><td style=text-align:center>65.5</td><td style=text-align:center>71.9</td><td style=text-align:center>76.7</td><td style=text-align:center><strong>84.5</strong></td></tr><tr><td style=text-align:left>MBPP+</td><td style=text-align:center>53.9</td><td style=text-align:center>60.2</td><td style=text-align:center>55.4</td><td style=text-align:center>57.4</td><td style=text-align:center>63.2</td><td style=text-align:center><strong>67.2</strong></td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>38.5</td><td style=text-align:center>48.0</td><td style=text-align:center>39.5</td><td style=text-align:center>49.8</td><td style=text-align:center>53.5</td><td style=text-align:center><strong>59.4</strong></td></tr><tr><td style=text-align:left><em><strong>Multilingual Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Multi-Exam</td><td style=text-align:center>61.6</td><td style=text-align:center>65.8</td><td style=text-align:center>58.3</td><td style=text-align:center>65.5</td><td style=text-align:center>70.6</td><td style=text-align:center><strong>75.4</strong></td></tr><tr><td style=text-align:left>Multi-Understanding</td><td style=text-align:center>76.5</td><td style=text-align:center>82.2</td><td style=text-align:center>73.9</td><td style=text-align:center>77.0</td><td style=text-align:center>85.9</td><td style=text-align:center><strong>88.4</strong></td></tr><tr><td style=text-align:left>Multi-Mathematics</td><td style=text-align:center>56.1</td><td style=text-align:center>61.6</td><td style=text-align:center>49.3</td><td style=text-align:center>62.3</td><td style=text-align:center>68.5</td><td style=text-align:center><strong>73.7</strong></td></tr><tr><td style=text-align:left>Multi-Translation</td><td style=text-align:center>33.5</td><td style=text-align:center>38.7</td><td style=text-align:center>30.0</td><td style=text-align:center>34.5</td><td style=text-align:center>36.2</td><td style=text-align:center><strong>37.3</strong></td></tr></tbody></table><p>The Qwen2.5-14B model demonstrates a solid performance across various tasks, particularly excelling in general tasks like MMLU and BBH, where it achieves scores of 79.7 and 78.2, outcompeting competitors of larger sizes. Meanwhile, Qwen2.5-32B, in particular, showcases exceptional capabilities, often surpassing larger models of similar model sizes. Notably, it outperforms its predecessor Qwen1.5-32B significantly, especially in challenging areas such as mathematics and coding, with notable scores of 57.7 in MATH and 84.5 in MBPP.</p><h3 id=qwen25-7b-performance>Qwen2.5-7B Performance<a hidden class=anchor aria-hidden=true href=#qwen25-7b-performance>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Mistral-7B</th><th style=text-align:center>Llama3-8B</th><th style=text-align:center>Gemma2-9B</th><th style=text-align:center>Qwen2-7B</th><th style=text-align:center><strong>Qwen2.5-7B</strong></th></tr></thead><tbody><tr><td style=text-align:left>#Non-emb Params</td><td style=text-align:center>7.0B</td><td style=text-align:center>7.0B</td><td style=text-align:center>8.2B</td><td style=text-align:center>6.5B</td><td style=text-align:center>6.5B</td></tr><tr><td style=text-align:left><em><strong>General Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>64.2</td><td style=text-align:center>66.6</td><td style=text-align:center>71.3</td><td style=text-align:center>70.3</td><td style=text-align:center><strong>74.2</strong></td></tr><tr><td style=text-align:left>MMLU-pro</td><td style=text-align:center>30.9</td><td style=text-align:center>35.4</td><td style=text-align:center>44.7</td><td style=text-align:center>40.1</td><td style=text-align:center><strong>45.0</strong></td></tr><tr><td style=text-align:left>MMLU-redux</td><td style=text-align:center>58.1</td><td style=text-align:center>61.6</td><td style=text-align:center>67.9</td><td style=text-align:center>68.1</td><td style=text-align:center><strong>71.1</strong></td></tr><tr><td style=text-align:left>BBH</td><td style=text-align:center>56.1</td><td style=text-align:center>57.7</td><td style=text-align:center>68.2</td><td style=text-align:center>62.3</td><td style=text-align:center><strong>70.4</strong></td></tr><tr><td style=text-align:left>ARC-C</td><td style=text-align:center>60.0</td><td style=text-align:center>59.3</td><td style=text-align:center><strong>68.2</strong></td><td style=text-align:center>60.6</td><td style=text-align:center>63.7</td></tr><tr><td style=text-align:left>Trurhfulqa</td><td style=text-align:center>42.2</td><td style=text-align:center>44.0</td><td style=text-align:center>45.3</td><td style=text-align:center>54.2</td><td style=text-align:center><strong>56.4</strong></td></tr><tr><td style=text-align:left>Winogrande</td><td style=text-align:center>78.4</td><td style=text-align:center>77.4</td><td style=text-align:center><strong>79.5</strong></td><td style=text-align:center>77.0</td><td style=text-align:center>75.9</td></tr><tr><td style=text-align:left>Hellaswag</td><td style=text-align:center><strong>83.3</strong></td><td style=text-align:center>82.1</td><td style=text-align:center>81.9</td><td style=text-align:center>80.7</td><td style=text-align:center>80.2</td></tr><tr><td style=text-align:left><em><strong>Mathematics & Science Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>24.7</td><td style=text-align:center>25.8</td><td style=text-align:center>32.8</td><td style=text-align:center>30.8</td><td style=text-align:center><strong>36.4</strong></td></tr><tr><td style=text-align:left>Theoremqa</td><td style=text-align:center>19.2</td><td style=text-align:center>22.1</td><td style=text-align:center>28.9</td><td style=text-align:center>29.6</td><td style=text-align:center><strong>36.0</strong></td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>10.2</td><td style=text-align:center>20.5</td><td style=text-align:center>37.7</td><td style=text-align:center>43.5</td><td style=text-align:center><strong>49.8</strong></td></tr><tr><td style=text-align:left>MMLU-stem</td><td style=text-align:center>50.1</td><td style=text-align:center>55.3</td><td style=text-align:center>65.1</td><td style=text-align:center>64.2</td><td style=text-align:center><strong>72.3</strong></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>36.2</td><td style=text-align:center>55.3</td><td style=text-align:center>70.7</td><td style=text-align:center>80.2</td><td style=text-align:center><strong>85.4</strong></td></tr><tr><td style=text-align:left><em><strong>Coding Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>29.3</td><td style=text-align:center>33.5</td><td style=text-align:center>37.8</td><td style=text-align:center>51.2</td><td style=text-align:center><strong>57.9</strong></td></tr><tr><td style=text-align:left>HumanEval+</td><td style=text-align:center>24.4</td><td style=text-align:center>29.3</td><td style=text-align:center>30.5</td><td style=text-align:center>43.3</td><td style=text-align:center><strong>50.6</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center>51.1</td><td style=text-align:center>53.9</td><td style=text-align:center>62.2</td><td style=text-align:center>64.2</td><td style=text-align:center><strong>74.9</strong></td></tr><tr><td style=text-align:left>MBPP+</td><td style=text-align:center>40.9</td><td style=text-align:center>44.4</td><td style=text-align:center>50.6</td><td style=text-align:center>51.9</td><td style=text-align:center><strong>62.9</strong></td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>29.4</td><td style=text-align:center>22.6</td><td style=text-align:center>34.9</td><td style=text-align:center>41.0</td><td style=text-align:center><strong>50.3</strong></td></tr><tr><td style=text-align:left><em><strong>Multilingual Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Multi-Exam</td><td style=text-align:center>47.1</td><td style=text-align:center>52.3</td><td style=text-align:center><strong>61.2</strong></td><td style=text-align:center>59.2</td><td style=text-align:center>59.4</td></tr><tr><td style=text-align:left>Multi-Understanding</td><td style=text-align:center>63.3</td><td style=text-align:center>68.6</td><td style=text-align:center>78.3</td><td style=text-align:center>72.0</td><td style=text-align:center><strong>79.3</strong></td></tr><tr><td style=text-align:left>Multi-Mathematics</td><td style=text-align:center>26.3</td><td style=text-align:center>36.3</td><td style=text-align:center>53.0</td><td style=text-align:center>57.5</td><td style=text-align:center><strong>57.8</strong></td></tr><tr><td style=text-align:left>Multi-Translation</td><td style=text-align:center>23.3</td><td style=text-align:center>31.9</td><td style=text-align:center><strong>36.5</strong></td><td style=text-align:center>31.5</td><td style=text-align:center>32.4</td></tr></tbody></table><p>The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.</p><h3 id=qwen25-05b15b3b-performance>Qwen2.5-0.5B/1.5B/3B Performance<a hidden class=anchor aria-hidden=true href=#qwen25-05b15b3b-performance>#</a></h3><table><thead><tr><th style=text-align:left>Datasets</th><th style=text-align:center>Qwen2-0.5B</th><th style=text-align:center><strong>Qwen2.5-0.5B</strong></th><th style=text-align:center>Qwen2-1.5B</th><th style=text-align:center><strong>Qwen2.5-1.5B</strong></th><th style=text-align:center>Gemma2-2.6B</th><th style=text-align:center><strong>Qwen2.5-3B</strong></th></tr></thead><tbody><tr><td style=text-align:left><em><strong>General Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MMLU</td><td style=text-align:center>44.3</td><td style=text-align:center>47.5</td><td style=text-align:center>55.9</td><td style=text-align:center>60.9</td><td style=text-align:center>52.2</td><td style=text-align:center><strong>65.6</strong></td></tr><tr><td style=text-align:left>MMLU-pro</td><td style=text-align:center>14.7</td><td style=text-align:center>15.7</td><td style=text-align:center>21.6</td><td style=text-align:center>28.5</td><td style=text-align:center>23.0</td><td style=text-align:center><strong>34.6</strong></td></tr><tr><td style=text-align:left>MMLU-redux</td><td style=text-align:center>40.7</td><td style=text-align:center>45.1</td><td style=text-align:center>51.8</td><td style=text-align:center>58.5</td><td style=text-align:center>50.9</td><td style=text-align:center><strong>63.7</strong></td></tr><tr><td style=text-align:left>BBH</td><td style=text-align:center>18.2</td><td style=text-align:center>20.3</td><td style=text-align:center>36.5</td><td style=text-align:center>45.1</td><td style=text-align:center>41.9</td><td style=text-align:center><strong>56.3</strong></td></tr><tr><td style=text-align:left>ARC-C</td><td style=text-align:center>31.0</td><td style=text-align:center>35.6</td><td style=text-align:center>43.7</td><td style=text-align:center>54.7</td><td style=text-align:center>55.7</td><td style=text-align:center><strong>56.5</strong></td></tr><tr><td style=text-align:left>Trurhfulqa</td><td style=text-align:center>39.7</td><td style=text-align:center>40.2</td><td style=text-align:center>45.9</td><td style=text-align:center>46.6</td><td style=text-align:center>36.2</td><td style=text-align:center><strong>48.9</strong></td></tr><tr><td style=text-align:left>Winogrande</td><td style=text-align:center>56.9</td><td style=text-align:center>56.3</td><td style=text-align:center>65.0</td><td style=text-align:center>65.0</td><td style=text-align:center><strong>71.5</strong></td><td style=text-align:center>71.1</td></tr><tr><td style=text-align:left>Hellaswag</td><td style=text-align:center>49.1</td><td style=text-align:center>52.1</td><td style=text-align:center>67.0</td><td style=text-align:center>67.9</td><td style=text-align:center>74.6</td><td style=text-align:center><strong>74.6</strong></td></tr><tr><td style=text-align:left><em><strong>Mathematics & Science Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GPQA</td><td style=text-align:center>29.8</td><td style=text-align:center>24.8</td><td style=text-align:center>20.7</td><td style=text-align:center>24.2</td><td style=text-align:center>25.3</td><td style=text-align:center><strong>26.3</strong></td></tr><tr><td style=text-align:left>Theoremqa</td><td style=text-align:center>9.6</td><td style=text-align:center>16.0</td><td style=text-align:center>14.8</td><td style=text-align:center>22.1</td><td style=text-align:center>15.9</td><td style=text-align:center><strong>27.4</strong></td></tr><tr><td style=text-align:left>MATH</td><td style=text-align:center>11.2</td><td style=text-align:center>19.5</td><td style=text-align:center>21.6</td><td style=text-align:center>35.0</td><td style=text-align:center>18.3</td><td style=text-align:center><strong>42.6</strong></td></tr><tr><td style=text-align:left>MMLU-stem</td><td style=text-align:center>27.5</td><td style=text-align:center>39.8</td><td style=text-align:center>42.7</td><td style=text-align:center>54.8</td><td style=text-align:center>45.8</td><td style=text-align:center><strong>62.5</strong></td></tr><tr><td style=text-align:left>GSM8K</td><td style=text-align:center>36.4</td><td style=text-align:center>41.6</td><td style=text-align:center>46.9</td><td style=text-align:center>68.5</td><td style=text-align:center>30.3</td><td style=text-align:center><strong>79.1</strong></td></tr><tr><td style=text-align:left><em><strong>Coding Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>HumanEval</td><td style=text-align:center>22.6</td><td style=text-align:center>30.5</td><td style=text-align:center>34.8</td><td style=text-align:center>37.2</td><td style=text-align:center>19.5</td><td style=text-align:center><strong>42.1</strong></td></tr><tr><td style=text-align:left>HumanEval+</td><td style=text-align:center>18.9</td><td style=text-align:center>26.8</td><td style=text-align:center>29.9</td><td style=text-align:center>32.9</td><td style=text-align:center>15.9</td><td style=text-align:center><strong>36.0</strong></td></tr><tr><td style=text-align:left>MBPP</td><td style=text-align:center>33.1</td><td style=text-align:center>39.3</td><td style=text-align:center>46.9</td><td style=text-align:center><strong>60.2</strong></td><td style=text-align:center>42.1</td><td style=text-align:center>57.1</td></tr><tr><td style=text-align:left>MBPP+</td><td style=text-align:center>27.6</td><td style=text-align:center>33.8</td><td style=text-align:center>37.6</td><td style=text-align:center><strong>49.6</strong></td><td style=text-align:center>33.6</td><td style=text-align:center>49.4</td></tr><tr><td style=text-align:left>MultiPL-E</td><td style=text-align:center>16.3</td><td style=text-align:center>18.9</td><td style=text-align:center>27.9</td><td style=text-align:center>33.1</td><td style=text-align:center>17.6</td><td style=text-align:center><strong>41.2</strong></td></tr><tr><td style=text-align:left><em><strong>Multilingual Tasks</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Multi-Exam</td><td style=text-align:center>29.4</td><td style=text-align:center>30.8</td><td style=text-align:center>43.1</td><td style=text-align:center>47.9</td><td style=text-align:center>38.1</td><td style=text-align:center><strong>54.6</strong></td></tr><tr><td style=text-align:left>Multi-Understanding</td><td style=text-align:center>40.4</td><td style=text-align:center>41.0</td><td style=text-align:center>50.7</td><td style=text-align:center>65.1</td><td style=text-align:center>46.8</td><td style=text-align:center><strong>76.6</strong></td></tr><tr><td style=text-align:left>Multi-Mathematics</td><td style=text-align:center>7.8</td><td style=text-align:center>13.5</td><td style=text-align:center>21.3</td><td style=text-align:center>37.5</td><td style=text-align:center>18.2</td><td style=text-align:center><strong>48.9</strong></td></tr><tr><td style=text-align:left>Multi-Translation</td><td style=text-align:center>14.1</td><td style=text-align:center>15.3</td><td style=text-align:center>23.8</td><td style=text-align:center>25.0</td><td style=text-align:center>26.9</td><td style=text-align:center><strong>29.3</strong></td></tr></tbody></table><p>For edge-side models, Qwen2.5-0.5B, 1.5B, and 3B continue to maintain strong performance across nearly all benchmarks. Notably, the Qwen2.5-0.5B model outperforms the Gemma2-2.6B on various math and coding tasks.</p><h2 id=instruction-tuned-model-evaluation>Instruction-tuned Model Evaluation<a hidden class=anchor aria-hidden=true href=#instruction-tuned-model-evaluation>#</a></h2><p>The evaluation of instruction-tuned models mainly focuses on the model performance of natural language understanding, general question answering, reasoning, coding, mathematics, instruction following, human alignment, etc.</p><p>The datasets for evaluation include:</p><p><strong>General Tasks</strong>: MMLU-Pro, MMLU-redux</p><p><strong>Math & Science Tasks</strong>: GPQA, GSM8K, MATH</p><p><strong>Coding Tasks</strong>: HumanEval, MBPP, MultiPL-E, LiveCodeBench 2305-2409, LiveBench 0831</p><p><strong>Instruction & Alignment Tasks</strong>: IFeval strict-prompt, Arena-Hard, AlignBench v1.1, MTbench</p><h3 id=qwen25-72b-instruct-performance>Qwen2.5-72B-Instruct Performance<a hidden class=anchor aria-hidden=true href=#qwen25-72b-instruct-performance>#</a></h3><table><thead><tr><th>Datasets</th><th>Mistral-Large2 Instruct</th><th>Llama-3.1-70B-Instruct</th><th>Llama-3.1-405B-Instruct</th><th>Qwen2-72B-Instruct</th><th><strong>Qwen2.5-72B-Instruct</strong></th></tr></thead><tbody><tr><td>MMLU-Pro</td><td>69.4</td><td>66.4</td><td><strong>73.3</strong></td><td>64.4</td><td>71.1</td></tr><tr><td>MMLU-redux</td><td>83.0</td><td>83.0</td><td>86.2</td><td>81.6</td><td><strong>86.8</strong></td></tr><tr><td>GPQA</td><td><strong>52.0</strong></td><td>46.7</td><td>51.1</td><td>42.4</td><td>49.0</td></tr><tr><td>MATH</td><td>69.9</td><td>68.0</td><td>73.8</td><td>69.0</td><td><strong>83.1</strong></td></tr><tr><td>GSM8K</td><td>92.7</td><td>95.1</td><td><strong>96.8</strong></td><td>93.2</td><td>95.8</td></tr><tr><td>HumanEval</td><td><strong>92.1</strong></td><td>80.5</td><td>89.0</td><td>86.0</td><td>86.6</td></tr><tr><td>MBPP</td><td>80.0</td><td>84.2</td><td>84.5</td><td>80.2</td><td><strong>88.2</strong></td></tr><tr><td>MultiPL-E</td><td><strong>76.9</strong></td><td>68.2</td><td>73.5</td><td>69.2</td><td>75.1</td></tr><tr><td>LiveCodeBench 2305-2409</td><td>42.2</td><td>32.1</td><td>41.6</td><td>32.2</td><td><strong>55.5</strong></td></tr><tr><td>LiveBench 0831</td><td>48.5</td><td>46.6</td><td><strong>53.2</strong></td><td>41.5</td><td>52.3</td></tr><tr><td>IFeval strict-prompt</td><td>64.1</td><td>83.6</td><td><strong>86.0</strong></td><td>77.6</td><td>84.1</td></tr><tr><td>Arena-Hard</td><td>73.1</td><td>55.7</td><td>69.3</td><td>48.1</td><td><strong>81.2</strong></td></tr><tr><td>AlignBench v1.1</td><td>7.69</td><td>5.94</td><td>5.95</td><td>8.15</td><td><strong>8.16</strong></td></tr><tr><td>MTbench</td><td>8.61</td><td>8.79</td><td>9.08</td><td>9.12</td><td><strong>9.35</strong></td></tr></tbody></table><p>The Qwen2.5-72B-Instruct model delivers exceptional performance, even surpassing the larger Llama-3.1-405B in several critical tasks. Qwen2.5-72B-Instruct excels in mathematics (MATH: 83.1), coding (LiveCodeBench: 55.5), and chatting (Arena-Hard: 81.2). Compared to its base model Qwen2.5-72B and its predecessor Qwen2-72B-Instruct, the Qwen2.5-72B-Instruct showcases comprehensive improvements across all tasks.</p><h3 id=qwen-turbo--qwen25-14b-instruct--qwen25-32b-instruct-performance>Qwen-Turbo & Qwen2.5-14B-Instruct & Qwen2.5-32B-Instruct Performance<a hidden class=anchor aria-hidden=true href=#qwen-turbo--qwen25-14b-instruct--qwen25-32b-instruct-performance>#</a></h3><table><thead><tr><th>Datasets</th><th>Qwen2-57B-A14B-Instruct</th><th>Gemma2-27B-IT</th><th>GPT4o-mini</th><th><strong>Qwen-Turbo</strong></th><th><strong>Qwen2.5-14B-Instruct</strong></th><th><strong>Qwen2.5-32B-Instruct</strong></th></tr></thead><tbody><tr><td>MMLU-Pro</td><td>52.8</td><td>55.5</td><td>63.1</td><td>64.8</td><td>63.7</td><td><strong>69.0</strong></td></tr><tr><td>MMLU-redux</td><td>72.6</td><td>75.7</td><td>81.5</td><td>80.4</td><td>80.0</td><td><strong>83.9</strong></td></tr><tr><td>GPQA</td><td>34.3</td><td>38.4</td><td>40.2</td><td>44.4</td><td>45.5</td><td><strong>49.5</strong></td></tr><tr><td>MATH</td><td>49.1</td><td>54.4</td><td>70.2</td><td>81.0</td><td>80.0</td><td><strong>83.1</strong></td></tr><tr><td>GSM8K</td><td>85.3</td><td>90.4</td><td>93.2</td><td>93.6</td><td>94.8</td><td><strong>95.9</strong></td></tr><tr><td>HumanEval</td><td>79.9</td><td>78.7</td><td><strong>88.4</strong></td><td>86.6</td><td>83.5</td><td><strong>88.4</strong></td></tr><tr><td>MBPP</td><td>70.9</td><td>81.0</td><td><strong>85.7</strong></td><td>80.2</td><td>82.0</td><td>84.0</td></tr><tr><td>MultiPL-E</td><td>66.4</td><td>67.4</td><td>75.0</td><td>73.0</td><td>72.8</td><td><strong>75.4</strong></td></tr><tr><td>LiveCodeBench 2305-2409</td><td>22.5</td><td>-</td><td>40.7</td><td>43.1</td><td>42.6</td><td><strong>51.2</strong></td></tr><tr><td>LiveBench 0831</td><td>31.1</td><td>39.6</td><td>43.3</td><td>41.6</td><td>44.4</td><td><strong>50.7</strong></td></tr><tr><td>IFeval strict-prompt</td><td>59.9</td><td>77.1</td><td>80.4</td><td>74.9</td><td><strong>81.0</strong></td><td>79.5</td></tr><tr><td>Arena-Hard</td><td>17.8</td><td>57.5</td><td><strong>74.9</strong></td><td>68.4</td><td>68.3</td><td>74.5</td></tr><tr><td>AlignBench v1.1</td><td>7.02</td><td>7.22</td><td>7.81</td><td><strong>7.99</strong></td><td>7.94</td><td>7.93</td></tr><tr><td>MTbench</td><td>8.55</td><td>9.10</td><td>-</td><td>8.86</td><td>8.88</td><td><strong>9.20</strong></td></tr></tbody></table><p>The Qwen2.5-32B-Instruct model demonstrates superior performance across most tasks when compared to other models of similar size. In comparison to GPT-4o-mini, our open-source model, Qwen2.5-14B-Instruct, along with our API model, Qwen-Turbo, also deliver competitive results across all benchmarks.</p><h3 id=qwen25-7b-instruct-performance>Qwen2.5-7B-Instruct Performance<a hidden class=anchor aria-hidden=true href=#qwen25-7b-instruct-performance>#</a></h3><table><thead><tr><th>Datasets</th><th>Gemma2-9b-IT</th><th>Llama3.1-8B-Instruct</th><th>Qwen2-7B-Instruct</th><th><strong>Qwen2.5-7B-Instruct</strong></th></tr></thead><tbody><tr><td>MMLU-Pro</td><td>52.1</td><td>48.3</td><td>44.1</td><td><strong>56.3</strong></td></tr><tr><td>MMLU-redux</td><td>72.8</td><td>67.2</td><td>67.3</td><td><strong>75.4</strong></td></tr><tr><td>GPQA</td><td>32.8</td><td>32.8</td><td>34.3</td><td><strong>36.4</strong></td></tr><tr><td>MATH</td><td>44.3</td><td>51.9</td><td>52.9</td><td><strong>75.5</strong></td></tr><tr><td>GSM8K</td><td>76.7</td><td>84.5</td><td>85.7</td><td><strong>91.6</strong></td></tr><tr><td>HumanEval</td><td>68.9</td><td>72.6</td><td>79.9</td><td><strong>84.8</strong></td></tr><tr><td>MBPP</td><td>74.9</td><td>69.6</td><td>67.2</td><td><strong>79.2</strong></td></tr><tr><td>MultiPL-E</td><td>53.4</td><td>50.7</td><td>59.1</td><td><strong>70.4</strong></td></tr><tr><td>LiveCodeBench 2305-2409</td><td>18.9</td><td>8.3</td><td>23.9</td><td><strong>28.7</strong></td></tr><tr><td>LiveBench 0831</td><td>30.6</td><td>26.7</td><td>29.2</td><td><strong>35.9</strong></td></tr><tr><td>IFeval strict-prompt</td><td>70.1</td><td><strong>75.9</strong></td><td>54.7</td><td>71.2</td></tr><tr><td>Arena-Hard</td><td>41.6</td><td>27.8</td><td>25.0</td><td><strong>52.0</strong></td></tr><tr><td>AlignBench v1.1</td><td>7.05</td><td>4.75</td><td>7.13</td><td><strong>7.33</strong></td></tr><tr><td>MTbench</td><td>8.49</td><td>8.23</td><td>8.26</td><td><strong>8.75</strong></td></tr></tbody></table><p>The Qwen2.5-7B-Instruct model significantly outperforms its competitors, Gemma2-9b-IT and Llama3.1-8B-Instruct, across all tasks except IFeval. Notably, Qwen2.5-7B-Instruct demonstrates clear advantages in mathematics (MATH: 75.5) and coding (HumanEval: 84.8).</p><h3 id=qwen25-3b-instruct-performance>Qwen2.5-3B-Instruct Performance<a hidden class=anchor aria-hidden=true href=#qwen25-3b-instruct-performance>#</a></h3><table><thead><tr><th>Datasets</th><th>Gemma2-2B-IT</th><th>Phi3.5-mini-Instruct</th><th>MiniCPM3-4B</th><th><strong>Qwen2.5-3B-Instruct</strong></th></tr></thead><tbody><tr><td>Non-Emb Params</td><td>2.0B</td><td>3.6B</td><td>4.0B</td><td>2.8B</td></tr><tr><td>MMLU-Pro</td><td>26.7</td><td><strong>47.5</strong></td><td>43.0</td><td>43.7</td></tr><tr><td>MMLU-redux</td><td>51.9</td><td><strong>67.7</strong></td><td>59.9</td><td>64.4</td></tr><tr><td>GPQA</td><td>29.3</td><td>27.2</td><td><strong>31.3</strong></td><td>30.3</td></tr><tr><td>MATH</td><td>26.6</td><td>48.5</td><td>46.6</td><td><strong>65.9</strong></td></tr><tr><td>GSM8K</td><td>63.2</td><td>86.2</td><td>81.1</td><td><strong>86.7</strong></td></tr><tr><td>HumanEval</td><td>68.9</td><td>72.6</td><td><strong>74.4</strong></td><td><strong>74.4</strong></td></tr><tr><td>MBPP</td><td><strong>74.9</strong></td><td>63.2</td><td>72.5</td><td>72.7</td></tr><tr><td>MultiPL-E</td><td>30.5</td><td>47.2</td><td>49.1</td><td><strong>60.2</strong></td></tr><tr><td>LiveCodeBench 2305-2409</td><td>5.8</td><td>15.8</td><td><strong>23.8</strong></td><td>19.9</td></tr><tr><td>LiveBench 0831</td><td>20.1</td><td>27.4</td><td><strong>27.6</strong></td><td>26.8</td></tr><tr><td>IFeval strict-prompt</td><td>51.0</td><td>52.1</td><td><strong>68.4</strong></td><td>58.2</td></tr></tbody></table><p>As for the edge-side instruction model, the Qwen2.5-3B-Instruct model has fewer parameters than both the Phi3.5-mini-Instruct and MiniCPM3-4B models. Despite this, it outperforms them in mathematics and coding tasks while delivering competitive results in language understanding.</p><h3 id=qwen25-05b15b-instruct-performance>Qwen2.5-0.5B/1.5B-Instruct Performance<a hidden class=anchor aria-hidden=true href=#qwen25-05b15b-instruct-performance>#</a></h3><table><thead><tr><th>Datasets</th><th>Qwen2-0.5B-Instruct</th><th><strong>Qwen2.5-0.5B-Instruct</strong></th><th>Qwen2-1.5B-Instruct</th><th><strong>Qwen2.5-1.5B-Instruct</strong></th></tr></thead><tbody><tr><td>MMLU-Pro</td><td>14.4</td><td><strong>15.0</strong></td><td>22.9</td><td><strong>32.4</strong></td></tr><tr><td>MMLU-redux</td><td>12.9</td><td><strong>24.1</strong></td><td>41.2</td><td><strong>50.7</strong></td></tr><tr><td>GPQA</td><td>23.7</td><td><strong>29.8</strong></td><td>21.2</td><td><strong>29.8</strong></td></tr><tr><td>MATH</td><td>13.9</td><td><strong>34.4</strong></td><td>25.3</td><td><strong>55.2</strong></td></tr><tr><td>GSM8K</td><td>40.1</td><td><strong>49.6</strong></td><td>61.6</td><td><strong>73.2</strong></td></tr><tr><td>HumanEval</td><td>31.1</td><td><strong>35.4</strong></td><td>42.1</td><td><strong>61.6</strong></td></tr><tr><td>MBPP</td><td>39.7</td><td><strong>49.6</strong></td><td>44.2</td><td><strong>63.2</strong></td></tr><tr><td>MultiPL-E</td><td>20.8</td><td><strong>28.5</strong></td><td>38.5</td><td><strong>50.4</strong></td></tr><tr><td>LiveCodeBench 2305-2409</td><td>1.6</td><td><strong>5.1</strong></td><td>4.5</td><td><strong>14.8</strong></td></tr><tr><td>LiveBench 0831</td><td>7.4</td><td><strong>12.6</strong></td><td>12.4</td><td><strong>18.8</strong></td></tr><tr><td>IFeval strict-prompt</td><td>14.6</td><td><strong>27.9</strong></td><td>29.0</td><td><strong>42.5</strong></td></tr></tbody></table><p>Qwen2.5-1.5B-Instruct and Qwen2.5-0.5B-Instruct have seen large performance improvements over their previous versions, making them well-suited for edge-side applications in highly resource-constrained environments.</p><h3 id=performances-on-multilingualism>Performances on Multilingualism<a hidden class=anchor aria-hidden=true href=#performances-on-multilingualism>#</a></h3><p>To evaluate the multilingual performance of instruction-tuned models, we collect and extend benchmarks as follows:</p><ul><li><strong>IFEval (multilingual)</strong>: We translate the examples from IFEval (English version) to construct multilingual IFEval examples after removing examples with language-specific contents (e.g., &ldquo;start with letter A&rdquo;). We collect 100 examples for each language among Arabic (ar), Spanish (es), French (fr), Indonesian (in), Japanese (ja), Korean (ko), Portuguese (pt), and Vietnamese (vi) languages. All examples are checked and post-edited (if neccessary) by paid volunteers.</li><li><strong>Knowledge</strong>: We use 5 MMLU-like benchmarks (multi-choice) to testify the knowledge utilization ability of Qwen2.5 series models on multilingualism, including AMMLU (Arabic), JMMLU (Japanese), KMMLU (Korean), IndoMMLU (Indonesian), and TurkishMMLU (Turkish). Also, we present the performances on translated MMLU (i.e., okapi_MMLU, from English to multiple languages).</li><li><strong>MGSM8K (extended)</strong>: Aside from the examples in the original MGSM8K benchmark, we extend the language support with Arabic (ar), Korean (ko), Portuguese (pt), and Vietnamese (vi). We translate 250 examples (same as the other languages engaged in MGSM8K) into those 4 languages. All examples are also checked and post-edited (if necessary) by paid volunteers.</li><li><strong>Cultural Nuances</strong>: We also use BLEnD, a benchmark aiming at testifying cultural nuances of LLMs, to testify LLMs from the Qwen2.5 series.</li></ul><table><thead><tr><th style=text-align:center>Datasets</th><th style=text-align:center>Qwen2-72B-Instruct</th><th style=text-align:center>Llama3.1-70B-Instruct</th><th style=text-align:center>Qwen2.5-32B-Instruct</th><th style=text-align:center>Mistral-Large-Instruct-2407 (123B)</th><th style=text-align:center>GPT4o-mini</th><th style=text-align:center>Qwen2.5-72B-Instruct</th></tr></thead><tbody><tr><td style=text-align:center><em><strong>Instruction Following</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>IFEval (multilingual)</td><td style=text-align:center>79.69</td><td style=text-align:center>80.47</td><td style=text-align:center>82.68</td><td style=text-align:center>82.69</td><td style=text-align:center>85.03</td><td style=text-align:center><strong>86.98</strong></td></tr><tr><td style=text-align:center><em><strong>Knowledge</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>AMMLU (Arabic)</td><td style=text-align:center>68.85</td><td style=text-align:center>70.08</td><td style=text-align:center>70.44</td><td style=text-align:center>69.24</td><td style=text-align:center>69.73</td><td style=text-align:center><strong>72.44</strong></td></tr><tr><td style=text-align:center>JMMLU (Japanese)</td><td style=text-align:center>77.37</td><td style=text-align:center>73.89</td><td style=text-align:center>76.55</td><td style=text-align:center>75.77</td><td style=text-align:center>73.74</td><td style=text-align:center><strong>80.56</strong></td></tr><tr><td style=text-align:center>KMMLU (Korean)</td><td style=text-align:center>57.04</td><td style=text-align:center>53.23</td><td style=text-align:center>60.75</td><td style=text-align:center>56.42</td><td style=text-align:center>56.77</td><td style=text-align:center><strong>61.96</strong></td></tr><tr><td style=text-align:center>IndoMMLU (Indonesian)</td><td style=text-align:center>66.31</td><td style=text-align:center>67.50</td><td style=text-align:center>66.42</td><td style=text-align:center>63.21</td><td style=text-align:center>67.75</td><td style=text-align:center><strong>69.25</strong></td></tr><tr><td style=text-align:center>TurkishMMLU (Turkish)</td><td style=text-align:center>69.22</td><td style=text-align:center>66.89</td><td style=text-align:center>72.41</td><td style=text-align:center>64.78</td><td style=text-align:center>71.19</td><td style=text-align:center><strong>76.12</strong></td></tr><tr><td style=text-align:center>okapi MMLU (translated)</td><td style=text-align:center>77.84</td><td style=text-align:center>76.49</td><td style=text-align:center>77.16</td><td style=text-align:center>78.37</td><td style=text-align:center>73.44</td><td style=text-align:center><strong>79.97</strong></td></tr><tr><td style=text-align:center><em><strong>Math Reasoning</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>MGSM8K (extended)</td><td style=text-align:center>82.72</td><td style=text-align:center>73.31</td><td style=text-align:center>87.15</td><td style=text-align:center><strong>89.01</strong></td><td style=text-align:center>87.36</td><td style=text-align:center>88.16</td></tr><tr><td style=text-align:center><em><strong>Cultural Nuances</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>BLEnD</td><td style=text-align:center>25.90</td><td style=text-align:center>30.49</td><td style=text-align:center>27.88</td><td style=text-align:center>33.47</td><td style=text-align:center><strong>35.91</strong></td><td style=text-align:center>32.48</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Datasets</th><th style=text-align:center>Qwen2-7B-Instruct</th><th style=text-align:center>Llama3.1-8B-Instruct</th><th style=text-align:center>Qwen2.5-7B-Instruct</th><th style=text-align:center>Gemma-2-9B-Instruct</th><th style=text-align:center>Mistral-Nemo-Instruct-2407 (12B)</th><th style=text-align:center>Qwen2.5-14B-Instruct</th></tr></thead><tbody><tr><td style=text-align:center><em><strong>Instruction Following</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>IFEval (multilingual)</td><td style=text-align:center>51.43</td><td style=text-align:center>60.68</td><td style=text-align:center>74.87</td><td style=text-align:center><strong>77.47</strong></td><td style=text-align:center>64.59</td><td style=text-align:center>77.08</td></tr><tr><td style=text-align:center><em><strong>Knowledge</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>AMMLU (Arabic)</td><td style=text-align:center>54.87</td><td style=text-align:center>54.28</td><td style=text-align:center>59.78</td><td style=text-align:center>60.26</td><td style=text-align:center>53.92</td><td style=text-align:center><strong>66.81</strong></td></tr><tr><td style=text-align:center>JMMLU (Japanese)</td><td style=text-align:center>57.71</td><td style=text-align:center>53.26</td><td style=text-align:center>61.88</td><td style=text-align:center>64.59</td><td style=text-align:center>55.17</td><td style=text-align:center><strong>72.78</strong></td></tr><tr><td style=text-align:center>KMMLU (Korean)</td><td style=text-align:center>43.96</td><td style=text-align:center>42.28</td><td style=text-align:center>46.59</td><td style=text-align:center>46.24</td><td style=text-align:center>42.22</td><td style=text-align:center><strong>59.71</strong></td></tr><tr><td style=text-align:center>IndoMMLU (Indonesian)</td><td style=text-align:center>54.05</td><td style=text-align:center>53.92</td><td style=text-align:center>56.42</td><td style=text-align:center>61.73</td><td style=text-align:center>50.76</td><td style=text-align:center><strong>65.09</strong></td></tr><tr><td style=text-align:center>TurkishMMLU (Turkish)</td><td style=text-align:center>49.27</td><td style=text-align:center>45.61</td><td style=text-align:center>54.28</td><td style=text-align:center>55.44</td><td style=text-align:center>34.44</td><td style=text-align:center><strong>66.85</strong></td></tr><tr><td style=text-align:center>okapi MMLU (translated)</td><td style=text-align:center>60.47</td><td style=text-align:center>55.18</td><td style=text-align:center>66.98</td><td style=text-align:center>46.72</td><td style=text-align:center>59.65</td><td style=text-align:center><strong>72.12</strong></td></tr><tr><td style=text-align:center><em><strong>Math Reasoning</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>MGSM8K (extended)</td><td style=text-align:center>56.13</td><td style=text-align:center>66.05</td><td style=text-align:center>66.11</td><td style=text-align:center>78.37</td><td style=text-align:center>54.75</td><td style=text-align:center><strong>82.27</strong></td></tr><tr><td style=text-align:center><em><strong>Cultural Nuances</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:center>BLEnD</td><td style=text-align:center>22.49</td><td style=text-align:center>19.47</td><td style=text-align:center>23.66</td><td style=text-align:center><strong>28.31</strong></td><td style=text-align:center>26.61</td><td style=text-align:center>26.99</td></tr></tbody></table><h1 id=demo-cases>Demo Cases<a hidden class=anchor aria-hidden=true href=#demo-cases>#</a></h1><p>Here we provide several cases to demonstrate the new or enhanced capabilities of Qwen2.5, including generating JSON output, generating long texts, and understanding structured data.</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Generating JSON Output</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>JSON Output</div><div class=content><figure><video controls loop src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/json_output.mp4 autoplay></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Structured Data Understanding</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>Table Understanding</div><div class=content><figure><video controls loop src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/table_understanding.mp4 autoplay></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Long Text Generation</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>Text Generation</div><div class=content><figure><video controls loop src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/long_text.mp4 autoplay></video></figure></div></div></div></div></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="http://qwenlm.github.io/blog/qwen2.5-coder/"
                    >Qwen2.5-Coder: Code More, Learn More!</a>
                </h2>

                                    <time datetime="2024-09-18 16:00:02">
                        2024-09-18 16:00:02                    </time>
                    <p></p>
                
                
                <!-- Intentionally not escaping for html context -->
                <div class=post-content><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder/coder-main.png#center width=70%></figure><p><a href=https://github.com/QwenLM/Qwen2.5-Coder class="btn external" target=_blank>GITHUB</a> <a href=https://huggingface.co/Qwen class="btn external" target=_blank>HUGGING FACE</a> <a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a> <a href=https://huggingface.co/spaces/Qwen/Qwen2.5-Coder-7B-Instruct class="btn external" target=_blank>DEMO</a> <a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>In early April, we introduced CodeQwen1.5, which garnered significant attention from the community. Since then, we have been working to enhance the coding model. Today, we are excited to announce the release of the next generation of open-source coding models, <strong>Qwen2.5-Coder</strong>, and officially rename CodeQwen to Qwen-Coder. We think &ldquo;Coder&rdquo; is more human-like and agile, reflecting our vision of it becoming a true coding partner in the future. Qwen2.5-Coder is part of the Qwen2.5 series, available in three model sizes: 1.5B, 7B, and a 32B version (coming soon).</p><p>This update focuses on two main improvements: scaling up the code training data and enhancing coding capabilities while maintaining strong performance in other core areas like math and general tasks.</p><p>üíª Code More: Qwen2.5-Coder builds on the strong Qwen2.5 and continues training on a larger scale of code data, including source code, text-code grounding data, and synthetic data, totaling 5.5 trillion tokens. This leads to significant improvements in code-related tasks.</p><p>üìö Learn More: While enhancing coding abilities, we aimed to retain strengths in math and general capabilities from base model. Therefore, Qwen2.5-Coder incorporates additional data on mathematics and general abilities, providing a comprehensive foundation for real-world applications like Code Agent.</p><h1 id=qwen25-coder-base-models>Qwen2.5-Coder: Base Models<a hidden class=anchor aria-hidden=true href=#qwen25-coder-base-models>#</a></h1><p>Qwen2.5-Coder supports up to 128K tokens of context, covers 92 programming languages, and has achieved remarkable improvements across various code-related evaluation tasks, including code generation, multi-programming code generation, code completion, and code repair. Notably, the open-source 7B version of Qwen2.5-Coder has even outperformed larger models like DeepSeek-Coder-V2-Lite and CodeStral-22B, making it one of the most powerful base code models available. Beyond code tasks, Qwen2.5-Coder also demonstrates competitive math capabilities in evaluations such as GSM8K and Math. For general tasks, evaluations on MMLU and ARC show that Qwen2.5-Coder has retained the general ability performance of Qwen2.5.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder/qwen2.5-coder-base.jpg#center width=100%></figure><h1 id=qwen25-coder-instruct-instruction-tuned-models>Qwen2.5-Coder-Instruct: Instruction-Tuned Models<a hidden class=anchor aria-hidden=true href=#qwen25-coder-instruct-instruction-tuned-models>#</a></h1><p>Building on Qwen2.5-Coder, we fine-tuned it with instruction data, creating Qwen2.5-Coder-Instruct. This instruction-tuned model not only further improves task performance but also demonstrates exceptional generalization across various benchmarks.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder/qwen2.5-coder-instruct.jpg#center width=100%></figure><p>Qwen2.5-Coder-Instruct excels in several key areas:</p><ol><li><strong>Outstanding Multi-programming Expert</strong>: We expanded the multi-language evaluations using McEval, covering more than 40 programming languages. The results show that Qwen2.5-Coder-Instruct performs remarkably well across many languages, including niche ones.</li></ol><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder/mveval.jpg#center width=70%></figure><ol start=2><li><strong>Code Reasoning</strong>: We believe code reasoning is closely tied to general reasoning skills. We used CRUXEval as a benchmark, and the results show Qwen2.5-Coder-Instruct excels in code reasoning tasks. Interestingly, as code reasoning improves, the model&rsquo;s ability to follow complex instructions also gets better, encouraging us to further explore how code can enhance general skills.</li></ol><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder/crux.jpg#center width=70%></figure><ol start=3><li><strong>Math Reasoning</strong>: Math and code are often discussed together: math is the foundation of code, and code is a key tool for math. Qwen2.5-Coder-Instruct shines in both code and math tasks, proven to be a &ldquo;science student&rdquo;.</li></ol><table><thead><tr><th><strong>Model</strong></th><th><strong>Math</strong></th><th><strong>GSM8K</strong></th><th><strong>GaoKao2023en</strong></th><th><strong>OlympiadBench</strong></th><th><strong>CollegeMath</strong></th><th><strong>AIME24</strong></th></tr></thead><tbody><tr><td>DeepSeek-Coder-V2-Lite-Instruct</td><td>61.0</td><td><strong>87.6</strong></td><td>56.1</td><td>26.4</td><td>39.8</td><td>6.7</td></tr><tr><td>Qwen2.5-Coder-7B-Instruct</td><td><strong>66.8</strong></td><td>86.7</td><td><strong>60.5</strong></td><td><strong>29.8</strong></td><td><strong>43.5</strong></td><td><strong>10.0</strong></td></tr></tbody></table><ol start=4><li><strong>Basic capabilities</strong>: We also assessed the general capabilities, and the results indicate that Qwen2.5-Coder-Instruct maintains the advantages of Qwen2.5 in terms of general abilities.</li></ol><table><thead><tr><th><strong>Model</strong></th><th><strong>AMC23</strong></th><th><strong>MMLU-Pro</strong></th><th><strong>MMLU</strong></th><th><strong>IFEval</strong></th><th><strong>CEval</strong></th><th><strong>GPQA</strong></th></tr></thead><tbody><tr><td>DeepSeek-Coder-V2-Lite-Instruct</td><td>40.4</td><td>42.5</td><td>60.6</td><td>38.6</td><td>60.1</td><td>27.6</td></tr><tr><td>Qwen2.5-Coder-7B-Instruct</td><td><strong>42.5</strong></td><td><strong>45.6</strong></td><td><strong>68.7</strong></td><td><strong>58.6</strong></td><td><strong>61.4</strong></td><td><strong>35.6</strong></td></tr></tbody></table><h1 id=license>License<a hidden class=anchor aria-hidden=true href=#license>#</a></h1><p>Qwen2.5-Coder is released under the Apache 2.0 license. We hope this increased openness will accelerate its application in code intelligence.</p><h1 id=whats-next-for-qwen25-coder>What&rsquo;s Next for Qwen2.5-Coder?<a hidden class=anchor aria-hidden=true href=#whats-next-for-qwen25-coder>#</a></h1><p>We are preparing the 32B version of Qwen2.5-Coder, aiming to challenge proprietary models. Stay tuned‚Äîit&rsquo;s coming soon! Additionally, we&rsquo;re exploring powerful code-centric reasoning models to push the boundaries of code intelligence.</p><h1 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h1><pre tabindex=0><code>@article{hui2024qwen2,   title={Qwen2. 5-Coder Technical Report},   author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},   journal={arXiv preprint arXiv:2409.12186},   year={2024} } @article{yang2024qwen2,   title={Qwen2 technical report},   author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},   journal={arXiv preprint arXiv:2407.10671},   year={2024} } </code></pre></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="http://qwenlm.github.io/blog/qwen2.5-math/"
                    >Qwen2.5-Math: The world's leading open-sourced mathematical LLMs</a>
                </h2>

                                    <time datetime="2024-09-18 16:00:01">
                        2024-09-18 16:00:01                    </time>
                    <p></p>
                
                
                <!-- Intentionally not escaping for html context -->
                <div class=post-content><figure><img src=http://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/2024-08-qwen2.5-math-72B.png#center width=100%></figure><p><a href=https://github.com/QwenLM/Qwen2-Math class="btn external" target=_blank>GITHUB</a> <a href=https://huggingface.co/Qwen class="btn external" target=_blank>HUGGING FACE</a> <a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a> <a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><blockquote><div align=center><b>üö® Qwen2.5-Math mainly supports solving English and Chinese math problems through CoT and TIR. We do not recommend using this series of models for other tasks.</b></div></blockquote><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>A month ago, we released the first series of mathematical LLMs - <a href=https://qwenlm.github.io/blog/qwen2-math/>Qwen2-Math</a> - of our Qwen family. Today, we have upgraded it and open-sourced <strong>Qwen2.5-Math</strong> series, including base models <strong>Qwen2.5-Math-1.5B/7B/72B</strong>, instruction-tuned models <strong>Qwen2.5-Math-1.5B/7B/72B-Instruct</strong>, and mathematical reward model <strong>Qwen2.5-Math-RM-72B</strong>.</p><p>Unlike Qwen2-Math series which only supports using Chain-of-Thought (CoT) to solve English math problems, Qwen2.5-Math series is expanded to support using both CoT and Tool-integrated Reasoning (TIR) to solve math problems in both Chinese and English. The Qwen2.5-Math series models have achieved significant performance improvements compared to the Qwen2-Math series models on the Chinese and English mathematics benchmarks with CoT.</p><figure><img src=http://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/2024-08-qwen2.5-math-allsize.png#center width=100%></figure><p>While CoT plays a vital role in enhancing the reasoning capabilities of LLMs, it faces challenges in achieving computational accuracy and handling complex mathematical or algorithmic reasoning tasks, such as finding the roots of a quadratic equation or computing the eigenvalues of a matrix. TIR can further improve the model&rsquo;s proficiency in precise computation, symbolic manipulation, and algorithmic manipulation. Qwen2.5-Math-1.5B/7B/72B-Instruct achieve 79.7, 85.3, and 87.8 respectively on the MATH benchmark using TIR.</p><figure><img src=http://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/qwen2.5-math-pipeline.jpeg#center width=100%></figure><h2 id=qwen25-math-base-models>Qwen2.5-Math: Base Models<a hidden class=anchor aria-hidden=true href=#qwen25-math-base-models>#</a></h2><p>The overall specialization pipelines of Qwen2-Math and Qwen2.5-Math are shown in the figure above. After training of Qwen2-Math base models, we further upgrade them to Qwen2.5-Math models through three primary avenues:</p><ol><li><p>Utilizing Qwen2-Math-72B-Instruct models to synthesize additional high-quality mathematical pre-training data.</p></li><li><p>Aggregating more high-quality mathematical data, particularly in Chinese, from web sources, books, and codes across multiple recall cycles.</p></li><li><p>Leveraging the Qwen2.5 series base model for parameter initialization, which shows more powerful language understanding, code generation, and text reasoning capabilities.</p></li></ol><p>Ultimately, we construct <em>Qwen Math Corpus v2</em> for Qwen2.5-Math-1.5B/7B/72B pre-training, maintaining a context length of 4K. Compared to <em>Qwen Math Corpus v1</em> used for Qwen2-Math training, the total token count of <em>Qwen Math Corpus v2</em> has increased from 700B to over 1T.</p><p>We evaluate our Qwen2.5-Math base models on three widely used English math benchmarks GSM8K, Math, and MMLU-STEM. In addition, we also evaluate three Chinese math benchmarks CMATH, GaoKao Math Cloze, and GaoKao Math QA. All evaluations are tested with few-shot chain-of-thought prompting.</p><figure><img src=http://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/qwen2.5-math-table.png#center width=100%></figure><p>Compared to Qwen2-Math-1.5B/7B/72B, Qwen2.5-Math-1.5B/7B/72B have achieved significant improvements on all benchmarks. For example, Qwen2.5-Math-1.5B/7B/72B obtains 5.4, 5.0, 6.3 scores improvement on MATH, and 3.4, 12.2, 19.8 scores improvement on Gaokao Math QA.</p><h2 id=qwen25-math-instruct-instruction-tuned-models>Qwen2.5-Math-Instruct: Instruction-Tuned Models<a hidden class=anchor aria-hidden=true href=#qwen25-math-instruct-instruction-tuned-models>#</a></h2><p>Similar to Qwen2-Math-Instruct, we train a math-specific reward model Qwen2.5-Math-RM-72B based on Qwen2.5-Math-72B. This RM is used for constructing the SFT data through Rejection Sampling and also in the reinforcement learning with Group Relative Policy Optimization (GRPO) after SFT.</p><p>In the development of Qwen2.5-Math-Instruct, an additional iteration is conducted using the Qwen2-Math-Instruct models and Qwen2.5-Math-RM-72B to polish the quality of responses further during Rejection Sampling.</p><p>Compared with the post-training of Qwen2-Math, we further introduced TIR data and SFT data in Chinese and English for Qwen2.5 post-training.</p><p>We evaluate Qwen2.5-Math-Instruct on mathematical benchmarks in both English and Chinese. In addition to the widely-used benchmarks, such as GSM8K and Math, we also involve more exams that are more challenging to fully inspect the capabilities of Qwen2.5-Math-Instruct, such as OlympiadBench, CollegeMath, GaoKao, AIME2024, and AMC2023. For Chinese mathematical benchmarks, we use CMATH, Gaokao (Chinese College Entrance Examination 2024), and CN Middle School 24 (China High School Entrance Examination 2024).</p><p>We report greedy, Maj@8 and RM@8 performance on all benchmarks in the zero-shot setting, except for the multi-choice benchmarks (including MMLU STEM and multiple-choice problems in GaoKao and CN Middle School 24) with a 5-shot setting.</p><p>The Qwen2.5-Math-72B-Instruct model outperforms the Qwen2-Math-72B-Instruct model by an average margin of 4.4 and 6.1 points in English and Chinese, respectively, establishing itself as the best open-source mathematical model currently available.</p><p>The flagship model, Qwen2.5-Math-72B-Instruct, significantly outperforms both open-source models and leading closed-source models (e.g., GPT-4o, Gemini Math-Specialized 1.5 Pro). Under the TIR setting of RM@8, a high score of 92.9 was achieved on MATH.</p><p>With the aid of synthesized pre-training and supervised fine-tuning data from the 72B model, Qwen2.5-Math-7B-Instruct surpasses Qwen2-Math-Instruct 72B in performance. Under CoT and TIR settings, it achieves MATH scores of 83.6 and 85.3, respectively.</p><p>Even our smallest 1.5B model, achieves a MATH score of around 80 when utilizing the Python Interpreter, outperforming the majority of current models in this domain.</p><figure><img src=http://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/math_instruct_en.jpg#center width=120%></figure><figure><img src=http://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/math_instruct_zh.jpg#center width=90%></figure><p>In more complex mathematical competition evaluations such as AIME 2024 and AMC 2023, Qwen2.5-Math-Instruct also performs well across various settings, including Greedy, Maj@64, RM@64, and RM@256.</p><p>With the support of the Qwen2.5-Math-RM-72B, Qwen2.5-Math-1.5B-Instruct, using the RM@256 in CoT mode, successfully solves 29 out of 40 problems on AMC 2023.</p><p>Moreover, Qwen2.5-Math-72B-Instruct nearly achieves a perfect score in TIR mode, solving almost all the problems.</p><p>On the extremely difficult AIME 2024 benchmark, Claude3 Opus, GPT-4 Turbo, and Gemini 1.5 Pro manage to solve only 1 or 2 questions out of 30.</p><p>In contrast, Qwen2.5-Math-72B-Instruct solves 9 problems in Greedy decoding CoT mode and 12 problems in TIR mode. With the help of the RM, Qwen2.5-Math-7B-Instruct could even solve up to 21 problems, further demonstrating the outstanding mathematical problem-solving ability of Qwen2.5-Math-Instruct.</p><figure><img src=http://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/math_instruct_aime.jpg#center width=80%></figure><h2 id=decontamination>Decontamination<a hidden class=anchor aria-hidden=true href=#decontamination>#</a></h2><p>Decontamination is critical to ensuring unbiased model performance evaluation.</p><p>Following prior work Qwen2, we exclude potentially contaminated training samples using 13-gram matching. To improve the accuracy of this matching process, we perform text normalization, removing irrelevant punctuation and symbols.</p><p>To further reduce false negatives, particularly for common mathematical expressions, we introduce an additional criterion: the ratio of the longest common subsequence must exceed $0.6$ for a sample to be considered contaminated.</p><p>For pre-training data, we filter potentially contaminated samples against datasets such as GSM8K and MATH. When dealing with post-training data, including SFT data, RM training data, and the RL query set, we exclude any potentially contaminated problems or solutions across all reported evaluation datasets. These evaluation datasets include GSM8K, MATH, Minerva Math, Gaokao 2023 En, Olympiad Bench, College Math, MMLU STEM, GaoKao, CMATH, CN Middle School 24, AIME 24, and AMC 23.</p><p>During the analysis of contaminated samples, we identify that some existing training datasets (e.g., the MATH training dataset) contain a significant proportion of problems that share highly similar concepts or structures with those found in test datasets. Although these variations are not exact duplicates, they could potentially compromise the integrity of our evaluation. Therefore, we continue to exclude such samples from the training corpora.</p><h2 id=demo>Demo<a hidden class=anchor aria-hidden=true href=#demo>#</a></h2><p>We develop a demo that supports the TIR mode in <a href=https://github.com/QwenLM/Qwen-Agent>Qwen-Agent</a>, which allows running code locally to experience Tool-Integrated Reasoning capabilities of Qwen2.5-Math.</p><figure><img src=http://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/qwen2.5-math-example1.png#center width=80%></figure><p>Furthermore, we provide a multi-modal mathematic demo in <a href=https://huggingface.co/spaces/Qwen/Qwen2-Math-Demo>Huggingface</a> and <a href=https://www.modelscope.cn/studios/qwen/Qwen-Math-demo>Modelscope</a>. This WebUI is based on Qwen2-VL for OCR and Qwen2-Math for mathematical reasoning. You can input either images, texts, or sketches of mathematical and arithmetic problems.</p><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p>We introduce Qwen2.5-Math, which features several key technical highlights:</p><p>(1) Extensive using of synthesized mathematical data from Qwen2-Math during the pre-training phase.</p><p>(2) Iterative generation of fine-tuning data and reinforcement training guided by the reward model during the post-training phase.</p><p>(3) Supporting for bilingual (English and Chinese) queries, along with chain-of-thought and tool-integrated reasoning capabilities.</p><p>As a result, Qwen2.5-Math represents the most advanced open-source math model series to date. The Qwen2.5-Math-1.5B-Instruct model already surpasses most previous 70B math models, while the Qwen2.5-Math-7B-Instruct matches the performance of Qwen2-Math-72B-Instruct. Our flagship model, Qwen2.5-Math-7B-Instruct, outperforms Qwen2-Math-72B-Instruct with an average score increase of 4.7 points across 7 tasks.</p><p>We hope that the advances we‚Äôve made with specialized models like Qwen2.5-Math will continue to strengthen the overall capabilities of the Qwen model and bring us closer to achieving artificial general intelligence.</p></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="http://qwenlm.github.io/blog/qwen2-vl/"
                    >Qwen2-VL: To See the World More Clearly</a>
                </h2>

                                    <time datetime="2024-08-28 16:24:00">
                        2024-08-28 16:24:00                    </time>
                    <p></p>
                
                
                <!-- Intentionally not escaping for html context -->
                <div class=post-content><figure><img src=https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen2-VL/qwen2vl-head.jpeg#center width=100%></figure><p><a href=https://huggingface.co/spaces/Qwen/Qwen2-VL class="btn external" target=_blank>DEMO</a> <a href=https://github.com/QwenLM/Qwen2-VL class="btn external" target=_blank>GITHUB</a> <a href=https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d class="btn external" target=_blank>HUGGING FACE</a> <a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a> <a href=https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api class="btn external" target=_blank>API</a> <a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><p>After a year&rsquo;s relentless efforts, today we are thrilled to release <strong>Qwen2-VL</strong>! Qwen2-VL is the latest version of the vision language models based on <strong>Qwen2</strong> in the Qwen model familities. Compared with Qwen-VL, Qwen2-VL has the capabilities of:</p><ul><li><p><strong>SoTA understanding of images of various resolution & ratio</strong>: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.</p></li><li><p><strong>Understanding videos of 20min+</strong>: Qwen2-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.</p></li><li><p><strong>Agent that can operate your mobiles, robots, etc.</strong>: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.</p></li><li><p><strong>Multilingual Support</strong>: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.<br><br></p></li></ul><p>We opensource Qwen2-VL-2B and Qwen2-VL-7B with Apache 2.0 license, and we release the API of Qwen2-VL-72B! The opensource is integrated to Hugging Face Transformers, vLLM, and other third-party frameworks. Hope you enjoy!</p><h2 id=performance>Performance<a hidden class=anchor aria-hidden=true href=#performance>#</a></h2><p>We evaluate our model‚Äôs visual capabilities across six key dimensions: complex college-level problem-solving, mathematical abilities, document and table comprehension, multilingual text-image understanding, general scenario question-answering, video comprehension, and agent-based interactions. Overall, our 72B model showcases top-tier performance across most metrics, often surpassing even closed-source models like GPT-4o and Claude 3.5-Sonnet. Notably, it demonstrates a significant edge in document understanding.</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/qwen2-vl-72b.jpg#center width=100%></figure><p>At the 7B scale, we‚Äôve managed to retain support for image, multi-image, and video inputs, delivering competitive performance in a more cost-effective model size. Specifically, our model excels in document understanding tasks such as DocVQA and in multilingual text understanding from images, as assessed by MTVQA, establishing state-of-the-art performance.</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/qwen2-vl-7b.jpg#center width=100%></figure><p>Additionally, we‚Äôre excited to introduce a smaller 2B model, optimized for potential mobile deployment. Despite its compact size, this model boasts strong performance in image, video, and multilingual comprehension. It particularly shines in video-related tasks, document understanding, and general scenario question-answering when compared to other models of similar scale.</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/qwen2-vl-2b.jpg#center width=100%></figure><h2 id=model-capabilities>Model Capabilities<a hidden class=anchor aria-hidden=true href=#model-capabilities>#</a></h2><h3 id=1-enhanced-recognition-capabilities>1. Enhanced Recognition Capabilities<a hidden class=anchor aria-hidden=true href=#1-enhanced-recognition-capabilities>#</a></h3><p>Qwen2-VL now boasts improved object recognition, extending beyond plants and landmarks to comprehend complex relationships between multiple objects in a scene. We&rsquo;ve also significantly boosted the model&rsquo;s ability to recognize handwritten text and multiple languages within images, making it more accessible to users worldwide.</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Multi-object Recognition</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Output the color and number of each box. <img loading=lazy src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/boxes.png alt=image></div><div class=role>Qwen2-VL</div><div class=content><p>The image shows a stack of colorful blocks with numbers on them. Here is the color and number of each block from top to bottom:</p><ul><li>Top row (single block): Blue with the number 9</li><li>Second row (two blocks): Light blue with the number 7, and green with the number 8</li><li>Third row (three blocks): Purple with the number 4, pink with the number 5, and light green with the number 6</li><li>Bottom row (four blocks): Red with the number 0, orange with the number 1, yellow with the number 2, and light green with the number 3</li></ul></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Plant identification</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>What kind of flower is this? <img loading=lazy src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/flowers.jpg alt=image></div><div class=role>Qwen2-VL</div><div class=content>The flowers in the picture are forget-me-nots (Myosotis). These small, delicate flowers are known for their vibrant colors, typically blue, pink, or white, and are often used in bouquets and gardens for their charming appearance.</div></div></div><div class=example-content style=display:none><div class=title><span>Example: Document Parsing with Dense Formulas</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Convert the content in the image to Markdown <img loading=lazy src=http://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/arxiv_sample.png alt=image></div><div class=role>Qwen2-VL</div><div class=content><ol><li><p>If $i > 0$, let $\underline{d}&rsquo;&rsquo; := (i-1, d-i-l+1, l)$. Let $V_{\underline{d}}$ and $V_{\underline{d}&rsquo;&rsquo;}$ be $r+1$-dimensional subspaces of $H^0(\mathcal{L}_{\underline{d}})$ and $H^0(\mathcal{L}_{\underline{d}&rsquo;&rsquo;})$, respectively, such that $\varphi_{\underline{d}, \underline{d}&rsquo;&rsquo;}(V_{\underline{d}}) = V_{\underline{d}&rsquo;&rsquo;}^{X_1, 0}$. Then the distributivity holds in $V_{\underline{d}&rsquo;&rsquo;}$ if and only if $$ \dim (V_{\underline{d}}^{X_2, 0} + V_{\underline{d}}^{X_3, 0}) - \dim (V_{\underline{d}&rsquo;&rsquo;}^{X_2, 0} + V_{\underline{d}&rsquo;&rsquo;}^{X_3, 0}) = r + 1 - \dim (V_{\underline{d}&rsquo;&rsquo;}^{X_1, 0} + V_{\underline{d}&rsquo;&rsquo;}^{X_2, 0} + V_{\underline{d}&rsquo;&rsquo;}^{X_3, 0}) $$</p></li><li><p>If $i + l \leq d - 1$, let $\underline{d}&rsquo;&rsquo;&rsquo; := (i, d-i-l-1, l+1)$. Let $V_{\underline{d}}$ and $V_{\underline{d}&rsquo;&rsquo;&rsquo;}$ be $r+1$-dimensional subspaces of $H^0(\mathcal{L}_{\underline{d}})$ and $H^0(\mathcal{L}_{\underline{d}&rsquo;&rsquo;&rsquo;})$, respectively, such that $\varphi_{\underline{d}&rsquo;&rsquo;&rsquo;, \underline{d}}(V_{\underline{d}&rsquo;&rsquo;&rsquo;}) = V_{\underline{d}}^{X_3, 0}$. Then the distributivity holds in $V_{\underline{d}}$ if and only if $$ \dim (V_{\underline{d}&rsquo;&rsquo;&rsquo;}^{X_1, 0} + V_{\underline{d}&rsquo;&rsquo;&rsquo;}^{X_2, 0}) - \dim (V_{\underline{d}}^{X_1, 0} + V_{\underline{d}}^{X_2, 0}) = r + 1 - \dim (V_{\underline{d}}^{X_1, 0} + V_{\underline{d}}^{X_2, 0} + V_{\underline{d}}^{X_3, 0}), $$ where the maps $\varphi_{\underline{d}, \underline{d}&rsquo;&rsquo;}$ and $\varphi_{\underline{d}&rsquo;&rsquo;&rsquo;, \underline{d}}$ in Proposition 3.14 are the maps linking the corresponding sheaves. Another important result is Proposition 3.16, which establishes an inequality for any exact limit linear series. Specifically, our Proposition 3.16 says:</p></li></ol><p>Let ${(\mathcal{L}_{\underline{d}}, V_{\underline{d}})}_{\underline{d}}$ be an exact limit linear series of degree $d$ and dimension $r$. Then $$ \sum_{\underline{d}} \dim \left( \frac{V_{\underline{d}}}{V_{\underline{d}}^{X_1, 0} + V_{\underline{d}}^{X_2, 0} + V_{\underline{d}}^{X_3, 0}} \right) \geq r + 1. $$</p><p>As a consequence of Proposition 3.14 and Proposition 3.16, in Corollary 3.17, we get the following characterization of exact limit linear series satisfying the distributivity at each multidegree:</p><p>Let ${(\mathcal{L}_{\underline{d}}, V_{\underline{d}})}_{\underline{d}}$ be an exact limit linear series of degree $d$ and dimension $r$. Then $$ \sum_{\underline{d}} \dim \left( \frac{V_{\underline{d}}}{V_{\underline{d}}^{X_1, 0} + V_{\underline{d}}^{X_2, 0} + V_{\underline{d}}^{X_3, 0}} \right) = r + 1 $$ if and only if the distributivity holds in $V_{\underline{d}}$ for any $\underline{d}$.</p><p>In Section 3, we also study simple limit linear series. In Proposition 3.18, we characterize the multidegrees which determine all simple bases. In Proposition 3.20, following the construction in Mu√±oz [5], Proposition 4.4, we present a certain structure of linearly independent sections for any exact limit linear series satisfying the distributivity property. In propositions 3.21 and 3.22, we obtain some properties of that structure. These properties imply the existence of a simple basis (we give a proof of that in Proposition 3.23).</p><p>We conclude Section 3 with a characterization of simple limit linear series among the exact limit linear series. Specifically, our Corollary 3.24 says:</p><p>Let ${(\mathcal{L}_{\underline{d}}, V_{\underline{d}})}_{\underline{d}}$ be an exact limit linear series of degree $d$ and dimension $r$. Then the following statements are equivalent:</p><ol><li>${(\mathcal{L}_{\underline{d}}, V_{\underline{d}})}_{\underline{d}}$ is simple.</li><li>$\sum_{\underline{d}} \dim \left( \frac{V_{\underline{d}}}{V_{\underline{d}}^{X_1, 0} + V_{\underline{d}}^{X_2, 0} + V_{\underline{d}}^{X_3, 0}} \right) = r + 1$.</li><li>The distributivity holds in $V_{\underline{d}}$ for any $\underline{d}$.</li></ol><p>In a subsequent work, given a refined limit linear series, we will construct all simple extensions by combining our Proposition 3.14 and the method of Mu√±oz [6] for the construction of all exact extensions.</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Multilingual Text Recognition</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>ËΩ¨ÂΩïÂõæ‰∏≠ÂÜÖÂÆπÔºåÂπ∂ËØÜÂà´Âõæ‰∏≠ÊØèÊÆµÂÜÖÂÆπÁöÑËØ≠Ë®Ä„ÄÇ <img loading=lazy src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/screenshot_multilingual4.png alt=image></div><div class=role>Qwen2-VL</div><div class=content><ol><li><p>Ê±âËØ≠Ôºå‰πüÁß∞‰∏∫‚ÄúÂçéËØ≠‚Äù„ÄÇÊòØ‰∏≠ÂõΩ‰ΩøÁî®‰∫∫Êï∞ÊúÄÂ§öÁöÑËØ≠Ë®ÄÔºå‰πüÊòØ‰∏ñÁïå‰∏ä‰Ωú‰∏∫Á¨¨‰∏ÄËØ≠Ë®Ä‰ΩøÁî®‰∫∫Êï∞ÊúÄÂ§öÁöÑËØ≠Ë®Ä„ÄÇÊòØ‰∏≠Âçé‰ºòÁßÄ‰º†ÁªüÊñáÂåñÁöÑÈáçË¶ÅËΩΩ‰Ωì„ÄÇÁπÅ‰ΩìÂ≠óÔºåÂèàÁ®±ÁÇ∫‚ÄúÁπÅÈ´î‰∏≠Êñá‚ÄùÔºåËàá‚ÄúÁ∞°ÂåñÂ≠ó‚Äù/‚ÄúÁ∞°È´îÂ≠ó‚ÄùÁõ∏Â∞ç„ÄÇ‰∏ÄËà¨ÊòØÊåáÊº¢Â≠óÁ∞°ÂåñÈÅãÂãïË¢´Á∞°ÂåñÂ≠óÊâÄ‰ª£ÊõøÁöÑÊº¢Â≠ó„ÄÇ</p><ul><li>ËØ≠Ë®ÄÔºö‰∏≠Êñá</li></ul></li><li><p>Êó•Êú¨Ë™û„ÅØÂú∞Êñπ„Åî„Å®„Å´Â§öÊßò„Å™ÊñπË®Ä„Åå„ÅÇ„Çä„ÄÅ„Å®„Çä„Çè„ÅëÁêâÁêÉË´∏Â≥∂„ÅßÊñπË®ÄÂ∑Æ„ÅåËëó„Åó„ÅÑ„ÄÇ</p><ul><li>ËØ≠Ë®ÄÔºöÊó•ËØ≠</li></ul></li><li><p>ÌïúÍµ≠Ïñ¥(ÈüìÂúãË™û), Ï°∞ÏÑ†Îßê(ÊúùÈÆÆÎßê)Îäî ÎåÄÌïúÎØºÍµ≠Í≥º Ï°∞ÏÑ†ÎØºÏ£ºÏ£ºÏùòÏù∏ÎØºÍ≥µÌôîÍµ≠Ïùò Í≥µÏö©Ïñ¥Ïù¥Îã§. ÎëòÏùÄ ÌëúÍ∏∞ÎÇò Î¨∏Î≤ïÏóêÏÑúÎäî Ï∞®Ïù¥Í∞Ä ÏóÜÏßÄÎßå ÎèôÏÇ¨ Ïñ¥ÎØ∏ÎÇò ÌëúÌòÑÏóêÏÑú Ï∞®Ïù¥Í∞Ä ÏûàÎã§.</p><ul><li>ËØ≠Ë®ÄÔºöÈü©ËØ≠</li></ul></li><li><p>Le fran√ßais est une langue indo-europ√©enne de la famille des langues romanes dont les locuteurs sont appel√©s ¬´ francophones ¬ª.</p><ul><li>ËØ≠Ë®ÄÔºöÊ≥ïËØ≠</li></ul></li><li><p>El espa√±ol o castellano es una lengua romance procedente del lat√≠n hablado, perteneciente a la familia de lenguas indoeuropeas.</p><ul><li>ËØ≠Ë®ÄÔºöË•øÁè≠ÁâôËØ≠</li></ul></li><li><p>A l√≠ngua portuguesa, tamb√©m designada portugu√™s, √© uma l√≠ngua indo-europeia rom√¢nica flexiva ocidental originada no galego-portugu√™s falado no Reino da Galiza e no norte de Portugal.</p><ul><li>ËØ≠Ë®ÄÔºöËë°ËêÑÁâôËØ≠</li></ul></li><li><p>Is ceann de na teangacha Ceilteacha √≠ an Ghaeilge (n√≥ Gaeilge na h√âireann mar a thugtar uirthi corruair), agus ceann de na tr√≠ cinn de theangacha Ceilteacha ar a dtugtar na teangacha Gaelacha (Gaeilge, Gaeilge Mhanann agus Gaeilge na hAlban) go h√°irithe.</p><ul><li>ËØ≠Ë®ÄÔºöÁà±Â∞îÂÖ∞ËØ≠</li></ul></li><li><p>English is a West Germanic language in the Indo-European language family, whose speakers, called Anglophones, originated in early medieval England on the island of Great Britain.</p><ul><li>ËØ≠Ë®ÄÔºöËã±ËØ≠</li></ul></li><li><p>Die deutsche Sprache bzw. Deutsch ist eine westgermanische Sprache, die weltweit etwa 90 bis 105 Millionen Menschen als Muttersprache und weiteren rund 80 Millionen als Zweit- oder Fremdsprache dient.</p><ul><li>ËØ≠Ë®ÄÔºöÂæ∑ËØ≠</li></ul></li><li><p>Jƒôzyk polski, polszczyzna ‚Äî jƒôzyk z grupy zachodnios≈Çowia≈Ñskiej (do kt√≥rej nale≈ºƒÖ r√≥wnie≈º czeski, kaszubski, s≈Çowacki, jƒôzyki ≈Çu≈ºyckie czy wymar≈Çy jƒôzyk drzewia≈Ñski), stanowiƒÖcej czƒô≈õƒá rodziny indoeuropejskiej.</p><ul><li>ËØ≠Ë®ÄÔºöÊ≥¢ÂÖ∞ËØ≠</li></ul></li><li><p>Œó ŒµŒªŒªŒ∑ŒΩŒπŒ∫ŒÆ Œ≥ŒªœéœÉœÉŒ± Œ±ŒΩŒÆŒ∫ŒµŒπ œÉœÑŒ∑ŒΩ ŒπŒΩŒ¥ŒøŒµœÖœÅœâœÄŒ±œäŒ∫ŒÆ ŒøŒπŒ∫ŒøŒ≥Œ≠ŒΩŒµŒπŒ± Œ∫Œ±Œπ Œ±œÄŒøœÑŒµŒªŒµŒØ œÑŒø ŒºŒøŒΩŒ±Œ¥ŒπŒ∫œå ŒºŒ≠ŒªŒøœÇ œÑŒøœÖ ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œøœç Œ∫ŒªŒ¨Œ¥ŒøœÖ.</p><ul><li>ËØ≠Ë®ÄÔºöÂ∏åËÖäËØ≠</li></ul></li><li><p>Ti·∫øng Vi·ªát hay Vi·ªát ng·ªØ l√† m·ªôt ng√¥n ng·ªØ thu·ªôc ng·ªØ h·ªá Nam √Å, ƒë∆∞·ª£c c√¥ng nh·∫≠n l√† ng√¥n ng·ªØ ch√≠nh th·ª©c t·∫°i Vi·ªát Nam.</p><ul><li>ËØ≠Ë®ÄÔºöË∂äÂçóËØ≠</li></ul></li><li><p>–ú–æ–Ω–≥–æ–ª —Ö—ç–ª –Ω—å –ú–æ–Ω–≥–æ–ª —É–ª—Å—ã–Ω –∞–ª–±–∞–Ω —ë—Å–Ω—ã —Ö—ç–ª —é–º.</p><ul><li>ËØ≠Ë®ÄÔºöËíôÂè§ËØ≠</li></ul></li><li><p>–ß–∏—Å–ª–æ –≤–ª–∞–¥–µ—é—â–∏—Ö —Ä—É—Å—Å–∫–∏–º —è–∑—ã–∫–æ–º –≤ –†–æ—Å—Å–∏–∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 137,5 –º–ª–Ω —á–µ–ª–æ–≤–µ–∫ (2010). –í—Å–µ–≥–æ –≤ –º–∏—Ä–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º –≥–æ–≤–æ—Ä—è—Ç 258,2 –º–ª–Ω —á–µ–ª–æ–≤–µ–∫ (2022)</p><ul><li>ËØ≠Ë®ÄÔºö‰øÑËØ≠</li></ul></li><li><p>‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§µ‡§ø‡§∂‡•ç‡§µ ‡§ï‡•Ä ‡§è‡§ï ‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ ‡§≠‡§æ‡§∑‡§æ ‡§π‡•à ‡§î‡§∞ ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•Ä ‡§è‡§ï ‡§∞‡§æ‡§ú‡§≠‡§æ‡§∑‡§æ ‡§π‡•à‡•§ ‡§ï‡•á‡§®‡•ç‡§¶‡•ç‡§∞‡•Ä‡§Ø ‡§∏‡•ç‡§§‡§∞ ‡§™‡§∞ ‡§≠‡§æ‡§∞‡§§ ‡§Æ‡•á‡§Ç ‡§∏‡§π-‡§Ü‡§ß‡§ø‡§ï‡§æ‡§∞‡§ø‡§ï ‡§≠‡§æ‡§∑‡§æ ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§π‡•à‡•§</p><ul><li>ËØ≠Ë®ÄÔºöÂç∞Âú∞ËØ≠</li></ul></li><li><p>Kiswahili ni lugha ya Kibantu yenye misamiati mingi ya Kiarabu (35%), lakini sasa ya Kiingereza pia (10%), inayozungumzwa katika eneo kubwa la Afrika ya Mashariki.</p><ul><li>ËØ≠Ë®ÄÔºöÊñØÁì¶Â∏åÈáåËØ≠</li></ul></li></ol></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Handwritten Text Recognition</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>What is the text? <img loading=lazy src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/multilingual_2.png alt=image></div><div class=role>Qwen2-VL</div><div class=content>The text on the wall is in Portuguese and reads: &ldquo;n√£o √© normal ter medo de andar sozinha na rua!!!&rdquo; This translates to: &ldquo;It is not normal to be afraid to walk alone on the street!!!&rdquo;</div></div></div><div class=example-content style=display:none><div class=title><span>Example: Handwritten Text Recognition</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>ËØÜÂà´Âõæ‰∏≠ÊâÄÊúâÊñáÂ≠óÔºåÊó†ÈúÄÊ∑ªÂä†Ê†áÁÇπ„ÄÇ <img loading=lazy src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/Handwritten_chinese.jpeg alt=image></div><div class=role>Qwen2-VL</div><div class=content><p>Â∞äÊï¨ÁöÑËÄÅÂ∏à</p><p>Â±ï‰ø°ËàíÈ¢ú</p><p>Ê∞îÊ∏©È™§Èôç</p><p>Ê†ëÂè∂Êó†Â£∞Êó†ÊÅØÁöÑÊéâËêΩ</p><p>ÊïôËØ≤Â¶ÇÊò•È£é</p><p>Â∏àÊÅ©Ê∑±‰ººÊµ∑</p><p>ÊÑøÊ∏©È¶®„ÄÅÂñúÊÇ¶‰∏éÂπ∏Á¶è</p><p>Â¶ÇÂêåÂêêÈú≤Ëä¨Ëä≥ÁöÑËä±Êúµ</p><p>Ê∞∏ËøúÊ¥ãÊ∫¢Âú®ÊÇ®Âπ∏Á¶èÁöÑÊó∂ÂÖâÈáå</p></div></div></div></div><h3 id=2-visual-reasoning-solving-real-world-problems>2. Visual Reasoning: Solving Real-World Problems<a hidden class=anchor aria-hidden=true href=#2-visual-reasoning-solving-real-world-problems>#</a></h3><p>In this iteration, we have significantly enhanced Qwen2-VL&rsquo;s mathematical and coding proficiencies. The model is not only capable of solving problems by analyzing pictures but can also interpret and solve complex mathematical problems through chart analysis. Extremely aspect-ratio-distorted images can also be correctly interpreted. Additionally, we have reinforced the model&rsquo;s capability to extract information from real-world images and charts and improved its instruction-following skills. This fusion of visual perception and logical reasoning empowers the model to tackle practical issues, bridging the gap between abstract concepts and tangible solutions.</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Code</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Solve the algorithmic problem in the web page. <img loading=lazy src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/Code.jpeg alt=image></div><div class=role>Qwen2-VL</div><div class=content><p>To solve the &ldquo;Snake in Matrix&rdquo; problem, we need to implement a function that takes the size of the matrix <code>n</code> and a list of commands, and returns the final position of the snake after executing all the commands. Here&rsquo;s a step-by-step approach to solve this problem:</p><ol><li><p><strong>Initialize Variables</strong>:</p><ul><li>Initialize the current position of the snake as <code>0</code>.</li><li>Initialize the direction of the snake as &ldquo;RIGHT&rdquo; (initially, the snake moves to the right).</li></ul></li><li><p><strong>Define Movement Functions</strong>:</p><ul><li>Create a function to move the snake based on the current direction and the size of the matrix <code>n</code>.</li><li>Update the position of the snake accordingly.</li></ul></li><li><p><strong>Process Commands</strong>:</p><ul><li>Iterate through each command in the list of commands.</li><li>For each command, update the direction and position of the snake.</li></ul></li><li><p><strong>Return Final Position</strong>:</p><ul><li>After processing all commands, return the final position of the snake.</li></ul></li></ol><p>Here&rsquo;s the Python code to implement the solution:</p><div class=highlight><pre>class  Solution :   
    def  finalPositionOfSnake ( self ,  n :  int ,  commands :  List [ str ])  -&gt;  int :   
        # Initialize variables   
        position  =  0   
        direction  =  &#34;RIGHT&#34;   
          
        # Define movement functions   
        def  move_right ():   
            nonlocal  position   
            position  +=  1   
          
        def  move_down ():   
            nonlocal  position   
            position  +=  n   
          
        def  move_left ():   
            nonlocal  position   
            position  -=  1   
          
        def  move_up ():   
            nonlocal  position   
            position  -=  n   
          
        # Process commands   
        for  command  in  commands :   
            if  command  ==  &#34;RIGHT&#34; :   
                move_right ()   
            elif  command  ==  &#34;DOWN&#34; :   
                move_down ()   
            elif  command  ==  &#34;LEFT&#34; :   
                move_left ()   
            elif  command  ==  &#34;UP&#34; :   
                move_up ()   
          
        # Return final position   
        return  position   
</pre></div><p>This code defines a class <code>Solution</code> with a method <code>finalPositionOfSnake</code> that takes the size of the matrix <code>n</code> and a list of commands, and returns the final position of the snake after executing all the commands. The movement functions (<code>move_right</code>, <code>move_down</code>, <code>move_left</code>, <code>move_up</code>) are used to update the position of the snake based on the current direction.</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Math</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Find the length of AC in the isosceles triangle ABC. <img loading=lazy src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/triangle.jpg alt=image></div><div class=role>Qwen2-VL</div><div class=content><p>Since triangle ABC is isosceles, we know that AC = BC. Therefore, we can set the expressions for AC and BC equal to each other and solve for x:</p><p>4x + 1 = 5x - 0.5</p><p>Subtracting 4x from both sides, we get:</p><p>1 = x - 0.5</p><p>Adding 0.5 to both sides, we get:</p><p>1.5 = x</p><p>Now that we know the value of x, we can substitute it back into the expression for AC to find its length:</p><p>AC = 4x + 1</p><p>AC = 4(1.5) + 1</p><p>AC = 6 + 1</p><p>AC = 7</p><p>Therefore, the length of AC is 7.</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Information Extraction of High Resolution Images</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Extract all page titles in the screenshot of search result. <img loading=lazy src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/screenshot1.png alt=image></div><div class=role>Qwen2-VL</div><div class=content><p>The page titles in the screenshot of the search result are:</p><ol><li>‰Ω†Â•ΩÔºåQwen2</li><li>QwenLM/Qwen2</li><li>Qwen2 - a Qwen Collection</li><li>Âø´ÈÄüÂºÄÂßã</li><li>Qwen2-7B</li><li>Qwen2-MathÔºåÊñ∞‰∏Ä‰ª£Êï∞Â≠¶Ê®°Âûã</li><li>Qwen</li><li>Qwen/Qwen2-7B-Instruct</li><li>[2407.10671] Qwen2 Technical Report</li><li>qwen2</li></ol></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Math</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Please tell me how many days are between Ruka&rsquo;s birthday and Yuki&rsquo;s birthday? Think carefully step by step. <img loading=lazy src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/math1.jpg alt=image></div><div class=role>Qwen2-VL</div><div class=content><p>To determine the number of days between Ruka&rsquo;s and Yuki&rsquo;s birthdays, we need to calculate the difference between their respective birth dates.</p><ol><li>Ruka&rsquo;s birthday is on June 24.</li><li>Yuki&rsquo;s birthday is on September 17.</li></ol><p>Let&rsquo;s break it down step by step:</p><ol><li><p><strong>From June 24 to June 30:</strong></p><ul><li>June 24 to June 30 is 6 days.</li></ul></li><li><p><strong>From July 1 to August 31:</strong></p><ul><li>July has 31 days.</li><li>August has 31 days.</li><li>Total days in July and August: 31 + 31 = 62 days.</li></ul></li><li><p><strong>From September 1 to September 17:</strong></p><ul><li>September 1 to September 17 is 17 days.</li></ul></li></ol><p>Now, we add all these days together:</p><ul><li>June 24 to June 30: 6 days</li><li>July 1 to August 31: 62 days</li><li>September 1 to September 17: 17 days</li></ul><p>Total days = 6 + 62 + 17 = 85 days</p><p>Therefore, there are 85 days between Ruka&rsquo;s and Yuki&rsquo;s birthdays.</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Information Extraction</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>What are the Linux versions and their release dates in the picture? Return results as a JSON list. <img loading=lazy src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/information_extraction.jpg alt=image></div><div class=role>Qwen2-VL</div><div class=content><div class=highlight><pre>[   
  {   
    &#34;version&#34; :  &#34;6.11-rc3&#34; ,   
    &#34;release_date&#34; :  &#34;2024-08-11&#34;   
  },   
  {   
    &#34;version&#34; :  &#34;6.10.4&#34; ,   
    &#34;release_date&#34; :  &#34;2024-08-11&#34;   
  },   
  {   
    &#34;version&#34; :  &#34;6.9.12 [EOL]&#34; ,   
    &#34;release_date&#34; :  &#34;2024-07-27&#34;   
  },   
  {   
    &#34;version&#34; :  &#34;6.6.45&#34; ,   
    &#34;release_date&#34; :  &#34;2024-08-11&#34;   
  },   
  {   
    &#34;version&#34; :  &#34;6.1.104&#34; ,   
    &#34;release_date&#34; :  &#34;2024-08-11&#34;   
  },   
  {   
    &#34;version&#34; :  &#34;5.15.164&#34; ,   
    &#34;release_date&#34; :  &#34;2024-07-27&#34;   
  },   
  {   
    &#34;version&#34; :  &#34;5.10.223&#34; ,   
    &#34;release_date&#34; :  &#34;2024-07-27&#34;   
  },   
  {   
    &#34;version&#34; :  &#34;5.4.281&#34; ,   
    &#34;release_date&#34; :  &#34;2024-07-27&#34;   
  },   
  {   
    &#34;version&#34; :  &#34;4.19.319&#34; ,   
    &#34;release_date&#34; :  &#34;2024-07-27&#34;   
  },   
  {   
    &#34;version&#34; :  &#34;next-20240812&#34; ,   
    &#34;release_date&#34; :  &#34;2024-08-12&#34;   
  }   
]   
</pre></div></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Information Extraction and Organization</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Â∞ÜÂõæ‰∏≠ÁöÑÊØèÊó•ÁöÑÊúÄÈ´òÊ∞îÊ∏©ÂíåÊúÄ‰ΩéÊ∞îÊ∏©Êï¥ÁêÜÊàêË°®Ê†º„ÄÇ <img loading=lazy src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/table_weather.jpg alt=image></div><div class=role>Qwen2-VL</div><div class=content><p>‰ª•‰∏ãÊòØÂõæ‰∏≠ÊØèÊó•ÁöÑÊúÄÈ´òÊ∞îÊ∏©ÂíåÊúÄ‰ΩéÊ∞îÊ∏©Êï¥ÁêÜÊàêÁöÑË°®Ê†ºÔºö</p><table><thead><tr><th>Êó•Êúü</th><th>ÊúÄÈ´òÊ∞îÊ∏©</th><th>ÊúÄ‰ΩéÊ∞îÊ∏©</th></tr></thead><tbody><tr><td>08/12</td><td>31¬∞C</td><td>23¬∞C</td></tr><tr><td>08/13</td><td>30¬∞C</td><td>22¬∞C</td></tr><tr><td>08/14</td><td>30¬∞C</td><td>23¬∞C</td></tr><tr><td>08/15</td><td>31¬∞C</td><td>24¬∞C</td></tr><tr><td>08/16</td><td>31¬∞C</td><td>24¬∞C</td></tr><tr><td>08/17</td><td>30¬∞C</td><td>24¬∞C</td></tr><tr><td>08/18</td><td>30¬∞C</td><td>23¬∞C</td></tr><tr><td>08/19</td><td>30¬∞C</td><td>22¬∞C</td></tr></tbody></table><p>Â∏åÊúõËøôÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÔºÅ</p></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Information Extraction of High Resolution Images with Extreme Aspect Ratio</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>qwen-vl-max-0809ÂØπËßÜÈ¢ëÂÜÖÂÆπÁöÑÁêÜËß£ÂäüËÉΩÂ≠òÂú®Âì™‰∫õÈôêÂà∂Ôºü <img loading=lazy src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/screenshot2.png alt=image></div><div class=role>Qwen2-VL</div><div class=content>qwen-vl-max-0809ÂØπËßÜÈ¢ëÂÜÖÂÆπÁöÑÁêÜËß£ÂäüËÉΩÂ≠òÂú®‰ª•‰∏ãÈôêÂà∂ÔºöËßÜÈ¢ëÊñá‰ª∂Â§ßÂ∞èÊúÄÂ§ß‰∏∫150MBÔºåËßÜÈ¢ëÊñá‰ª∂Ê†ºÂºèÊîØÊåÅMP4„ÄÅAVI„ÄÅMKV„ÄÅMOV„ÄÅFLV„ÄÅWMVÁ≠âÔºåËßÜÈ¢ëÂ∞∫ÂØ∏Êó†ÈôêÂà∂Ôºå‰ΩÜËßÜÈ¢ëÊñá‰ª∂‰ºöË¢´Ë∞ÉÊï¥Âà∞Á∫¶600KÂÉèÁ¥†Êï∞ÔºåÊõ¥Â§ßÂ∞∫ÂØ∏ÁöÑËßÜÈ¢ëÊñá‰ª∂‰∏ç‰ºöÊúâÊõ¥Â•ΩÁöÑÁêÜËß£ÊïàÊûú„ÄÇ</div></div></div></div><h3 id=3-video-understanding-and-live-chat>3. Video Understanding and Live Chat<a hidden class=anchor aria-hidden=true href=#3-video-understanding-and-live-chat>#</a></h3><p>Beyond static images, Qwen2-VL extends its prowess to video content analysis. It can summarize video content, answer questions related to it, and maintain a continuous flow of conversation in real-time, offering live chat support. This functionality allows it to act as a personal assistant, helping users by providing insights and information drawn directly from video content.</p><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Video Understanding</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>Please describe the video in detail.<figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4></video></figure></div><div class=role>Qwen2-VL</div><div class=content>The video begins with a man speaking to the camera, followed by a group of people sitting in a control room. The camera then cuts to two men floating inside a space station, where they are seen speaking to the camera. The men appear to be astronauts, and they are wearing space suits. The space station is filled with various equipment and machinery, and the camera pans around to show the different areas of the station. The men continue to speak to the camera, and they appear to be discussing their mission and the various tasks they are performing. Overall, the video provides a fascinating glimpse into the world of space exploration and the daily lives of astronauts.</div><div class=role>User</div><div class=content>What is the color of the clothes the two astronauts wearing?</div><div class=role>Qwen2-VL</div><div class=content>The two astronauts are wearing blue and black clothes.</div></div></div><div class=example-content style=display:none><div class=title><span>Example: Multi-Video Understanding</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>User</div><div class=content>ËßÜÈ¢ë‰∏≠ÁöÑ‰∫∫Âú®ÂÅö‰ªÄ‰πàÔºü<figure><video controls src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/part1_woaudio.mp4></video></figure></div><div class=role>Qwen2-VL</div><div class=content>ËßÜÈ¢ë‰∏≠ÁöÑ‰∫∫Ê≠£Âú®‰ΩøÁî®Âê∏Â∞òÂô®Ê∏ÖÁêÜÂú∞Êùø„ÄÇ</div><div class=role>User</div><div class=content>Â¶ÇÊûúË¶ÅËøõ‰∏ÄÊ≠•Ê∏ÖÊ¥ÅÂú∞Èù¢ÁöÑËØùÔºåÊé•‰∏ãÊù•ËØ•ÊÄé‰πàÂÅöÔºü</div><div class=role>Qwen2-VL</div><div class=content>Â¶ÇÊûúË¶ÅËøõ‰∏ÄÊ≠•Ê∏ÖÊ¥ÅÂú∞Èù¢ÔºåÂèØ‰ª•‰ΩøÁî®ÊãñÊääÊàñÊπøÂ∏ÉÊì¶Êã≠Âú∞Èù¢Ôºå‰ª•ÂéªÈô§Âê∏Â∞òÂô®Êó†Ê≥ïÊ∏ÖÈô§ÁöÑÊ±°Ê∏çÂíåÁÅ∞Â∞ò„ÄÇ</div><div class=role>User</div><div class=content>Â∑≤ÁªèÁî®ÊãñÊääÊ∏ÖÊ¥Å‰∫ÜÂú∞Èù¢ÔºåËØÑ‰ª∑‰∏Ä‰∏ãÊ∏ÖÊ¥ÅÂ∑•‰ΩúÔºü<figure><video controls src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/part2_woaudio.mp4></video></figure></div><div class=role>Qwen2-VL</div><div class=content>‰ªéËßÜÈ¢ë‰∏≠ÂèØ‰ª•ÁúãÂá∫ÔºåÊ∏ÖÊ¥ÅÂ∑•‰ΩúÂ∑≤ÁªèÂÆåÊàêÔºåÂú∞Èù¢Â∑≤ÁªèÂèòÂæóÂπ≤ÂáÄÊï¥Ê¥Å„ÄÇ</div><div class=role>User</div><div class=content>ÈÄöËøá‰Ω†ÁúãÂà∞ÁöÑÔºåÊé•‰∏ãÊù•ËøòËÉΩÂÅö‰∫õ‰ªÄ‰πàÔºü</div><div class=role>Qwen2-VL</div><div class=content>Êé•‰∏ãÊù•ÂèØ‰ª•Â∞ÜÊãñÊääÊ∏ÖÊ¥óÂπ≤ÂáÄÔºåÊôæÂπ≤Â§áÁî®„ÄÇ</div></div></div></div><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Live chat</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>Live chat</div><div class=content><figure><video controls loop src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/live_chat.mp4 autoplay></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Screen sharing chat</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>Screen sharing chat</div><div class=content><figure><video controls src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/Screen%20sharing%20chat.mov></video></figure></div></div></div></div><h3 id=4-visual-agent-capabilities-function-calling-and-visual-interactions>4. Visual Agent Capabilities: Function Calling and Visual Interactions.<a hidden class=anchor aria-hidden=true href=#4-visual-agent-capabilities-function-calling-and-visual-interactions>#</a></h3><p>Qwen2-VL demonstrates strong potential as a visual agent, facilitating interactions similar to human perceptions of the world.</p><ul><li>The model facilitates Function Calling, enabling it to harness external tools for real-time data retrieval ‚Äì be it flight statuses, weather forecasts, or package tracking ‚Äì by deciphering visual cues. This integration of visual interpretation with functional execution elevates its utility, making it a powerful tool for information management and decision-making.</li></ul><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Function Calling</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>Ask about the weather</div><div class=content><figure><video controls loop src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/funcall_weather.mov autoplay></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Code Interpreter</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>Write code based on the structure diagram.</div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/Code_Interpreter3.mov></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Code Interpreter</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>Visualize Charts</div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/Code_Interpreter.mov></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Code Interpreter</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>Implement the formula in code</div><div class=content><figure><video controls src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/Code_Interpreter2.mov></video></figure></div></div></div></div><ul><li>Visual Interactions represent a significant stride towards mimicking human perception. By allowing the model to engage with visual stimuli akin to human senses, we&rsquo;re pushing the boundaries of AI&rsquo;s ability to perceive and respond to its environment. This capability paves the way for more intuitive and immersive interactions, where Qwen2-VL acts not just as an observer, but an active participant in our visual experiences.</li></ul><div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: UI Interactions</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>Operate a Mobile Phone</div><div class=content><figure><video controls loop src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/Operate%20a%20Mobile%20Phone.mp4 autoplay></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Play a Game</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>21 points</div><div class=content><figure><video controls src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/Play%20a%20Game.mp4></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Visual Interactions</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>Operate a Robotic Arm</div><div class=content><figure><video controls src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/Operate%20a%20Robotic%20Arm.mp4></video></figure></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Action and Reasoning</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=role>Put a cold mug in the microwave</div><div class=content><figure><video controls src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/pick_clean_then_place_in_recep_trial.mp4></video></figure></div></div></div></div><p>Certainly, the model is not perfect and has some limitations that I hope you can understand. For example, the model is unable to extract audio from videos, and its knowledge is only up to date as of June 2023. Additionally, the model cannot guarantee complete accuracy when processing complex instructions or scenarios, and it is relatively weak in tasks involving counting, character recognition, and 3D spatial awareness.</p><h2 id=model-architecture>Model Architecture<a hidden class=anchor aria-hidden=true href=#model-architecture>#</a></h2><p>Overall, we&rsquo;ve continued with the Qwen-VL architecture, which leverages a Vision Transformer (ViT) model and Qwen2 language models. For all these variants, we utilized a ViT with approximately 600M parameters, designed to handle both image and video inputs seamlessly. To further enhance the model&rsquo;s ability to effectively perceive and comprehend visual information in videos, we introduced several key upgrades:</p><ul><li>A key architectural improvement in Qwen2-VL is the implementation of <strong>Naive Dynamic Resolution support</strong>. Unlike its predecessor, Qwen2-VL can handle arbitrary image resolutions, mapping them into a dynamic number of visual tokens, thereby ensuring consistency between the model input and the inherent information in images. This approach more closely mimics human visual perception, allowing the model to process images of any clarity or size.</li></ul><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/qwen2_vl.jpg#center width=90%></figure><ul><li>Another key architectural enhancement is the innovation of <strong>Multimodal Rotary Position Embedding (M-ROPE)</strong>. By deconstructing the original rotary embedding into three parts representing temporal and spatial (height and width) informationÔºåM-ROPE enables LLM to concurrently capture and integrate 1D textual, 2D visual, and 3D video positional information.</li></ul><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/mrope.png#center width=90%></figure><h2 id=developing-with-qwen2-vl>Developing with Qwen2-VL<a hidden class=anchor aria-hidden=true href=#developing-with-qwen2-vl>#</a></h2><p>To use the largest Qwen2-VL model, Qwen2-VL-72B, you can access it through our official API (sign up the account and obtain the API key through <a href=https://dashscope.aliyun.com/>DashScope</a>) temporarily as demonstrated below:</p><div class=highlight><pre>from  openai  import  OpenAI   
import  os   
import  base64   
  
  
def  encode_image ( image_path ):   
    with  open ( image_path ,  &#34;rb&#34; )  as  image_file :   
        return  base64 . b64encode ( image_file . read ()) . decode ( &#34;utf-8&#34; )   
  
  
# Path to your image   
image_path  =  &#34;dog_and_girl.jpeg&#34;   
  
# Getting the base64 string   
base64_image  =  encode_image ( image_path )   
  
  
def  get_response ():   
    client  =  OpenAI (   
        api_key = os . getenv ( &#34;DASHSCOPE_API_KEY&#34; ),   
        base_url = &#34;https://dashscope.aliyuncs.com/compatible-mode/v1&#34; ,   
    )   
    completion  =  client . chat . completions . create (   
        model = &#34;qwen-vl-max-0809&#34; ,   
        messages = [   
            {   
                &#34;role&#34; :  &#34;user&#34; ,   
                &#34;content&#34; :  [   
                    { &#34;type&#34; :  &#34;text&#34; ,  &#34;text&#34; :  &#34;What is this?&#34; },   
                    {   
                        &#34;type&#34; :  &#34;image_url&#34; ,   
                        &#34;image_url&#34; :  {   
                            &#34;url&#34; :  &#34;https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg&#34;   
                        },   
                    },   
                    {   
                        &#34;type&#34; :  &#34;image_url&#34; ,   
                        &#34;image_url&#34; :  { &#34;url&#34; :  f &#34;data:image/jpeg;base64, { base64_image } &#34; },   
                    },   
                ],   
            }   
        ],   
        top_p = 0.8 ,   
        stream = True ,   
        stream_options = { &#34;include_usage&#34; :  True },   
    )   
    for  chunk  in  completion :   
        print ( chunk . model_dump_json ())   
  
  
if  __name__  ==  &#34;__main__&#34; :   
    get_response ()   
</pre></div><p>The 2B and 7B models of the Qwen2-VL series are open-sourced and accessible on Hugging Face and ModelScope. You can explore the model cards for detailed usage instructions, features, and performance metrics. Below we provide an example of the simplest usage with HF Transformers.</p><p>Make sure you install <code>transformers</code> from source by <code>pip install git+https://github.com/huggingface/transformers</code> as codes for Qwen2-VL were just merged into the main branch. If you didn&rsquo;t install it from source, you may encounter the following error:</p><div class=highlight><pre>KeyError: &#39;qwen2_vl&#39;   
</pre></div><p>We offer a toolkit to help you handle various types of visual input more conveniently. It supports inputs including base64, URLs, and interleaved images and videos. You can install it using the following command:</p><div class=highlight><pre>pip install qwen-vl-utils  
</pre></div><p>Here is a code snippet for demonstration. Specifically, we recommend using flash attention 2 if possible for the sake of acceleration and memory saving.</p><div class=highlight><pre>from  transformers  import  Qwen2VLForConditionalGeneration ,  AutoTokenizer ,  AutoProcessor   
from  qwen_vl_utils  import  process_vision_info   
  
# default: Load the model on the available device(s)   
model  =  Qwen2VLForConditionalGeneration . from_pretrained (   
    &#34;Qwen/Qwen2-VL-7B-Instruct&#34; ,  device_map = &#34;auto&#34;   
)   
  
# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.   
# model = Qwen2VLForConditionalGeneration.from_pretrained(   
#     &#34;Qwen/Qwen2-VL-7B-Instruct&#34;,   
#     torch_dtype=torch.bfloat16,   
#     attn_implementation=&#34;flash_attention_2&#34;,   
#     device_map=&#34;auto&#34;,   
# )   
  
# default processer   
processor  =  AutoProcessor . from_pretrained ( &#34;Qwen/Qwen2-VL-7B-Instruct&#34; )   
  
# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.   
# min_pixels = 256*28*28   
# max_pixels = 1280*28*28   
# processor = AutoProcessor.from_pretrained(&#34;Qwen/Qwen2-VL-7B-Instruct&#34;, min_pixels=min_pixels, max_pixels=max_pixels)   
  
messages  =  [   
    {   
        &#34;role&#34; :  &#34;user&#34; ,   
        &#34;content&#34; :  [   
            {   
                &#34;type&#34; :  &#34;image&#34; ,   
                &#34;image&#34; :  &#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&#34; ,   
            },   
            { &#34;type&#34; :  &#34;text&#34; ,  &#34;text&#34; :  &#34;Describe this image.&#34; },   
        ],   
    }   
]   
  
# Preparation for inference   
text  =  processor . apply_chat_template (   
    messages ,  tokenize = False ,  add_generation_prompt = True   
)   
image_inputs ,  video_inputs  =  process_vision_info ( messages )   
inputs  =  processor (   
    text = [ text ],   
    images = image_inputs ,   
    videos = video_inputs ,   
    padding = True ,   
    return_tensors = &#34;pt&#34; ,   
)   
  
# Inference: Generation of the output   
generated_ids  =  model . generate ( ** inputs ,  max_new_tokens = 128 )   
generated_ids_trimmed  =  [   
    out_ids [ len ( in_ids )  :]  for  in_ids ,  out_ids  in  zip ( inputs . input_ids ,  generated_ids )   
]   
output_text  =  processor . batch_decode (   
    generated_ids_trimmed ,  skip_special_tokens = True ,  clean_up_tokenization_spaces = False   
)   
print ( output_text )   
</pre></div><p>To facilitate seamless integration and use of our latest models, we support a range of tools and frameworks in the open-source ecosystem, including quantization (<a href=https://github.com/AutoGPTQ/AutoGPTQ>AutoGPTQ</a>, <a href=https://github.com/casper-hansen/AutoAWQ>AutoAWQ</a>), deployment (<a href=https://github.com/vllm-project/vllm>vLLM</a>), finetuning (<a href=https://github.com/hiyouga/LLaMA-Factory>Llama-Factory</a>), etc.</p><h2 id=license>License<a hidden class=anchor aria-hidden=true href=#license>#</a></h2><p>Both the open-source Qwen2-VL-2B and Qwen2-VL-7B are under Apache 2.0.</p><h2 id=whats-next>What&rsquo;s Next<a hidden class=anchor aria-hidden=true href=#whats-next>#</a></h2><p>We look forward to your feedback and the innovative applications you will build with Qwen2-VL. In the near future, we are going to build stronger vision language models upon our next-version language models and endeavor to integrate more modalities towards an omni model!</p></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="http://qwenlm.github.io/blog/qwen2-audio/"
                    >Qwen2-Audio: Chat with Your Voice!</a>
                </h2>

                                    <time datetime="2024-08-09 08:18:19">
                        2024-08-09 08:18:19                    </time>
                    <p></p>
                
                
                <!-- Intentionally not escaping for html context -->
                <div class=post-content><p><a href=https://huggingface.co/spaces/Qwen/Qwen2-Audio-Instruct-Demo class="btn external" target=_blank>DEMO</a> <a href=https://arxiv.org/pdf/2407.10759 class="btn external" target=_blank>PAPER</a> <a href=https://github.com/QwenLM/Qwen2-Audio class="btn external" target=_blank>GITHUB</a> <a href=https://huggingface.co/collections/Qwen/qwen2-audio-66b628d694096020e0c52ff6 class="btn external" target=_blank>HUGGING FACE</a> <a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a> <a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><p>To achieve the objective of building an AGI system, the model should be capable of understanding information from different modalities. Thanks to the rapid development of large language models, LLMs are now capable of understanding language and reasoning. Previously we have taken a step forward to extend our LLM, i.e., Qwen, to more modalities, including vision and audio, and built Qwen-VL and Qwen-Audio. Today, we release Qwen2-Audio, the next version of Qwen-Audio, which is capable of accepting audio and text inputs and generating text outputs. Qwen2-Audio has the following features:</p><ul><li><p>Voice Chat: for the first time, users can use the voice to give instructions to the audio-language model without ASR modules.</p></li><li><p>Audio Analysis: the model is capable of analyzing audio information, including speech, sound, music, etc., with text instructions.</p></li><li><p>Multilingual: the model supports more than 8 languages and dialects, e.g., Chinese, English, Cantonese, French, Italian, Spanish, German, and Japanese.</p></li></ul><br><p>We open-weight Qwen2-Audio-7B and Qwen2-Audio-7B-Instruct in Hugging Face and ModelScope, and we have built a demo for users to interact with. Below are some examples to show the model performance:</p><style>.example-content .grid-layout{display:grid;grid-template-columns:1fr;row-gap:var(--content-gap)}.example-container{width:100%;padding:0;margin:0}.grid-layout{display:flex;flex-direction:column;width:100%;grid-template-columns:1fr}.message{display:flex;flex-direction:column;align-items:flex-start;width:100%}.role{font-weight:700;margin-right:5px;white-space:nowrap}.content{display:flex;align-items:center;white-space:pre-wrap;width:100%;margin-bottom:10px;margin-left:50px}.wide-content{width:100%}.audio-icon{flex-shrink:0;margin-right:10px;margin-bottom:10px}.text-content{width:90%;white-space:pre-wrap;margin-top:10px}audio{vertical-align:middle}</style><h3 id=voice-chat>Voice Chat<a hidden class=anchor aria-hidden=true href=#voice-chat>#</a></h3><div class="full-width-container example-container"><div class=example-content><div class=title><span>Voice Chat: Identify speaker information and respond</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/guess age.wav" type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>Yes, the speaker is female and in her twenties.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/1718857224593-3efbd24f-f7f8-4f10-abba-24df5d92210b.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>I'm sorry to hear that you're feeling stressed about your exam. It's completely normal to feel anxious when you're preparing for an important test. Here are some tips that might help you: &nbsp 1. Start studying early: Don't leave everything until the last minute. Break up your study sessions into smaller chunks of time, and try to stick to a regular schedule. 2. Find a quiet place to study: A quiet environment can help you concentrate better. If possible, find a library or a study room where you can work without distractions. 3.Take breaks: It's important to take breaks during your study sessions. This will give your brain a chance to rest and recharge. Try taking short breaks every hour or so. 4. Get enough sleep: Lack of sleep can make it harder to focus and retain information. Make sure you're getting enough sleep each night, ideally around 7-8 hours. 5. Practice relaxation techniques: Deep breathing exercises, meditation, or yoga can all help you relax and reduce stress. &nbsp Remember, it's okay if you're not feeling well-prepared. Just do your best, and trust yourself. Good luck with your exam!</div></div></div></div></div><div class=example-content style=display:none><div class=title><span>Voice Chat: Speech translation</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/1718864196340-99f0394d-7d0b-4b4b-b86a-4d8f3a2ac09c.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>ÊØè‰∏™‰∫∫ÈÉΩÂ∏åÊúõË¢´Ê¨£ËµèÔºåÊâÄ‰ª•Â¶ÇÊûú‰Ω†Ê¨£ËµèÊüê‰∫∫Ôºå‰∏çË¶ÅÊääÂÆÉ‰øùÂØÜ„ÄÇ</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/translate it into German.wav" type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>Jeder m√∂chte gesch√§tzt werden, also wenn du jemanden sch√§tzt, lass es nicht als Geheimnis verbergen.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/how about into French.wav" type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>Chacun veut √™tre appreci√©, alors si tu appr√©cies quelqu'un, ne le gardes pas secret.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/ÁªôÊàëÁîüÊàê‰∏Ä‰∫õÁ±ª‰ººËøôÂè•ËØùÊÑèÊÄùÁöÑÔºåÂÖ∂‰ªñËØ¥Ê≥ïÁöÑÂè•Â≠êÔºåÁî®‰∏≠ÊñáÁîüÊàê.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>1. ÊØè‰∏™‰∫∫ÈÉΩÂñúÊ¨¢Ë¢´ËµûÁæéÔºåÊâÄ‰ª•Â¶ÇÊûú‰Ω†ÂñúÊ¨¢‰∏Ä‰∏™‰∫∫Ôºå‰∏çË¶ÅÈöêËóè‰Ω†ÁöÑËµûËµè„ÄÇ 2. ÊâÄÊúâ‰∫∫ÈÉΩÊ∏¥ÊúõÂæóÂà∞ËÆ§ÂèØÔºåÂõ†Ê≠§Â¶ÇÊûú‰Ω†Ê¨£ËµèÊüê‰∫∫ÔºåËØ∑‰∏çË¶ÅÁã¨Ëá™‰∫´Áî®ËøôÁßçÊÑüËßâ„ÄÇ 3. ‰∫∫‰ª¨ÈÉΩÂ∏åÊúõË¢´Ê¨£ËµèÔºåÊâÄ‰ª•Â¶ÇÊûú‰Ω†ÂñúÊ¨¢Êüê‰∫∫Ôºå‰∏çË¶ÅÁã¨Ëá™‰øùÂØÜÊ¨£Ëµè‰ªñ‰∫∫ÊòØ‰∏ÄÁßçÁæéÂæ∑ÔºåÊâÄ‰ª•‰∏çË¶ÅËÆ©Âà´‰∫∫Áü•ÈÅì‰Ω†ÂØπ‰ªñ‰ª¨ÁöÑËµûËµèÊòØÁßòÂØÜÁöÑ„ÄÇ 4. ‰∫∫‰ª¨ÈÉΩÂ∏åÊúõÂèóÂà∞ËµûÊâ¨ÔºåÊâÄ‰ª•Â¶ÇÊûú‰Ω†Ê¨£ËµèÊüê‰∫∫Ôºå‰∏çÂ¶®ÂÖ¨ÂºÄË°®ËææÂá∫Êù•„ÄÇ 5. Â¶ÇÊûú‰Ω†ÂñúÊ¨¢Êüê‰∫∫Ôºå‰∏çË¶ÅÁäπË±´ÂëäËØâ‰ªñ‰ª¨ÔºåÂõ†‰∏∫ÊØè‰∏™‰∫∫ÈÉΩÊúâÊùÉÂà©‰∫´Âèó‰ªñ‰∫∫ÁöÑËµûËµè„ÄÇ</div></div></div></div></div><div class=example-content style=display:none><div class=title><span>Voice Chat: Detecting background noise and responding accordingly</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/audio-0-100.wav.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>Use headphones to block out external noise and find a quiet place to study. Prioritize your tasks and set realistic goals for yourself. Use active learning techniques such as summarizing, note-taking, and quizzing to help you retain information. Take breaks and practice self-care to avoid burnout.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/en-noise-2.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>Yes, you can try negotiating with them to reach a mutually beneficial agreement. You could also consider seeking help from a mediator or professional negotiator if needed.</div></div></div></div></div></div><h3 id=audio-analysis>Audio Analysis<a hidden class=anchor aria-hidden=true href=#audio-analysis>#</a></h3><div class="full-width-container example-container"><div class=example-content><div class=title><span>Audio Analysis : Speech transcription and story telling</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/en-speech-1.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>The original content of this audio is: 'Stop all the clocks, cut off the telephone, prevent the dog from barking, silence the pianos and with muffled drum bring out the coffin, let the mourners come.'</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content>No less than 50 words and no more than 200 words, add scenes such as rain and thunder to the poem. Finally, give the verses you created in lines.</div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>In the midst of an eerie stormy night, With raindrops falling heavy and bright, The world seemed to hush its voice, As winds howled through the darkened space. &nbsp Stop all the clocks, the time stood still, As the lightning flashed through the sky, The telephone's ring was drowned out, By the roaring thunder that shook the ground. &nbsp Prevent the dog from barking, its cries were silenced, By the fear that gripped it in the night, Silence the pianos, their keys were muted, As the mournful sound of raindrops beat on wood. &nbsp With muffled drums, a slow march began, A solemn procession through the storm, The coffin was carried, heavy and solemn, Let the mourners come, to pay their final respects. &nbsp As the storm slowly passed, the world emerged, A new day dawned, but memories lingered, Of the night when time stood still, And the mourners came, to say goodbye.</div></div></div></div></div><div class=example-content style=display:none><div class=title><span>Audio Analysis: Sound analysis</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content>What do you hear? <audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/sound-3-1.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>I heard the sound of an alarm and a truck's air brake mixed with the noise of a heavy engine running and shifting gears.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content>Under what circumstances can you hear such sounds?</div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>These types of sounds might be commonly heard in industrial settings like construction sites or factories where machinery is operated, or during emergency situations like traffic accidents where sirens are triggered.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/en-sound-1.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>It's understandable to feel scared under any situation that makes you feel uncomfortable. It's important to remember that it's okay to be scared. Here are some things you can do: &nbsp 1. Take deep breaths: Inhale deeply through your nose and exhale slowly through your mouth. This can help calm your mind and body. 2. Find a safe place: If possible, move to a safe location away from the source of the fear. If you're in a car, pull over to a parking lot or a safe spot on the side of the road. 3. Stay informed: If you have access to information about the situation, stay informed but don't overdo it. Too much information can create more anxiety. 4. Talk to someone: Talking to someone you trust can help you process your feelings and gain perspective. You can call a friend, family member, or even a professional counselor if needed. 5. Practice relaxation techniques: Listening to music, doing yoga or meditation, or engaging in other relaxing activities can help reduce stress and anxiety. &nbsp Remember, it's okay to take time for yourself to process your emotions and come up with a plan that works best for you.</div></div></div></div></div><div class=example-content style=display:none><div class=title><span>Audio Analysis: Music analysis</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/_zVFfYfyyFg.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>This is a pop punk music piece. There is a male vocalist singing melodically in the lead. The tune is being played by the guitar with the bass guitar playing in the background. The rhythmic background consists of loud and aggressive sounding drums. The atmosphere is rebellious. This piece could be used in the soundtrack of a high school drama movie or TV show during scenes where the characters are rebelling against authority.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/key.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>The key of this music is F# major.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/tempo.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>The tempo of this music is 104.17 bpm.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/timesignature.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>The time signature of this music is 4/4.</div></div></div></div></div><div class=example-content style=display:none><div class=title><span>Audio Analysis: Robustness of mixed audio analysis</span> <a class=next-button>Next</a></div><div class=grid-layout><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/spk_0.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>The old man laid down his hand to light a cigar.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/spk_1.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>The old man laid down his hand to light a cigar.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/spk_2.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>The old man laid down his hand to light a cigar.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/spk_3.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>The old man laid down his hand to light a cigar.</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/robust_1.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>The lyrics of the singing are 'Waiting for my favorite song.'</div></div></div><div class=message><div class=role>User</div><div class="content wide-content"><div class=text-content><audio class=audio-icon controls><source src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/robust_2.wav type=audio/wav>Your browser does not support the audio element.</audio></div></div></div><div class=message><div class=role>Qwen2-Audio</div><div class="content wide-content"><div class=text-content>The lyrics of the singing are 'Waiting for my favorite song.'</div></div></div></div></div></div><h2 id=performance>Performance<a hidden class=anchor aria-hidden=true href=#performance>#</a></h2><p>We have conducted a series of experiments on benchmark datasets, including LibriSpeech, Common Voice 15, Fleurs, Aishell2, CoVoST2, Meld, Vocalsound, and AIR-Benchmark, to evaluate the performance of Qwen2-Audio in comparison with our previously released Qwen-Audio as well as the state-of-the-art models in each tasks. Below we demonstrate a figure to show how Qwen2-Audio performs against the competitors. In all the tasks, Qwen2-Audio significantly surpasses either previous SOTAs or Qwen-Audio.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/radar_compare_qwen_audio.png#center width=80%></figure><p>More specific results on the datasets are listed below in the table.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/blog/qwenaudio/table.jpg#center width=100%></figure><h2 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h2><p>Below is a demonstration of the training architecture. Specifically, we start from the Qwen language model and an audio encoder as the foundation models. We sequentially apply multi-task pretraining for the audio language alignment, and supervised finetuning as well as direct preference optimization to grasp the capabilities of downstream tasks and model human preference.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/demo/framework.png#center width=80%></figure><h2 id=how-to-use>How to Use<a hidden class=anchor aria-hidden=true href=#how-to-use>#</a></h2><p>Now Qwen2-Audio is officially supported by Hugging Face Transfomers. We advise you install the latest version of <code>transformers</code> from source:</p><div class=highlight><pre>pip install git+https://github.com/huggingface/transformers  
</pre></div><p>We demonstrate how to use <code>Qwen2-Audio-7B-Instruct</code> to play with voice chat and audio analysis. Here is an example of voice chat:</p><div class=highlight><pre>from  io  import  BytesIO   
from  urllib.request  import  urlopen   
import  librosa   
from  transformers  import  Qwen2AudioForConditionalGeneration ,  AutoProcessor   
  
processor  =  AutoProcessor . from_pretrained ( &#34;Qwen/Qwen2-Audio-7B-Instruct&#34; )   
model  =  Qwen2AudioForConditionalGeneration . from_pretrained ( &#34;Qwen/Qwen2-Audio-7B-Instruct&#34; ,  device_map = &#34;auto&#34; )   
  
conversation  =  [   
    { &#34;role&#34; :  &#34;user&#34; ,  &#34;content&#34; :  [   
        { &#34;type&#34; :  &#34;audio&#34; ,  &#34;audio_url&#34; :  &#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav&#34; },   
    ]},   
    { &#34;role&#34; :  &#34;assistant&#34; ,  &#34;content&#34; :  &#34;Yes, the speaker is female and in her twenties.&#34; },   
    { &#34;role&#34; :  &#34;user&#34; ,  &#34;content&#34; :  [   
        { &#34;type&#34; :  &#34;audio&#34; ,  &#34;audio_url&#34; :  &#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav&#34; },   
    ]},   
]   
text  =  processor . apply_chat_template ( conversation ,  add_generation_prompt = True ,  tokenize = False )   
audios  =  []   
for  message  in  conversation :   
    if  isinstance ( message [ &#34;content&#34; ],  list ):   
        for  ele  in  message [ &#34;content&#34; ]:   
            if  ele [ &#34;type&#34; ]  ==  &#34;audio&#34; :   
                audios . append ( librosa . load (   
                    BytesIO ( urlopen ( ele [ &#39;audio_url&#39; ]) . read ()),    
                    sr = processor . feature_extractor . sampling_rate )[ 0 ]   
                )   
  
inputs  =  processor ( text = text ,  audios = audios ,  return_tensors = &#34;pt&#34; ,  padding = True )   
inputs . input_ids  =  inputs . input_ids . to ( &#34;cuda&#34; )   
  
generate_ids  =  model . generate ( ** inputs ,  max_length = 256 )   
generate_ids  =  generate_ids [:,  inputs . input_ids . size ( 1 ):]   
  
response  =  processor . batch_decode ( generate_ids ,  skip_special_tokens = True ,  clean_up_tokenization_spaces = False )[ 0 ]   
</pre></div><p>In voice chat mode, the user input is simply the audio without text, and the instructions from the users are inside the audio. Next is an example of audio analysis:</p><div class=highlight><pre>from  io  import  BytesIO   
from  urllib.request  import  urlopen   
import  librosa   
from  transformers  import  Qwen2AudioForConditionalGeneration ,  AutoProcessor   
  
processor  =  AutoProcessor . from_pretrained ( &#34;Qwen/Qwen2-Audio-7B-Instruct&#34; )   
model  =  Qwen2AudioForConditionalGeneration . from_pretrained ( &#34;Qwen/Qwen2-Audio-7B-Instruct&#34; ,  device_map = &#34;auto&#34; )   
  
conversation  =  [   
    { &#39;role&#39; :  &#39;system&#39; ,  &#39;content&#39; :  &#39;You are a helpful assistant.&#39; },    
    { &#34;role&#34; :  &#34;user&#34; ,  &#34;content&#34; :  [   
        { &#34;type&#34; :  &#34;audio&#34; ,  &#34;audio_url&#34; :  &#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3&#34; },   
        { &#34;type&#34; :  &#34;text&#34; ,  &#34;text&#34; :  &#34;What&#39;s that sound?&#34; },   
    ]},   
    { &#34;role&#34; :  &#34;assistant&#34; ,  &#34;content&#34; :  &#34;It is the sound of glass shattering.&#34; },   
    { &#34;role&#34; :  &#34;user&#34; ,  &#34;content&#34; :  [   
        { &#34;type&#34; :  &#34;text&#34; ,  &#34;text&#34; :  &#34;What can you do when you hear that?&#34; },   
    ]},   
    { &#34;role&#34; :  &#34;assistant&#34; ,  &#34;content&#34; :  &#34;Stay alert and cautious, and check if anyone is hurt or if there is any damage to property.&#34; },   
    { &#34;role&#34; :  &#34;user&#34; ,  &#34;content&#34; :  [   
        { &#34;type&#34; :  &#34;audio&#34; ,  &#34;audio_url&#34; :  &#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac&#34; },   
        { &#34;type&#34; :  &#34;text&#34; ,  &#34;text&#34; :  &#34;What does the person say?&#34; },   
    ]},   
]   
text  =  processor . apply_chat_template ( conversation ,  add_generation_prompt = True ,  tokenize = False )   
audios  =  []   
for  message  in  conversation :   
    if  isinstance ( message [ &#34;content&#34; ],  list ):   
        for  ele  in  message [ &#34;content&#34; ]:   
            if  ele [ &#34;type&#34; ]  ==  &#34;audio&#34; :   
                audios . append (   
                    librosa . load (   
                        BytesIO ( urlopen ( ele [ &#39;audio_url&#39; ]) . read ()),    
                        sr = processor . feature_extractor . sampling_rate )[ 0 ]   
                )   
  
inputs  =  processor ( text = text ,  audios = audios ,  return_tensors = &#34;pt&#34; ,  padding = True )   
inputs . input_ids  =  inputs . input_ids . to ( &#34;cuda&#34; )   
  
generate_ids  =  model . generate ( ** inputs ,  max_length = 256 )   
generate_ids  =  generate_ids [:,  inputs . input_ids . size ( 1 ):]   
  
response  =  processor . batch_decode ( generate_ids ,  skip_special_tokens = True ,  clean_up_tokenization_spaces = False )[ 0 ]   
</pre></div><p>In comparison, in audio analysis mode, there is additional text instructions. However, switching between either mode only needs to modify your user inputs and there is no need to worry about others such as system prompt.</p><h2 id=next-step>Next Step<a hidden class=anchor aria-hidden=true href=#next-step>#</a></h2><p>This time we bring a new audio language model, Qwen2-Audio, which supports voice chat and audio analysis at the same time and understands more than 8 languages and dialects. In the near future, we plan to train improved Qwen2-Audio models on larger pretraining datasets, enabling the model to support longer audio (over 30s). We also plan to build larger Qwen2-Audio models to explore the scaling laws of audio language models.</p></div>
                
                            </section>
                    <section class="feeditem">
                <h2>
                    <a
                        class="itemtitle"
                        href="http://qwenlm.github.io/blog/qwen2-math/"
                    >Introducing Qwen2-Math</a>
                </h2>

                                    <time datetime="2024-08-07 16:00:00">
                        2024-08-07 16:00:00                    </time>
                    <p></p>
                
                
                <!-- Intentionally not escaping for html context -->
                <div class=post-content><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/blog/qwen2-math/fig1.jpg#center width=100%></figure><p><a href=https://github.com/QwenLM/Qwen2-Math class="btn external" target=_blank>GITHUB</a> <a href=https://huggingface.co/Qwen class="btn external" target=_blank>HUGGING FACE</a> <a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a> <a href=https://discord.gg/yPEP2vHTu4 class="btn external" target=_blank>DISCORD</a></p><blockquote><div align=center><b>üö® This model mainly supports English. We will release bilingual (English and Chinese) math models soon.</b></div></blockquote><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>Over the past year, we have dedicated significant effort to researching and enhancing the reasoning capabilities of large language models, with a particular focus on their ability to solve arithmetic and mathematical problems. Today, we are delighted to introduce a series of <strong>math-specific</strong> large language models of our Qwen2 series, Qwen2-Math and Qwen2-Math-Instruct-1.5B/7B/72B. Qwen2-Math is a series of specialized math language models built upon the Qwen2 LLMs, which significantly outperforms the mathematical capabilities of open-source models and even closed-source models (e.g., GPT-4o). We hope that Qwen2-Math can contribute to the community for solving complex mathematical problems.</p><p>We evaluate our math-specific models on a series of math benchmarks. The results below demonstrate that our largest math-specific model Qwen2-Math-72B-Instruct outperforms the state-of-the-art models, including GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Pro, and Llama-3.1-405B.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/blog/qwen2-math/fig2.jpg#center width=100%></figure><h2 id=qwen2-math-base-models>Qwen2-Math: Base Models<a hidden class=anchor aria-hidden=true href=#qwen2-math-base-models>#</a></h2><p>The base models of Qwen2-Math are initialized with Qwen2-1.5B/7B/72B, and then pretrained on a meticulously designed Mathematics-specific Corpus. This corpus contains large-scale high-quality mathematical web texts, books, codes, exam questions, and mathematical pre-training data synthesized by Qwen2.</p><p>We evaluate our Qwen2-Math base models on three widely used English math benchmarks GSM8K, Math, and MMLU-STEM. In addition, we also evaluate three Chinese math benchmarks CMATH, GaoKao Math Cloze, and GaoKao Math QA. All evaluations are tested with few-shot chain-of-thought prompting.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/blog/qwen2-math/Base-Table1.png#center width=90%></figure><h2 id=qwen2-math-instruct-instruction-tuned-models>Qwen2-Math-Instruct: Instruction-Tuned Models<a hidden class=anchor aria-hidden=true href=#qwen2-math-instruct-instruction-tuned-models>#</a></h2><p>We first trained a math-specific reward model based on Qwen2-Math-72B. We then combined this dense reward signal with a binary signal indicating whether the model answered correctly. This combined signal is used as supervision for constructing the SFT data through Rejection Sampling and also in the reinforcement learning with Group Relative Policy Optimization (GRPO) after SFT.</p><p>We evaluate Qwen2-Math-Instruct on mathematical benchmarks in both English and Chinese. In addition to the widely-used benchmarks, such as GSM8K and Math, we also involve more exams that are much challenging to fully inspect the capabilities of Qwen2-Math-Instruct, such as OlympiadBench, CollegeMath, GaoKao, AIME2024, and AMC2023. For Chinese mathematical benchmarks, we use CMATH, Gaokao (Chinese college entrance examination 2024), and CN Middle School 24 (China High School Entrance Examination 2024).</p><p>We report greedy , Maj@8 and RM@8 performance on all benchmarks in the zero-shot setting, except for the multi-choice benchmarks (including MMLU STEM and multiple-choice problems in GaoKao and CN Middle School 24) with a 5-shot setting. Qwen2-Math-Instruct achieves the best performance among models of the same size, with RM@8 outperforming Maj@8, particularly in the 1.5B and 7B models. This demonstrates the effectiveness of our math reward model.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/blog/qwen2-math/Instruct-Table1.png#center width=100%></figure><p>In more complex mathematical competition evaluations such as AIME 2024 and AMC 2023, Qwen2-Math-Instruct also performs well across various settings, including Greedy, Maj@64, RM@64, and RM@256.</p><figure><img src=https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/blog/qwen2-math/Instruct-Table2.png#center width=70%></figure><h2 id=case-study>Case Study<a hidden class=anchor aria-hidden=true href=#case-study>#</a></h2><p>Here we list some test cases, which include some IMO math problems. From the experimental results and case study, we find that Qwen2-Math is capable of solving simple math competition problems. Feel free to click the expandable blocks to check the cases!</p><blockquote><p>All the solutions are generated by our model without modification. Please note that we do not guarantee the correctness of the claims in the process.</p></blockquote><details><summary>Problem From IMO Shortlist 2002</summary><p><strong>Problem</strong></p><p>What is the smallest positive integer $t$ such that there exist integers $x_1,x_2,\ldots,x_t$ with</p><p>$$ x^3_1+x^3_2+,\ldots,+x^3_t=2002^{2002}, $$</p><p><strong>Qwen2-Math-72B-Instruct Solution</strong></p><p>To determine the smallest positive integer $ t $ such that there exist integers $ x_1, x_2, \ldots, x_t $ satisfying the equation</p><p>$$ x_1^3 + x_2^3 + \cdots + x_t^3 = 2002^{2002}, $$</p><p>we start by analyzing the properties of cubes and their sums. We need to find the smallest $ t $ for which the sum of $ t $ cubes equals $ 2002^{2002} $.</p><p>First, we note that $ 2002 $ can be factored into prime factors:</p><p>$$ 2002 = 2 \times 7 \times 11 \times 13. $$</p><p>Thus,</p><p>$$ 2002^{2002} = (2 \times 7 \times 11 \times 13)^{2002} = 2^{2002} \times 7^{2002} \times 11^{2002} \times 13^{2002}. $$</p><p>Next, we consider the properties of cubes modulo 9. Cubes modulo 9 can only be congruent to $ 0, 1, $ or $ -1 $. This is because:</p><p>$$ \begin{aligned} 0^3 &\equiv 0 \pmod{9}, \\\ 1^3 &\equiv 1 \pmod{9}, \\\ 2^3 &\equiv 8 \equiv -1 \pmod{9}, \\\ 3^3 &\equiv 27 \equiv 0 \pmod{9}, \\\ 4^3 &\equiv 64 \equiv 1 \pmod{9}, \\\ 5^3 &\equiv 125 \equiv -1 \pmod{9}, \\\ 6^3 &\equiv 216 \equiv 0 \pmod{9}, \\\ 7^3 &\equiv 343 \equiv 1 \pmod{9}, \\\ 8^3 &\equiv 512 \equiv -1 \pmod{9}. \end{aligned} $$</p><p>Now, we need to determine $ 2002^{2002} \mod 9 $. First, we find $ 2002 \mod 9 $:</p><p>$$ 2002 \div 9 = 222 \text{ remainder } 4 \quad \Rightarrow \quad 2002 \equiv 4 \pmod{9}. $$</p><p>Thus,</p><p>$$ 2002^{2002} \equiv 4^{2002} \pmod{9}. $$</p><p>Next, we use Euler&rsquo;s theorem, which states that $ a^{\phi(n)} \equiv 1 \pmod{n} $ for $ \gcd(a, n) = 1 $. Here, $ \phi(9) = 6 $, so:</p><p>$$ 4^6 \equiv 1 \pmod{9}. $$</p><p>We can reduce the exponent $ 2002 \mod 6 $:</p><p>$$ 2002 \div 6 = 333 \text{ remainder } 4 \quad \Rightarrow \quad 2002 \equiv 4 \pmod{6}. $$</p><p>Thus,</p><p>$$ 4^{2002} \equiv 4^4 \pmod{9}. $$</p><p>$$ 4^4 = 256. $$</p><p>Finally, we find $ 256 \mod 9 $:</p><p>$$ 256 \div 9 = 28 \text{ remainder } 4 \quad \Rightarrow \quad 256 \equiv 4 \pmod{9}. $$</p><p>Therefore,</p><p>$$ 2002^{2002} \equiv 4 \pmod{9}. $$</p><p>We need to find the smallest $ t $ such that the sum of $ t $ cubes is congruent to 4 modulo 9. The possible sums of cubes modulo 9 are:</p><p>$$ \begin{aligned} 0 &\equiv 0 \pmod{9}, \\\ 1 &\equiv 1 \pmod{9}, \\\ -1 &\equiv 8 \pmod{9}, \\\ 1 + 1 &\equiv 2 \pmod{9}, \\\ 1 + (-1) &\equiv 0 \pmod{9}, \\\ -1 + (-1) &\equiv -2 \equiv 7 \pmod{9}, \\\ 1 + 1 + 1 &\equiv 3 \pmod{9}, \\\ 1 + 1 + (-1) &\equiv 1 \pmod{9}, \\\ 1 + (-1) + (-1) &\equiv -1 \equiv 8 \pmod{9}, \\\ -1 + (-1) + (-1) &\equiv -3 \equiv 6 \pmod{9}, \\\ 1 + 1 + 1 + 1 &\equiv 4 \pmod{9}. \end{aligned} $$</p><p>We see that the sum of four cubes can be congruent to 4 modulo 9. Therefore, the smallest $ t $ is 4.</p><p>Thus, the smallest positive integer $ t $ such that there exist integers $ x_1, x_2, \ldots, x_t $ with</p><p>$$ x_1^3 + x_2^3 + \cdots + x_t^3 = 2002^{2002} $$</p><p>is</p><p>$$ \boxed{4}. $$</p></details><details><summary>Problem From IMO Shortlist 2022</summary><p><strong>Problem</strong></p><p>For a positive integer $n$ we denote by $s(n)$ the sum of the digits of $n$. Let $P(x)=x^n+a_{n-1}x^{n-1}+\cdots+a_1x+a_0$ be a polynomial, where $n \geqslant 2$ and $a_i$ is a positive integer for all $ 0 \leqslant i \leqslant n-1$. Could it be the case that, for all positive integers $k$, $s(k)$ and $s(P(k))$ have the same parity?</p><p><strong>Qwen2-Math-72B-Instruct Solution</strong></p><p>To determine whether it is possible for $ s(k) $ and $ s(P(k)) $ to have the same parity for all positive integers $ k $, we need to analyze the behavior of the sum of the digits function $ s $ and the polynomial $ P(x) $.</p><p>First, recall that the sum of the digits of a number $ k $ modulo 2 is equivalent to $ k \mod 2 $. This is because the sum of the digits of $ k $ in base 10 is congruent to $ k \mod 9 $, and since $ 9 \equiv 1 \mod 2 $, the sum of the digits modulo 2 is the same as $ k \mod 2 $.</p><p>Given the polynomial $ P(x) = x^n + a_{n-1}x^{n-1} + \cdots + a_1x + a_0 $, we need to determine the parity of $ P(k) $ for any positive integer $ k $. Specifically, we need to check if $ P(k) \mod 2 $ is the same as $ k \mod 2 $.</p><p>Let&rsquo;s consider the polynomial modulo 2:</p><p>$$ P(x) \equiv x^n + a_{n-1}x^{n-1} + \cdots + a_1x + a_0 \mod 2. $$</p><p>Since $ a_i $ are positive integers, $ a_i \mod 2 $ is either 0 or 1. Let $ b_i = a_i \mod 2 $. Then we can rewrite the polynomial modulo 2 as:</p><p>$$ P(x) \equiv x^n + b_{n-1}x^{n-1} + \cdots + b_1x + b_0 \mod 2. $$</p><p>Now, we need to check the parity of $ P(k) $ for $ k \equiv 0 \mod 2 $ and $ k \equiv 1 \mod 2 $.</p><ol><li><p><strong>Case 1: $ k \equiv 0 \mod 2 $</strong></p><p>$$ P(0) \equiv b_0 \mod 2. $$</p><p>For $ s(P(0)) $ to have the same parity as $ s(0) $, $ b_0 $ must be 0. Therefore, $ a_0 $ must be even.</p></li><li><p><strong>Case 2: $ k \equiv 1 \mod 2 $</strong></p><p>$$ P(1) \equiv 1^n + b_{n-1} \cdot 1^{n-1} + \cdots + b_1 \cdot 1 + b_0 \equiv 1 + b_{n-1} + \cdots + b_1 + b_0 \mod 2. $$</p><p>For $ s(P(1)) $ to have the same parity as $ s(1) $, $ 1 + b_{n-1} + \cdots + b_1 + b_0 $ must be odd. Since $ 1 $ is odd, the sum $ b_{n-1} + \cdots + b_1 + b_0 $ must be even.</p></li></ol><p>From the above cases, we have:</p><ul><li>$ a_0 $ must be even.</li><li>The sum $ b_{n-1} + \cdots + b_1 + b_0 $ must be even.</li></ul><p>However, if $ a_0 $ is even, then $ b_0 = 0 $. This means the sum $ b_{n-1} + \cdots + b_1 $ must be even. But since $ b_i $ are either 0 or 1, the sum of $ b_{n-1} + \cdots + b_1 $ being even implies that there must be an even number of 1&rsquo;s among $ b_{n-1}, \ldots, b_1 $.</p><p>Now, consider the polynomial $ P(x) = x^n + 1 $. For this polynomial:</p><ul><li>$ a_0 = 1 $ (odd), which contradicts our requirement that $ a_0 $ must be even.</li><li>The sum $ b_{n-1} + \cdots + b_1 + b_0 = 1 $ (odd), which also contradicts our requirement that the sum must be even.</li></ul><p>Therefore, it is impossible for $ s(k) $ and $ s(P(k)) $ to have the same parity for all positive integers $ k $.</p><p>Thus, the final answer is:</p><p>$$ \boxed{\text{No}} $$</p></details><details><summary>Problem From IMO 2022</summary><p><strong>Problem</strong></p><p>Let $\mathbb{R}^+$ denote the set of positive real numbers. Find all functions $f: \mathbb{R}^+ \to \mathbb{R}^+$ such that for each $x \in \mathbb{R}^+$, there is exactly one $y \in \mathbb{R}^+$ satisfying</p><p>$$ xf(y)+yf(x) \leq 2 $$</p><p><strong>Qwen2-Math-72B-Instruct Solution</strong></p><p>To find all functions $ f: \mathbb{R}^+ \to \mathbb{R}^+ $ such that for each $ x \in \mathbb{R}^+ $, there is exactly one $ y \in \mathbb{R}^+ $ satisfying</p><p>$$ xf(y) + yf(x) \leq 2, $$</p><p>we will proceed step by step.</p><p><strong>Step 1: Analyze the given inequality</strong> Given $ x \in \mathbb{R}^+ $, there exists a unique $ y \in \mathbb{R}^+ $ such that</p><p>$$ xf(y) + yf(x) \leq 2. $$</p><p>We need to explore the implications of this condition.</p><p><strong>Step 2: Consider the case $ x = y $</strong> Substitute $ y = x $ into the inequality:</p><p>$$ xf(x) + xf(x) = 2xf(x) \leq 2. $$</p><p>This simplifies to:</p><p>$$ xf(x) \leq 1. $$</p><p>Thus, for all $ x \in \mathbb{R}^+ $,</p><p>$$ f(x) \leq \frac{1}{x}. $$</p><p><strong>Step 3: Consider the uniqueness condition</strong> For each $ x \in \mathbb{R}^+ $, there is exactly one $ y \in \mathbb{R}^+ $ such that $ xf(y) + yf(x) \leq 2 $. We need to ensure that this $ y $ is unique.</p><p><strong>Step 4: Test the function $ f(x) = \frac{1}{x} $</strong> Consider the function $ f(x) = \frac{1}{x} $. Substitute $ f(x) = \frac{1}{x} $ and $ f(y) = \frac{1}{y} $ into the inequality:</p><p>$$ x \cdot \frac{1}{y} + y \cdot \frac{1}{x} = \frac{x}{y} + \frac{y}{x}. $$</p><p>We need:</p><p>$$ \frac{x}{y} + \frac{y}{x} \leq 2. $$</p><p>Using the AM-GM inequality, we know:</p><p>$$ \frac{x}{y} + \frac{y}{x} \geq 2. $$</p><p>Equality holds if and only if $ \frac{x}{y} = 1 $, which implies $ x = y $.</p><p>Thus, for $ f(x) = \frac{1}{x} $, the inequality $ xf(y) + yf(x) \leq 2 $ holds if and only if $ x = y $. This satisfies the condition that there is exactly one $ y $ for each $ x $.</p><p><strong>Step 5: Verify if other functions can satisfy the condition</strong> Suppose there exists another function $ g: \mathbb{R}^+ \to \mathbb{R}^+ $ such that $ g(x) \neq \frac{1}{x} $ for some $ x $. Then, for some $ x $, $ g(x) &lt; \frac{1}{x} $ or $ g(x) > \frac{1}{x} $.</p><ul><li>If $ g(x) &lt; \frac{1}{x} $, then $ xg(y) + yg(x) $ might not satisfy the inequality for all $ y $.</li><li>If $ g(x) > \frac{1}{x} $, then $ xg(y) + yg(x) $ might exceed 2 for some $ y $.</li></ul><p>Thus, any function other than $ f(x) = \frac{1}{x} $ would not satisfy the uniqueness condition.</p><p><strong>Conclusion</strong> The only function $ f: \mathbb{R}^+ \to \mathbb{R}^+ $ that satisfies the given condition is:</p><p>$$ \boxed{f(x) = \frac{1}{x}}. $$</p></details><details><summary>Problem from International Zhautykov Olympiad 2020</summary><p><strong>Problem</strong></p><p>Let $Z$ be the set of all integers. Find all the function $f: Z \to Z$ such that</p><p>$$ f(4x+3y)=f(3x+y)+f(x+2y) $$</p><p>For all integers $x,y$</p><p><strong>Qwen2-Math-72B-Instruct Solution</strong></p><p>To find all functions $ f: \mathbb{Z} \to \mathbb{Z} $ that satisfy the functional equation</p><p>$$ f(4x + 3y) = f(3x + y) + f(x + 2y) $$</p><p>for all integers $ x $ and $ y $, we will proceed step by step.</p><p><strong>Step 1: Initial Substitution</strong></p><p>First, let&rsquo;s substitute specific values for $ x $ and $ y $ to gain insight into the function.</p><p>Substitution 1: $ x = 0 $</p><p>$$ f(4 \cdot 0 + 3y) = f(3 \cdot 0 + y) + f(0 + 2y) $$</p><p>$$ f(3y) = f(y) + f(2y) $$</p><p>Substitution 2: $ y = 0 $</p><p>$$ f(4x + 3 \cdot 0) = f(3x + 0) + f(x + 2 \cdot 0) $$</p><p>$$ f(4x) = f(3x) + f(x) $$</p><p><strong>Step 2: Exploring Further Substitutions</strong></p><p>We now have two equations:</p><ol><li>$ f(3y) = f(y) + f(2y) $</li><li>$ f(4x) = f(3x) + f(x) $</li></ol><p>Substitution 3: $ y = 1 $ in the first equation</p><p>$$ f(3 \cdot 1) = f(1) + f(2 \cdot 1) $$</p><p>$$ f(3) = f(1) + f(2) $$</p><p>Substitution 4: $ x = 1 $ in the second equation</p><p>$$ f(4 \cdot 1) = f(3 \cdot 1) + f(1) $$</p><p>$$ f(4) = f(3) + f(1) $$</p><p><strong>Step 3: Using the Equations to Find a Pattern</strong></p><p>We have:</p><p>$$ f(3) = f(1) + f(2) $$</p><p>$$ f(4) = f(3) + f(1) $$</p><p>Substitute $ f(3) $ from the first equation into the second equation:</p><p>$$ f(4) = (f(1) + f(2)) + f(1) $$</p><p>$$ f(4) = 2f(1) + f(2) $$</p><p><strong>Step 4: Generalizing the Function</strong></p><p>We hypothesize that $ f $ might be a linear function. Let&rsquo;s assume $ f(x) = kx $ for some integer $ k $.</p><p><em>Verification</em> Substitute $ f(x) = kx $ into the original functional equation:</p><p>$$ f(4x + 3y) = k(4x + 3y) $$</p><p>$$ f(3x + y) = k(3x + y) $$</p><p>$$ f(x + 2y) = k(x + 2y) $$</p><p>The original equation becomes:</p><p>$$ k(4x + 3y) = k(3x + y) + k(x + 2y) $$</p><p>$$ 4kx + 3ky = 3kx + ky + kx + 2ky $$</p><p>$$ 4kx + 3ky = 4kx + 3ky $$</p><p>This equation holds true for all $ x $ and $ y $, confirming that $ f(x) = kx $ is a solution.</p><p><strong>Conclusion</strong></p><p>The only functions $ f: \mathbb{Z} \to \mathbb{Z} $ that satisfy the given functional equation are of the form $ f(x) = kx $ for some integer $ k $.</p><p>Thus, the final answer is:</p><p>$$ \boxed{f(x) = kx \text{ for some integer } k} $$</p></details><details><summary>Problem From Baltic Way 2023</summary><p><strong>Problem</strong></p><p>Find all functions $f: \mathbb{R} \rightarrow \mathbb{R}$ such that</p><p>$$ f(f(x)+y)+xf(y)=f(xy+y)+f(x) $$</p><pre><code>for reals $x, y$. </code></pre><p><strong>Qwen2-Math-72B-Instruct Solution:</strong></p><p>To find all functions $ f: \mathbb{R} \rightarrow \mathbb{R} $ that satisfy the functional equation</p><p>$$ f(f(x) + y) + x f(y) = f(xy + y) + f(x), $$</p><p>we will proceed step by step.</p><p><strong>Step 1: Initial Substitution</strong></p><p>First, let&rsquo;s substitute $ y = 0 $ into the equation:</p><p>$$ f(f(x) + 0) + x f(0) = f(x \cdot 0 + 0) + f(x). $$</p><p>This simplifies to:</p><p>$$ f(f(x)) + x f(0) = f(0) + f(x). $$</p><p>Rearranging terms, we get:</p><p>$$ f(f(x)) = f(x) + f(0) - x f(0). $$</p><p><strong>Step 2: Analyzing the Simplified Equation</strong></p><p>Let $ c = f(0) $. The equation becomes:</p><p>$$ f(f(x)) = f(x) + c - x c. $$</p><p><strong>Step 3: Substituting $ x = 0 $</strong></p><p>Next, substitute $ x = 0 $ into the original equation:</p><p>$$ f(f(0) + y) + 0 \cdot f(y) = f(0 \cdot y + y) + f(0). $$</p><p>This simplifies to:</p><p>$$ f(c + y) = f(y) + c. $$</p><p><strong>Step 4: Substituting $ y = -c $</strong></p><p>Substitute $ y = -c $ into the equation $ f(c + y) = f(y) + c $:</p><p>$$ f(c - c) = f(-c) + c. $$</p><p>This simplifies to:</p><p>$$ f(0) = f(-c) + c. $$</p><p>Since $ f(0) = c $, we have:</p><p>$$ c = f(-c) + c. $$</p><p>Subtracting $ c $ from both sides, we get:</p><p>$$ f(-c) = 0. $$</p><p><strong>Step 5: Substituting $ y = -f(x) $</strong></p><p>Substitute $ y = -f(x) $ into the original equation:</p><p>$$ f(f(x) - f(x)) + x f(-f(x)) = f(x(-f(x)) - f(x)) + f(x). $$</p><p>This simplifies to:</p><p>$$ f(0) + x f(-f(x)) = f(-x f(x) - f(x)) + f(x). $$</p><p>Since $ f(0) = c $, we have:</p><p>$$ c + x f(-f(x)) = f(-x f(x) - f(x)) + f(x). $$</p><p><strong>Step 6: Considering $ f(x) = 0 $</strong></p><p>Assume $ f(x) = 0 $ for all $ x $. Substitute $ f(x) = 0 $ into the original equation:</p><p>$$ f(0 + y) + x f(y) = f(xy + y) + f(x). $$</p><p>This simplifies to:</p><p>$$ f(y) + 0 = f(y) + 0, $$</p><p>which is always true.</p><p><strong>Step 7: Verifying $ f(x) = x $</strong></p><p>Assume $ f(x) = x $. Substitute $ f(x) = x $ into the original equation:</p><p>$$ f(x + y) + x f(y) = f(xy + y) + f(x). $$</p><p>This simplifies to:</p><p>$$ x + y + xy = xy + y + x, $$</p><p>which is always true.</p><p><strong>Conclusion</strong></p><p>The only functions that satisfy the given functional equation are $ f(x) = 0 $ and $ f(x) = x $. Therefore, the solutions are:</p><p>$$ \boxed{f(x) = 0 \text{ or } f(x) = x}. $$</p></details><details><summary>Problem From Lusophon Mathematical Olympiad 2023</summary><p><strong>Problem</strong></p><p>A long time ago, there existed Martians with $ 3$ different colours: red, green and blue. As Mars was devastated by an intergalactic war, only $ 2$ Martians of each colours survived. In order to reconstruct the Martian population, they decided to use a machine that transforms two Martians of distinct colours into four Martians of colour different to the two initial ones. For example, if a red Martian and a blue Martian use the machine, they&rsquo;ll be transformed into four green Martians.</p><p>a) Is it possible that, after using that machine finitely many times, we have $ 2022 $ red Martians, $ 2022$ green Martians and $ 2022$ blue Martians?</p><p>b) Is it possible that, after using that machine finitely many times, we have $ 2021$ red Martians, $ 2022$ green Martians and $ 2023$ blue Martians?</p><p><strong>Qwen2-Math-72B-Instruct Solution</strong></p><p>Let&rsquo;s analyze the problem step by step.</p><p><strong>Part (a)</strong></p><p>We start with 2 red, 2 green, and 2 blue Martians. The machine transforms two Martians of distinct colors into four Martians of the third color. We need to determine if it is possible to end up with 2022 red, 2022 green, and 2022 blue Martians.</p><p>First, let&rsquo;s consider the parity (odd or even nature) of the number of Martians of each color. Initially, we have:</p><ul><li>2 red (even)</li><li>2 green (even)</li><li>2 blue (even)</li></ul><p>Each transformation changes the parity of the number of Martians of the two initial colors and the parity of the number of Martians of the third color. Specifically:</p><ul><li>If we transform 1 red and 1 green Martian, we get 4 blue Martians. This changes the parity of red and green Martians from even to odd, and the parity of blue Martians from even to odd.</li><li>If we transform 1 red and 1 blue Martian, we get 4 green Martians. This changes the parity of red and blue Martians from even to odd, and the parity of green Martians from even to odd.</li><li>If we transform 1 green and 1 blue Martian, we get 4 red Martians. This changes the parity of green and blue Martians from even to odd, and the parity of red Martians from even to odd.</li></ul><p>In all cases, the parity of the number of Martians of each color changes from even to odd or from odd to even. Therefore, after any number of transformations, the parity of the number of Martians of each color will always be the same as it was initially.</p><p>Since initially all three colors have an even number of Martians, after any number of transformations, the number of Martians of each color will remain even.</p><p>Given that 2022 is an even number, it is possible to have 2022 red, 2022 green, and 2022 blue Martians after a finite number of transformations.</p><p>Thus, the answer to part (a) is:</p><p>$$ \boxed{\text{Yes}} $$</p><p><strong>Part (b)</strong></p><p>We start again with 2 red, 2 green, and 2 blue Martians. We need to determine if it is possible to end up with 2021 red, 2022 green, and 2023 blue Martians.</p><p>As in part (a), we consider the parity of the number of Martians of each color. Initially, we have:</p><ul><li>2 red (even)</li><li>2 green (even)</li><li>2 blue (even)</li></ul><p>Each transformation changes the parity of the number of Martians of the two initial colors and the parity of the number of Martians of the third color. Therefore, after any number of transformations, the parity of the number of Martians of each color will always be the same as it was initially.</p><p>Since initially all three colors have an even number of Martians, after any number of transformations, the number of Martians of each color will remain even.</p><p>Given that 2021 is an odd number, it is impossible to have 2021 red Martians after any number of transformations. Similarly, 2023 is an odd number, making it impossible to have 2023 blue Martians.</p><p>Thus, the answer to part (b) is:</p><p>$$ \boxed{\text{No}} $$</p></details><details><summary>Problem From Balkan MO 2023</summary><p><strong>Problem</strong></p><p>Joe and Penny play a game. Initially there are $ 5000$ stones in a pile, and the two players remove stones from the pile by making a sequence of moves. On the $ k$-th move, any number of stones between $ 1$ and $ k$ inclusive may be removed. Joe makes the odd-numbered moves and Penny makes the even-numbered moves. The player who removes the very last stone is the winner. Who wins if both players play perfectly?</p><p><strong>Qwen2-Math-72B-Instruct Solution</strong></p><p>To determine the winner of the game, we need to analyze the strategy and the sequence of moves. Let&rsquo;s break down the problem step by step.</p><ol><li><p><strong>Understanding the Game Rules:</strong></p><ul><li>Initially, there are 5000 stones.</li><li>Joe makes the first move (odd-numbered move), Penny makes the second move (even-numbered move), and so on.</li><li>On the $ k $-th move, a player can remove between 1 and $ k $ stones.</li><li>The player who removes the last stone wins.</li></ul></li><li><p><strong>Key Insight:</strong></p><ul><li>The game is symmetric in the sense that the number of stones a player can remove on their turn increases linearly with the move number.</li><li>The total number of moves in the game is equal to the number of stones initially in the pile, which is 5000.</li></ul></li><li><p><strong>Total Number of Moves:</strong></p><ul><li>Since there are 5000 stones, there will be 5000 moves in total.</li><li>Joe makes the odd-numbered moves: 1st, 3rd, 5th, &mldr;, 4999th.</li><li>Penny makes the even-numbered moves: 2nd, 4th, 6th, &mldr;, 5000th.</li></ul></li><li><p><strong>Determining the Winner:</strong></p><ul><li>The last move (5000th move) is an even-numbered move.</li><li>Therefore, Penny will make the last move.</li></ul></li><li><p><strong>Conclusion:</strong></p><ul><li>Since Penny makes the last move, she will be the one to remove the last stone.</li><li>Hence, Penny wins the game if both players play perfectly.</li></ul></li></ol><p>Thus, the final answer is:</p><p>$$ \boxed{\text{Penny}} $$</p></details><details><summary>Problem From Math Odyssey</summary><p><strong>Problem</strong></p><p>The school offers 10 elective courses, and each student can enroll in any number of courses. The director selects $k$ students, where although each student&rsquo;s combination of courses is different, any two students have at least one course in common. At this point, it is found that any student outside these $k$ students cannot be classmates with these $k$ students regardless of how they enroll (having one course in common is enough to be classmates). Find the maximum number of $k$.</p><p><strong>Qwen2-Math-72B-Instruct Solution</strong></p><p>To solve this problem, we need to determine the maximum number of students $ k $ such that each student has a unique combination of elective courses, any two students share at least one course, and no student outside these $ k $ students can be classmates with all of them. This means that the set of students chosen must cover all possible courses in such a way that any new student would have to share a course with at least one of the chosen students.</p><p>First, let&rsquo;s consider the total number of possible combinations of courses. Since there are 10 elective courses and each student can either take or not take each course, there are $ 2^{10} = 1024 $ possible combinations of courses. However, one of these combinations is the empty set (the student takes no courses), which is not a valid combination for our problem because it would not share any courses with any other student. Therefore, there are $ 1024 - 1 = 1023 $ non-empty combinations of courses.</p><p>We need to find the maximum number of students $ k $ such that any two students share at least one course, and no student outside these $ k $ students can be classmates with all of them. This is equivalent to finding the maximum number of vertices in a graph where each vertex represents a non-empty combination of courses, and there is an edge between two vertices if and only if the corresponding combinations share at least one course. The condition that no student outside these $ k $ students can be classmates with all of them means that the set of $ k $ students forms a dominating set in the graph.</p><p>The maximum size of such a set is the complement of the minimum size of an independent set in the graph. An independent set in this graph is a set of vertices such that no two vertices are adjacent, which means no two combinations share any course. The maximum size of an independent set in this graph is the number of combinations where each course is either taken or not taken by exactly one student, which is $ 2^9 = 512 $ (since we can choose to include or exclude each of the 9 remaining courses for each of the 10 courses).</p><p>Therefore, the minimum size of an independent set is 512, and the maximum size of a dominating set (which is the complement of the independent set) is $ 1023 - 512 + 1 = 511 $. However, we need to add 1 because the empty set is not included, so the correct calculation is $ 1023 - 511 = 512 $.</p><p>Thus, the maximum number of students $ k $ is $\boxed{512}$.</p></details><details><summary>Problem from USAMO 2010</summary><p><strong>Problem</strong></p><p>Let $ABC$ be a triangle with $\angle A = 90^{\circ}$. Points $D$ and $E$ lie on sides $AC$ and $AB$, respectively, such that $\angle ABD = \angle DBC$ and $\angle ACE = \angle ECB$. Segments $BD$ and $CE$ meet at $I$. Determine whether or not it is possible for segments $AB$, $AC$, $BI$, $ID$, $CI$, $IE$ to all have integer lengths.</p><p><strong>Qwen2-Math-72B-Instruct Solution</strong></p><p>To determine whether it is possible for segments $AB$, $AC$, $BI$, $ID$, $CI$, and $IE$ to all have integer lengths in triangle $ABC$ with $\angle A = 90^\circ$, we start by analyzing the given conditions and properties of the triangle.</p><p>Given:</p><ul><li>$\angle A = 90^\circ$</li><li>$\angle ABD = \angle DBC$</li><li>$\angle ACE = \angle ECB$</li><li>Segments $BD$ and $CE$ intersect at $I$</li></ul><p>First, note that since $\angle A = 90^\circ$, triangle $ABC$ is a right triangle with $A$ as the right angle. Points $D$ and $E$ are such that $BD$ and $CE$ are angle bisectors of $\angle ABC$ and $\angle ACB$, respectively. Therefore, $I$ is the incenter of triangle $ABC$.</p><p><strong>Step 1: Coordinates and Lengths</strong> Assume $A$ is at the origin $(0, 0)$, $B$ is at $(a, 0)$, and $C$ is at $(0, b)$. The lengths of the sides are:</p><ul><li>$AB = a$</li><li>$AC = b$</li><li>$BC = \sqrt{a^2 + b^2}$</li></ul><p><strong>Step 2: Incenter Coordinates</strong> The incenter $I$ of a right triangle with legs $a$ and $b$ and hypotenuse $\sqrt{a^2 + b^2}$ is located at:</p><p>$$ I = \left( \frac{a}{2}, \frac{b}{2} \right) $$</p><p><strong>Step 3: Lengths of Segments $BI$ and $CI$</strong> Using the distance formula, we calculate $BI$ and $CI$:</p><p>$$ BI = \sqrt{\left( a - \frac{a}{2} \right)^2 + \left( 0 - \frac{b}{2} \right)^2} = \sqrt{\left( \frac{a}{2} \right)^2 + \left( \frac{b}{2} \right)^2} = \frac{\sqrt{a^2 + b^2}}{2} $$</p><p>$$ CI = \sqrt{\left( 0 - \frac{a}{2} \right)^2 + \left( b - \frac{b}{2} \right)^2} = \sqrt{\left( \frac{a}{2} \right)^2 + \left( \frac{b}{2} \right)^2} = \frac{\sqrt{a^2 + b^2}}{2} $$</p><p><strong>Step 4: Lengths of Segments $ID$ and $IE$</strong> Since $D$ and $E$ are points on $AC$ and $AB$ respectively, and $I$ is the incenter, $ID$ and $IE$ are the distances from the incenter to the sides of the triangle. For a right triangle, these distances are given by:</p><p>$$ ID = IE = r = \frac{a + b - \sqrt{a^2 + b^2}}{2} $$</p><p><strong>Step 5: Integer Lengths</strong> For $AB$, $AC$, $BI$, $ID$, $CI$, and $IE$ to all be integers, the following must hold:</p><ul><li>$a$ and $b$ are integers</li><li>$\frac{\sqrt{a^2 + b^2}}{2}$ is an integer</li><li>$\frac{a + b - \sqrt{a^2 + b^2}}{2}$ is an integer</li></ul><p><strong>Step 6: Analysis</strong> For $\frac{\sqrt{a^2 + b^2}}{2}$ to be an integer, $\sqrt{a^2 + b^2}$ must be an even integer. Let $\sqrt{a^2 + b^2} = 2k$ for some integer $k$. Then:</p><p>$$ a^2 + b^2 = 4k^2 $$</p><p>For $\frac{a + b - \sqrt{a^2 + b^2}}{2}$ to be an integer, $a + b - 2k$ must be even. Since $a$ and $b$ are integers, $a + b$ must be even.</p><p><strong>Conclusion</strong> Given the constraints, it is possible to find integer values for $a$ and $b$ such that all the conditions are satisfied. For example, if $a = 3$ and $b = 4$, then:</p><ul><li>$AB = 3$</li><li>$AC = 4$</li><li>$BC = 5$</li><li>$BI = CI = \frac{5}{2} = 2.5$ (not an integer)</li></ul><p>Thus, it is not possible for all segments $AB$, $AC$, $BI$, $ID$, $CI$, and $IE$ to have integer lengths simultaneously.</p><p>$$ \boxed{\text{No}} $$</p></details><details><summary>Problem from JBMO Shortlist 2011</summary><p><strong>Problem</strong></p><p>Let $AD,BF$ and ${CE}$ be the altitudes of $\vartriangle ABC$. A line passing through ${D}$ and parallel to ${AB}$intersects the line ${EF}$at the point ${G}$. If ${H}$ is the orthocenter of $\vartriangle ABC$, find the angle ${\angle{CGH}}$.</p><p><strong>Qwen2-Math-72B-Instruct Solution</strong></p><p>To solve for the angle $\angle CGH$ in the given triangle $\triangle ABC$ with altitudes $AD$, $BF$, and $CE$ intersecting at the orthocenter $H$, and a line through $D$ parallel to $AB$ intersecting $EF$ at $G$, we will follow these steps:</p><ol><li><p><strong>Identify Key Points and Properties:</strong></p><ul><li>$D$ is the foot of the altitude from $A$ to $BC$.</li><li>$E$ is the foot of the altitude from $B$ to $AC$.</li><li>$F$ is the foot of the altitude from $C$ to $AB$.</li><li>$H$ is the orthocenter of $\triangle ABC$.</li><li>$G$ is the intersection of the line through $D$ parallel to $AB$ and the line $EF$.</li></ul></li><li><p><strong>Use the Property of the Orthocenter:</strong></p><ul><li>The orthocenter $H$ lies on the altitudes $AD$, $BF$, and $CE$.</li><li>The points $D$, $E$, and $F$ are collinear with $H$ on the respective altitudes.</li></ul></li><li><p><strong>Analyze the Parallel Line:</strong></p><ul><li>Since $DG \parallel AB$, $\angle ADG = \angle DAB$ (corresponding angles).</li></ul></li><li><p><strong>Consider the Cyclic Quadrilateral:</strong></p><ul><li>The quadrilateral $AEHF$ is cyclic because $\angle AEH = \angle AFH = 90^\circ$.</li><li>Therefore, $\angle EHF = 180^\circ - \angle EAF$.</li></ul></li><li><p><strong>Use the Parallel Line Property Again:</strong></p><ul><li>Since $DG \parallel AB$, $\angle DGE = \angle BAE$ (alterFnate interior angles).</li></ul></li><li><p><strong>Relate Angles in Cyclic Quadrilateral:</strong></p><ul><li>In $\triangle AEF$, $\angle EAF = \angle EHF$ because they are opposite angles in the cyclic quadrilateral $AEHF$.</li></ul></li><li><p><strong>Determine $\angle CGH$:</strong></p><ul><li>Since $G$ lies on $EF$ and $DG \parallel AB$, $\angle DGE = \angle BAE$.</li><li>$\angle BAE = \angle EAF$ (as established).</li><li>Therefore, $\angle DGE = \angle EAF$.</li></ul></li><li><p><strong>Final Angle Calculation:</strong></p><ul><li>Since $\angle EAF = \angle EHF$ and $\angle EHF = 90^\circ$ (as $H$ is the orthocenter and $E$ and $F$ are feet of perpendiculars), we have: $$ \angle CGH = \angle EHF = 90^\circ. $$</li></ul></li></ol><p>Thus, the angle $\angle CGH$ is $\boxed{90^\circ}$.</p></details><h2 id=decontamination>Decontamination<a hidden class=anchor aria-hidden=true href=#decontamination>#</a></h2><p>We conduct decontamination methods on both our pretraining and post-training datasets. Specifically, for pretraining data, we target on math datasets, including GSM8K, MATH, and remove samples that have significant overlaps with the test sets. We use exact match to remove the identical samples and further apply 13-gram deduplication (with a condition that the ratio of longest common sequence should be larger than 0.6) to remove more samples that might cause contamination. For post-training data, we remove more postitive contaminated samples that have overlaps with GSM8K, MATH, Aqua, SAT Math, OlympiadBench, College Math, AIME24, AMC23, etc. with the same filtering method.</p><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p>This time, we&rsquo;re releasing a new model series focused on mathematical capabilities, Qwen2-Math, built upon the Qwen2 foundation. Our flagship model, Qwen2-Math-72B-Instruct, outperforms proprietary models such as GPT-4o and Claude 3.5 in math-related tasks. Given the current limitation of English-only support, we plan to release bilingual models that support both English and Chinese shortly, with the development of multilingual models also in the pipeline. Moreover, we will continue to enhance our models&rsquo; ability to solve complex and challenging mathematical problems.</p></div>
                
                            </section>
        
    </div>
 </body>
</html>
